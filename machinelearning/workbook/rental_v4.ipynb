{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f20f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49619c6b-09be-4640-9efe-089db8787f85",
   "metadata": {},
   "source": [
    " ## Load all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7481c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Building Class</th>\n",
       "      <th>Floor Area (SQM)</th>\n",
       "      <th>25th Percentile ($ PSM)</th>\n",
       "      <th>Median ($PSM)</th>\n",
       "      <th>75th Percentile ($PSM)</th>\n",
       "      <th>Reference Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Central Area</td>\n",
       "      <td>Category 1</td>\n",
       "      <td>100 &amp; Below</td>\n",
       "      <td>105.34</td>\n",
       "      <td>115.30</td>\n",
       "      <td>125.58</td>\n",
       "      <td>2025Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Central Area</td>\n",
       "      <td>Category 1</td>\n",
       "      <td>&gt;100 - 200</td>\n",
       "      <td>106.57</td>\n",
       "      <td>121.47</td>\n",
       "      <td>144.77</td>\n",
       "      <td>2025Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Central Area</td>\n",
       "      <td>Category 1</td>\n",
       "      <td>&gt;200 - 500</td>\n",
       "      <td>107.64</td>\n",
       "      <td>121.63</td>\n",
       "      <td>143.16</td>\n",
       "      <td>2025Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central Area</td>\n",
       "      <td>Category 1</td>\n",
       "      <td>&gt;500 - 1000</td>\n",
       "      <td>106.83</td>\n",
       "      <td>113.98</td>\n",
       "      <td>139.39</td>\n",
       "      <td>2025Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Central Area</td>\n",
       "      <td>Category 1</td>\n",
       "      <td>&gt;1000</td>\n",
       "      <td>100.11</td>\n",
       "      <td>126.22</td>\n",
       "      <td>138.86</td>\n",
       "      <td>2025Q2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Location Building Class Floor Area (SQM)  25th Percentile ($ PSM)  \\\n",
       "0  Central Area     Category 1      100 & Below                   105.34   \n",
       "1  Central Area     Category 1       >100 - 200                   106.57   \n",
       "2  Central Area     Category 1       >200 - 500                   107.64   \n",
       "3  Central Area     Category 1      >500 - 1000                   106.83   \n",
       "4  Central Area     Category 1            >1000                   100.11   \n",
       "\n",
       "   Median ($PSM)  75th Percentile ($PSM) Reference Period  \n",
       "0         115.30                  125.58           2025Q2  \n",
       "1         121.47                  144.77           2025Q2  \n",
       "2         121.63                  143.16           2025Q2  \n",
       "3         113.98                  139.39           2025Q2  \n",
       "4         126.22                  138.86           2025Q2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "file_path = 'CommercialOfficeRental.csv'\n",
    "office_df = pd.read_csv(file_path)\n",
    "office_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72bca72-9842-4e29-b2b7-f9e9e6723613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Floor Level</th>\n",
       "      <th>Floor Area (SQM)</th>\n",
       "      <th>25th Percentile ($ PSM)</th>\n",
       "      <th>Median ($PSM)</th>\n",
       "      <th>75th Percentile ($PSM)</th>\n",
       "      <th>Reference Period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B1 &amp; Below</td>\n",
       "      <td>30 &amp; Below</td>\n",
       "      <td>69.89</td>\n",
       "      <td>70.20</td>\n",
       "      <td>112.70</td>\n",
       "      <td>2021Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>B1 &amp; Below</td>\n",
       "      <td>&gt;30 - 100</td>\n",
       "      <td>65.45</td>\n",
       "      <td>87.61</td>\n",
       "      <td>98.41</td>\n",
       "      <td>2021Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>B1 &amp; Below</td>\n",
       "      <td>&gt;100 - 300</td>\n",
       "      <td>50.33</td>\n",
       "      <td>57.70</td>\n",
       "      <td>87.20</td>\n",
       "      <td>2021Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Level 1</td>\n",
       "      <td>30 &amp; Below</td>\n",
       "      <td>87.36</td>\n",
       "      <td>123.94</td>\n",
       "      <td>222.00</td>\n",
       "      <td>2021Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Level 1</td>\n",
       "      <td>&gt;30 - 100</td>\n",
       "      <td>75.26</td>\n",
       "      <td>109.37</td>\n",
       "      <td>147.04</td>\n",
       "      <td>2021Q1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Postal District Floor Level Floor Area (SQM)  25th Percentile ($ PSM)  \\\n",
       "0                1  B1 & Below       30 & Below                    69.89   \n",
       "1                1  B1 & Below        >30 - 100                    65.45   \n",
       "2                1  B1 & Below       >100 - 300                    50.33   \n",
       "3                1     Level 1       30 & Below                    87.36   \n",
       "4                1     Level 1        >30 - 100                    75.26   \n",
       "\n",
       "  Median ($PSM) 75th Percentile ($PSM) Reference Period  \n",
       "0         70.20                 112.70           2021Q1  \n",
       "1         87.61                  98.41           2021Q1  \n",
       "2         57.70                  87.20           2021Q1  \n",
       "3        123.94                 222.00           2021Q1  \n",
       "4        109.37                 147.04           2021Q1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'CommercialRetailRental.csv'\n",
    "retail_df = pd.read_csv(file_path)\n",
    "retail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9f193d-b3b7-416c-93e8-e1e1091c7f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Postal codes: (121154, 4)\n",
      "City coordinates: (332, 9)\n",
      "Street coordinates: (589, 3)\n",
      "Train stations: (209, 4)\n",
      "Postal district mapping: (28, 3)\n",
      "Missing counts per row:\n",
      " 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "327    0\n",
      "328    0\n",
      "329    0\n",
      "330    6\n",
      "331    0\n",
      "Length: 332, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load geographic datasets\n",
    "postal_codes = pd.read_csv('SG_postal.csv')\n",
    "city_coordinates = pd.read_csv('singapore_city_coordinates_improved.csv')\n",
    "street_coordinates = pd.read_csv('street_coordinates.csv')\n",
    "train_stations = pd.read_csv('mrt_lrt_data.csv')\n",
    "postal_district_mapping = pd.read_csv('sg_postal_districts.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Postal codes: {postal_codes.shape}\")\n",
    "print(f\"City coordinates: {city_coordinates.shape}\")\n",
    "print(f\"Street coordinates: {street_coordinates.shape}\")\n",
    "print(f\"Train stations: {train_stations.shape}\")\n",
    "print(f\"Postal district mapping: {postal_district_mapping.shape}\")\n",
    "\n",
    "\n",
    "missing_counts = city_coordinates.isnull().sum(axis=1)\n",
    "print(\"Missing counts per row:\\n\", missing_counts)\n",
    "\n",
    "missing_counts = city_coordinates.isna().sum(axis=1)  # Count missing values per row\n",
    "city_coordinates.drop(city_coordinates[missing_counts >= 4].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1b932-8082-413d-9f4e-303fc48fcf59",
   "metadata": {},
   "source": [
    "## Add geo data to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6288c5cf-29a8-401a-a382-568fe036a68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING OFFICE AND RETAIL DATA WITH POSTAL DISTRICT & LOCATION FEATURES\n",
      "======================================================================\n",
      "\n",
      "========================================\n",
      "PROCESSING OFFICE DATA\n",
      "========================================\n",
      "Adding geographic features to OFFICE summary data (356 rows)...\n",
      "Creating Postal District feature for office data...\n",
      "Created Postal District for 268 office rows\n",
      "Ensuring Location format for office data...\n",
      "Adding coordinates from postal districts...\n",
      "Added coordinates: 0 rows\n",
      "Added general location: 268 rows\n",
      "\n",
      "Final feature consolidation...\n",
      "  Added missing feature: Region_Classification\n",
      "  Added missing feature: Urban_Classification\n",
      "  Added missing feature: Distance_to_MRT_km\n",
      "  Added missing feature: Distance_to_CBD_km\n",
      "\n",
      "============================================================\n",
      "GEOGRAPHIC FEATURE ENGINEERING COMPLETE - OFFICE\n",
      "============================================================\n",
      "CORE FEATURES ADDED:\n",
      "  • Postal District: 268/356\n",
      "  • Location: 356/356\n",
      "  • General Location: 268/356\n",
      "\n",
      "GEOGRAPHIC FEATURES:\n",
      "  • Coordinates: 0/356\n",
      "  • MRT Distances: 0/356\n",
      "  • Urban Classification: 356/356\n",
      "\n",
      "SAMPLE OF ADDED FEATURES:\n",
      "       Location Postal District_x Postal_District_Str Postal District_y  \\\n",
      "0  Central Area                 1                   1               NaN   \n",
      "1  Central Area                 1                   1               NaN   \n",
      "2  Central Area                 1                   1               NaN   \n",
      "\n",
      "  Postal District                             General_Location  \\\n",
      "0               1  Raffles Place, Cecil, Marina, People's Park   \n",
      "1               1  Raffles Place, Cecil, Marina, People's Park   \n",
      "2               1  Raffles Place, Cecil, Marina, People's Park   \n",
      "\n",
      "   Region_Classification  Urban_Classification  \n",
      "0                    NaN                   NaN  \n",
      "1                    NaN                   NaN  \n",
      "2                    NaN                   NaN  \n",
      "\n",
      "========================================\n",
      "PROCESSING RETAIL DATA\n",
      "========================================\n",
      "Adding geographic features to RETAIL summary data (3753 rows)...\n",
      "Ensuring Postal District format for retail data...\n",
      "Creating Location feature for retail data...\n",
      "Created Location for 3753 retail rows\n",
      "Adding coordinates from postal districts...\n",
      "Added coordinates: 2136 rows\n",
      "Added general location: 3753 rows\n",
      "Calculating MRT distances for 2136 rows...\n",
      "Added MRT distances: 2136 rows\n",
      "Added urban classification\n",
      "\n",
      "Final feature consolidation...\n",
      "  Added missing feature: Region_Classification\n",
      "\n",
      "============================================================\n",
      "GEOGRAPHIC FEATURE ENGINEERING COMPLETE - RETAIL\n",
      "============================================================\n",
      "CORE FEATURES ADDED:\n",
      "  • Postal District: 3753/3753\n",
      "  • Location: 3753/3753\n",
      "  • General Location: 3753/3753\n",
      "\n",
      "GEOGRAPHIC FEATURES:\n",
      "  • Coordinates: 2136/3753\n",
      "  • MRT Distances: 2136/3753\n",
      "  • Urban Classification: 2136/3753\n",
      "\n",
      "SAMPLE OF ADDED FEATURES:\n",
      "  Postal District_x                                     Location  \\\n",
      "0                 1  Raffles Place, Cecil, Marina, People's Park   \n",
      "1                 1  Raffles Place, Cecil, Marina, People's Park   \n",
      "2                 1  Raffles Place, Cecil, Marina, People's Park   \n",
      "\n",
      "  Postal_District_Str Postal District_y Postal District  \\\n",
      "0                   1               NaN               1   \n",
      "1                   1               NaN               1   \n",
      "2                   1               NaN               1   \n",
      "\n",
      "                              General_Location Urban_Classification  \\\n",
      "0  Raffles Place, Cecil, Marina, People's Park              Unknown   \n",
      "1  Raffles Place, Cecil, Marina, People's Park              Unknown   \n",
      "2  Raffles Place, Cecil, Marina, People's Park              Unknown   \n",
      "\n",
      "   Region_Classification  \n",
      "0                    NaN  \n",
      "1                    NaN  \n",
      "2                    NaN  \n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "OFFICE DATA - Final shape: (356, 16)\n",
      "RETAIL DATA - Final shape: (3753, 16)\n",
      "\n",
      "KEY FEATURES IN BOTH DATASETS:\n",
      "  Location                  | Office: 356 | Retail: 3753\n",
      "  Postal District_x         | Office: 268 | Retail: 3753\n",
      "  Postal District           | Office: 268 | Retail: 3753\n",
      "  General_Location          | Office: 268 | Retail: 3753\n",
      "  Region_Classification     | Office:   0 | Retail:   0\n",
      "  Urban_Classification      | Office:   0 | Retail: 3753\n",
      "\n",
      " BOTH DATASETS NOW HAVE POSTAL DISTRICT AND LOCATION FEATURES!\n",
      " READY FOR FURTHER ANALYSIS OR MODELING!\n"
     ]
    }
   ],
   "source": [
    "def add_geographic_features_summary(main_df, postal_df, street_df, city_df, stations_df, district_df, dataset_type='office'):\n",
    "    \"\"\"Enrich summary real estate data with geographic features including Postal District and Location\"\"\"\n",
    "    \n",
    "    df_enriched = main_df.copy()\n",
    "    original_count = len(df_enriched)\n",
    "    \n",
    "    print(f\"Adding geographic features to {dataset_type.upper()} summary data ({original_count} rows)...\")\n",
    "    \n",
    "    # --- STRATEGY 1: ENSURE POSTAL DISTRICT FEATURE ---\n",
    "    \n",
    "    # For Office data: Create Postal District from Location if not present\n",
    "    if dataset_type == 'office' and 'Postal District' not in df_enriched.columns:\n",
    "        print(\"Creating Postal District feature for office data...\")\n",
    "        \n",
    "        # Map office locations to approximate postal districts\n",
    "        location_to_district = {\n",
    "            'Central Area': '1',  # Raffles Place\n",
    "            'Central Area Category 1': '1',\n",
    "            'Central Area Category 2': '2',\n",
    "            'Fringe Area': '3',  # Queenstown\n",
    "            'Outside Central Area': '5'  # Pasir Panjang\n",
    "        }\n",
    "        \n",
    "        if 'Location' in df_enriched.columns:\n",
    "            def location_to_postal_district(location):\n",
    "                if pd.isna(location): return np.nan\n",
    "                location_clean = str(location).strip()\n",
    "                for key, district in location_to_district.items():\n",
    "                    if key in location_clean:\n",
    "                        return district\n",
    "                return np.nan\n",
    "            \n",
    "            df_enriched['Postal District'] = df_enriched['Location'].apply(location_to_postal_district)\n",
    "            print(f\"Created Postal District for {df_enriched['Postal District'].notna().sum()} office rows\")\n",
    "    \n",
    "    # For Retail data: Ensure Postal District is properly formatted\n",
    "    elif dataset_type == 'retail' and 'Postal District' in df_enriched.columns:\n",
    "        print(\"Ensuring Postal District format for retail data...\")\n",
    "        df_enriched['Postal District'] = df_enriched['Postal District'].apply(\n",
    "            lambda x: str(int(x)) if pd.notna(x) else np.nan\n",
    "        )\n",
    "    \n",
    "    # --- STRATEGY 2: ENSURE LOCATION FEATURE ---\n",
    "    \n",
    "    # For Retail data: Create Location from Postal District if not present\n",
    "    if dataset_type == 'retail' and 'Location' not in df_enriched.columns:\n",
    "        print(\"Creating Location feature for retail data...\")\n",
    "        \n",
    "        if 'Postal District' in df_enriched.columns:\n",
    "            # Create a mapping from district to general location\n",
    "            district_df['Postal District'] = district_df['Postal District'].astype(str)\n",
    "            district_to_location = district_df.set_index('Postal District')['General Location'].to_dict()\n",
    "            \n",
    "            def postal_to_location(postal_district):\n",
    "                if pd.isna(postal_district): return np.nan\n",
    "                district_str = str(int(float(postal_district))) if '.' in str(postal_district) else str(postal_district)\n",
    "                return district_to_location.get(district_str, 'Unknown')\n",
    "            \n",
    "            df_enriched['Location'] = df_enriched['Postal District'].apply(postal_to_location)\n",
    "            print(f\"Created Location for {df_enriched['Location'].notna().sum()} retail rows\")\n",
    "    \n",
    "    # For Office data: Ensure Location is properly formatted\n",
    "    elif dataset_type == 'office' and 'Location' in df_enriched.columns:\n",
    "        print(\"Ensuring Location format for office data...\")\n",
    "        df_enriched['Location'] = df_enriched['Location'].astype(str)\n",
    "    \n",
    "    # --- STRATEGY 3: COORDINATES BASED ON AVAILABLE DATA ---\n",
    "    \n",
    "    # Create proper postal district string for both datasets\n",
    "    if 'Postal District' in df_enriched.columns:\n",
    "        df_enriched['Postal_District_Str'] = df_enriched['Postal District'].apply(\n",
    "            lambda x: str(int(float(x))) if pd.notna(x) and str(x).replace('.','').isdigit() else 'Unknown'\n",
    "        )\n",
    "    \n",
    "    # Get coordinates from postal districts\n",
    "    if 'Postal_District_Str' in df_enriched.columns and 'postal_code' in postal_df.columns:\n",
    "        print(\"Adding coordinates from postal districts...\")\n",
    "        \n",
    "        postal_df['Postal District'] = postal_df['postal_code'].astype(str).str[:2]\n",
    "        district_coords = postal_df.groupby('Postal District').agg({\n",
    "            'lat': 'mean',\n",
    "            'lon': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        df_enriched = df_enriched.merge(\n",
    "            district_coords.rename(columns={'lat': 'latitude', 'lon': 'longitude'}),\n",
    "            left_on='Postal_District_Str', right_on='Postal District', how='left'\n",
    "        )\n",
    "        \n",
    "        coord_matches = df_enriched['latitude'].notna().sum()\n",
    "        print(f\"Added coordinates: {coord_matches} rows\")\n",
    "    \n",
    "    # --- STRATEGY 4: ADD GENERAL LOCATION FROM DISTRICT MAPPING ---\n",
    "    \n",
    "    if 'Postal_District_Str' in df_enriched.columns:\n",
    "        district_df['Postal District'] = district_df['Postal District'].astype(str)\n",
    "        \n",
    "        df_enriched = df_enriched.merge(\n",
    "            district_df[['Postal District', 'General Location']].rename(columns={'General Location': 'General_Location'}),\n",
    "            left_on='Postal_District_Str', right_on='Postal District', how='left',\n",
    "            suffixes=('', '_from_district')\n",
    "        )\n",
    "        print(f\"Added general location: {df_enriched['General_Location'].notna().sum()} rows\")\n",
    "    \n",
    "    # --- STRATEGY 5: REGION CLASSIFICATION ---\n",
    "    \n",
    "    \n",
    "    # --- STRATEGY 6: MRT DISTANCES ---\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate Haversine distance between two points in kilometers\"\"\"\n",
    "        R = 6371  # Earth radius in kilometers\n",
    "        \n",
    "        lat1_rad = np.radians(lat1)\n",
    "        lon1_rad = np.radians(lon1)\n",
    "        lat2_rad = np.radians(lat2)\n",
    "        lon2_rad = np.radians(lon2)\n",
    "        \n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        \n",
    "        return R * c\n",
    "    \n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        station_coords = stations_df[['Latitude', 'Longitude']].dropna()\n",
    "        \n",
    "        if len(valid_coords) > 0 and len(station_coords) > 0:\n",
    "            print(f\"Calculating MRT distances for {len(valid_coords)} rows...\")\n",
    "            \n",
    "            min_dists = []\n",
    "            for idx, prop_row in valid_coords.iterrows():\n",
    "                prop_lat, prop_lon = prop_row['latitude'], prop_row['longitude']\n",
    "                \n",
    "                distances = []\n",
    "                for _, station_row in station_coords.iterrows():\n",
    "                    dist = haversine_distance(\n",
    "                        prop_lat, prop_lon,\n",
    "                        station_row['Latitude'], station_row['Longitude']\n",
    "                    )\n",
    "                    distances.append(dist)\n",
    "                \n",
    "                min_dists.append(min(distances) if distances else np.nan)\n",
    "            \n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_MRT_km'] = min_dists\n",
    "            print(f\"Added MRT distances: {len([x for x in min_dists if not np.isnan(x)])} rows\")\n",
    "    \n",
    "    # --- STRATEGY 7: URBAN CLASSIFICATION ---\n",
    "    \n",
    "    # CBD proximity (distance to Raffles Place)\n",
    "    cbd_coords = (1.2833, 103.8515)  # Raffles Place\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        if len(valid_coords) > 0:\n",
    "            cbd_distances = []\n",
    "            for idx, row in valid_coords.iterrows():\n",
    "                dist = haversine_distance(\n",
    "                    row['latitude'], row['longitude'],\n",
    "                    cbd_coords[0], cbd_coords[1]\n",
    "                )\n",
    "                cbd_distances.append(dist)\n",
    "            \n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_CBD_km'] = cbd_distances\n",
    "    \n",
    "    def classify_urban_rural(distance_to_cbd):\n",
    "        if pd.isna(distance_to_cbd): return 'Unknown'\n",
    "        if distance_to_cbd <= 5: return 'CBD'\n",
    "        elif distance_to_cbd <= 10: return 'Urban'\n",
    "        elif distance_to_cbd <= 20: return 'Suburban'\n",
    "        else: return 'Rural'\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        df_enriched['Urban_Classification'] = df_enriched['Distance_to_CBD_km'].apply(classify_urban_rural)\n",
    "        print(f\"Added urban classification\")\n",
    "    \n",
    "    # --- FINAL FEATURE CONSOLIDATION ---\n",
    "    \n",
    "    # Ensure both datasets have consistent feature names\n",
    "    print(\"\\nFinal feature consolidation...\")\n",
    "    \n",
    "    # List of expected geographic features\n",
    "    expected_geo_features = [\n",
    "        'Postal District', 'Location', 'General_Location', \n",
    "        'Region_Classification', 'Urban_Classification',\n",
    "        'latitude', 'longitude', 'Distance_to_MRT_km', 'Distance_to_CBD_km'\n",
    "    ]\n",
    "    \n",
    "    # Add missing features as NaN columns\n",
    "    for feature in expected_geo_features:\n",
    "        if feature not in df_enriched.columns:\n",
    "            df_enriched[feature] = np.nan\n",
    "            print(f\"  Added missing feature: {feature}\")\n",
    "    \n",
    "    # --- FINAL REPORT ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"GEOGRAPHIC FEATURE ENGINEERING COMPLETE - {dataset_type.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"CORE FEATURES ADDED:\")\n",
    "    print(f\"  • Postal District: {df_enriched['Postal District'].notna().sum()}/{original_count}\")\n",
    "    print(f\"  • Location: {df_enriched['Location'].notna().sum()}/{original_count}\")\n",
    "    print(f\"  • General Location: {df_enriched['General_Location'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    print(\"\\nGEOGRAPHIC FEATURES:\")\n",
    "    if 'latitude' in df_enriched.columns:\n",
    "        print(f\"  • Coordinates: {df_enriched['latitude'].notna().sum()}/{original_count}\")\n",
    "    if 'Distance_to_MRT_km' in df_enriched.columns:\n",
    "        print(f\"  • MRT Distances: {df_enriched['Distance_to_MRT_km'].notna().sum()}/{original_count}\")\n",
    "    # if 'Region_Classification' in df_enriched.columns:\n",
    "    #     print(f\"  • Region Classification: {(df_enriched['Region_Classification'] != 'Unknown').sum()}/{original_count}\")\n",
    "    if 'Urban_Classification' in df_enriched.columns:\n",
    "        print(f\"  • Urban Classification: {(df_enriched['Urban_Classification'] != 'Unknown').sum()}/{original_count}\")\n",
    "    \n",
    "    # Show sample of added features\n",
    "    print(f\"\\nSAMPLE OF ADDED FEATURES:\")\n",
    "    sample_cols = [col for col in df_enriched.columns if any(x in col.lower() for x in ['postal', 'location', 'region', 'urban'])]\n",
    "    if sample_cols:\n",
    "        print(df_enriched[sample_cols].head(3))\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = [col for col in df_enriched.columns if col in ['Postal_District_Str', 'Postal District_from_district', 'Postal District_y']]\n",
    "    df_enriched = df_enriched.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "# Apply to both datasets\n",
    "print(\"PROCESSING OFFICE AND RETAIL DATA WITH POSTAL DISTRICT & LOCATION FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PROCESSING OFFICE DATA\")\n",
    "print(\"=\"*40)\n",
    "office_enriched = add_geographic_features_summary(\n",
    "    office_df, \n",
    "    postal_codes, \n",
    "    street_coordinates, \n",
    "    city_coordinates, \n",
    "    train_stations, \n",
    "    postal_district_mapping,\n",
    "    dataset_type='office'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"PROCESSING RETAIL DATA\")\n",
    "print(\"=\"*40)\n",
    "retail_enriched = add_geographic_features_summary(\n",
    "    retail_df, \n",
    "    postal_codes, \n",
    "    street_coordinates, \n",
    "    city_coordinates, \n",
    "    train_stations, \n",
    "    postal_district_mapping,\n",
    "    dataset_type='retail'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"OFFICE DATA - Final shape: {office_enriched.shape}\")\n",
    "print(f\"RETAIL DATA - Final shape: {retail_enriched.shape}\")\n",
    "\n",
    "# Show the key features that were added to both datasets\n",
    "print(f\"\\nKEY FEATURES IN BOTH DATASETS:\")\n",
    "common_geo_features = [col for col in office_enriched.columns if col in retail_enriched.columns and any(x in col.lower() for x in ['postal', 'location', 'region', 'urban'])]\n",
    "\n",
    "for feature in common_geo_features:\n",
    "    office_non_null = office_enriched[feature].notna().sum()\n",
    "    retail_non_null = retail_enriched[feature].notna().sum()\n",
    "    print(f\"  {feature:25} | Office: {office_non_null:3} | Retail: {retail_non_null:3}\")\n",
    "\n",
    "print(f\"\\n BOTH DATASETS NOW HAVE POSTAL DISTRICT AND LOCATION FEATURES!\")\n",
    "print(f\" READY FOR FURTHER ANALYSIS OR MODELING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80082b8-77e9-4168-a157-d5781ec1c244",
   "metadata": {},
   "source": [
    "## merge both dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf8a321-1cd8-4c77-90d1-3e5739538f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING AND MERGING OFFICE & RETAIL DATASETS\n",
      "============================================================\n",
      "1. Checking available columns...\n",
      "   Office columns: ['Location', 'Building Class', 'Floor Area (SQM)', '25th Percentile ($ PSM)', 'Median ($PSM)', '75th Percentile ($PSM)', 'Reference Period', 'Postal District_x', 'latitude', 'longitude', 'Postal District', 'General_Location', 'Region_Classification', 'Urban_Classification', 'Distance_to_MRT_km', 'Distance_to_CBD_km']\n",
      "   Retail columns: ['Postal District_x', 'Floor Level', 'Floor Area (SQM)', '25th Percentile ($ PSM)', 'Median ($PSM)', '75th Percentile ($PSM)', 'Reference Period', 'Location', 'latitude', 'longitude', 'Postal District', 'General_Location', 'Distance_to_MRT_km', 'Distance_to_CBD_km', 'Urban_Classification', 'Region_Classification']\n",
      "\n",
      "2. Handling column inconsistencies...\n",
      "   Office columns after renaming: ['Location', 'Building_Class', 'Floor_Area_SQM', 'Percentile_25_PSM', 'Median_PSM', '75th Percentile ($PSM)', 'Reference_Period', 'Postal District_x', 'latitude', 'longitude', 'Postal District', 'General_Location', 'Region_Classification', 'Urban_Classification', 'Distance_to_MRT_km', 'Distance_to_CBD_km']\n",
      "   Retail columns after renaming: ['Postal District_x', 'Floor_Level', 'Floor_Area_SQM', 'Percentile_25_PSM', 'Median_PSM', '75th Percentile ($PSM)', 'Reference_Period', 'Location', 'latitude', 'longitude', 'Postal District', 'General_Location', 'Distance_to_MRT_km', 'Distance_to_CBD_km', 'Urban_Classification', 'Region_Classification']\n",
      "\n",
      "3. Handling Postal District columns...\n",
      "   Office: Found multiple Postal District columns: ['Postal District_x', 'Postal District']\n",
      "   Using 'Postal District_x' with 268 non-null values\n",
      "   Retail: Found multiple Postal District columns: ['Postal District_x', 'Postal District']\n",
      "   Using 'Postal District_x' with 3753 non-null values\n",
      "\n",
      "4. Adding property type identifiers...\n",
      "\n",
      "5. Creating flexible column selection...\n",
      "   Office available columns: 13\n",
      "   Retail available columns: 13\n",
      "\n",
      "6. Handling missing geographic data...\n",
      "   Office: Filling 356 missing coordinates\n",
      "   Retail: Filling 1617 missing coordinates\n",
      "\n",
      "7. Selecting final columns and merging...\n",
      "   Common columns: ['General_Location', 'Median_PSM', 'Reference_Period', 'Property_Type', 'Distance_to_MRT_km', 'longitude', 'latitude', 'Distance_to_CBD_km', 'Percentile_25_PSM', 'Floor_Area_SQM', 'Location', 'Postal_District_Final']\n",
      "   Office final: 356 rows, 12 columns\n",
      "   Retail final: 3753 rows, 12 columns\n",
      "   Merged total: 4109 rows, 12 columns\n",
      "\n",
      "============================================================\n",
      "MERGED DATASET ANALYSIS\n",
      "============================================================\n",
      "Total properties: 4109\n",
      "Office properties: 356\n",
      "Retail properties: 3753\n",
      "\n",
      "Dataset shape: (4109, 12)\n",
      "Columns: ['General_Location', 'Median_PSM', 'Reference_Period', 'Property_Type', 'Distance_to_MRT_km', 'longitude', 'latitude', 'Distance_to_CBD_km', 'Percentile_25_PSM', 'Floor_Area_SQM', 'Location', 'Postal_District_Final']\n",
      "\n",
      "MISSING VALUES SUMMARY:\n",
      "  General_Location: 88 missing (2.1%)\n",
      "  Postal_District_Final: 88 missing (2.1%)\n",
      "\n",
      "PRICE STATISTICS (Median PSM):\n",
      "               count unique     top freq\n",
      "Property_Type                           \n",
      "Office         356.0  322.0   64.58  4.0\n",
      "Retail          3753   3242  100.00   26\n",
      "\n",
      "============================================================\n",
      "SAVING DATASETS\n",
      "============================================================\n",
      " Merged dataset saved: office_retail_merged.csv\n",
      "Office dataset saved: office_cleaned.csv\n",
      " Retail dataset saved: retail_cleaned.csv\n",
      "\n",
      "======================================================================\n",
      " CLEANING AND MERGING COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      " FINAL DATASET: 4109 total properties\n",
      " Office: 356 properties\n",
      "  Retail: 3753 properties\n",
      "\n",
      "SAMPLE OF MERGED DATA:\n",
      "  Property_Type      Location Median_PSM\n",
      "0        Office  Central Area      115.3\n",
      "1        Office  Central Area     121.47\n",
      "2        Office  Central Area     121.63\n",
      "3        Office  Central Area     113.98\n",
      "4        Office  Central Area     126.22\n",
      "5        Office  Central Area      70.11\n",
      "6        Office  Central Area      83.96\n",
      "7        Office  Central Area      82.72\n"
     ]
    }
   ],
   "source": [
    "def clean_and_merge_datasets(office_enriched, retail_enriched):\n",
    "    \"\"\"Clean and merge office and retail datasets with robust column handling\"\"\"\n",
    "    \n",
    "    print(\"CLEANING AND MERGING OFFICE & RETAIL DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create copies to avoid modifying originals\n",
    "    office_clean = office_enriched.copy()\n",
    "    retail_clean = retail_enriched.copy()\n",
    "    \n",
    "    # 1. FIRST, LET'S SEE WHAT COLUMNS WE ACTUALLY HAVE\n",
    "    print(\"1. Checking available columns...\")\n",
    "    print(f\"   Office columns: {list(office_clean.columns)}\")\n",
    "    print(f\"   Retail columns: {list(retail_clean.columns)}\")\n",
    "    \n",
    "    # 2. HANDLE DUPLICATE AND MISSING COLUMNS\n",
    "    print(\"\\n2. Handling column inconsistencies...\")\n",
    "    \n",
    "    # Create consistent column names first\n",
    "    column_standardization = {\n",
    "        # Price columns (handle different naming patterns)\n",
    "        '25th Percentile ($ PSM)': 'Percentile_25_PSM',\n",
    "        '25th Percentile ($ PSM) ': 'Percentile_25_PSM',  # Extra space\n",
    "        'Median ($PSM)': 'Median_PSM',\n",
    "        'Median ($PSM) ': 'Median_PSM',\n",
    "        '75th Percentile ($ PSM)': 'Percentile_75_PSM', \n",
    "        '75th Percentile ($ PSM) ': 'Percentile_75_PSM',\n",
    "        \n",
    "        # Other columns\n",
    "        'Reference Period': 'Reference_Period',\n",
    "        'Floor Area (SQM)': 'Floor_Area_SQM',\n",
    "        'Building Class': 'Building_Class',\n",
    "        'Floor Level': 'Floor_Level'\n",
    "    }\n",
    "    \n",
    "    # Apply renaming to both datasets\n",
    "    office_clean = office_clean.rename(columns=column_standardization)\n",
    "    retail_clean = retail_clean.rename(columns=column_standardization)\n",
    "    \n",
    "    print(f\"   Office columns after renaming: {list(office_clean.columns)}\")\n",
    "    print(f\"   Retail columns after renaming: {list(retail_clean.columns)}\")\n",
    "    \n",
    "    # 3. HANDLE POSTAL DISTRICT DUPLICATES\n",
    "    print(\"\\n3. Handling Postal District columns...\")\n",
    "    \n",
    "    def handle_postal_district(df, dataset_name):\n",
    "        postal_cols = [col for col in df.columns if 'Postal District' in col]\n",
    "        if len(postal_cols) > 1:\n",
    "            print(f\"   {dataset_name}: Found multiple Postal District columns: {postal_cols}\")\n",
    "            # Use the one with more non-null values\n",
    "            non_null_counts = {col: df[col].notna().sum() for col in postal_cols}\n",
    "            best_col = max(non_null_counts, key=non_null_counts.get)\n",
    "            print(f\"   Using '{best_col}' with {non_null_counts[best_col]} non-null values\")\n",
    "            \n",
    "            # Create final postal district column\n",
    "            df['Postal_District_Final'] = df[best_col]\n",
    "            \n",
    "            # Drop the original postal district columns\n",
    "            df = df.drop(postal_cols, axis=1)\n",
    "            df = df.drop('Region_Classification', axis=1)\n",
    "            df = df.drop('Urban_Classification', axis=1)\n",
    "            \n",
    "        elif len(postal_cols) == 1:\n",
    "            df['Postal_District_Final'] = df[postal_cols[0]]\n",
    "            df = df.drop(postal_cols, axis=1)\n",
    "        else:\n",
    "            df['Postal_District_Final'] = np.nan\n",
    "            print(f\"   {dataset_name}: No Postal District columns found\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    office_clean = handle_postal_district(office_clean, 'Office')\n",
    "    retail_clean = handle_postal_district(retail_clean, 'Retail')\n",
    "    \n",
    "    # 4. ADD PROPERTY TYPE IDENTIFIER\n",
    "    print(\"\\n4. Adding property type identifiers...\")\n",
    "    office_clean['Property_Type'] = 'Office'\n",
    "    retail_clean['Property_Type'] = 'Retail'\n",
    "    \n",
    "    # 5. CREATE FLEXIBLE COMMON COLUMNS LIST\n",
    "    print(\"\\n5. Creating flexible column selection...\")\n",
    "    \n",
    "    # Define all possible columns we might want\n",
    "    all_possible_columns = [\n",
    "        'Property_Type', 'Location', 'Postal_District_Final', 'General_Location',\n",
    "        'Region_Classification', 'Urban_Classification', 'Floor_Area_SQM',\n",
    "        'Percentile_25_PSM', 'Median_PSM', 'Percentile_75_PSM', 'Reference_Period',\n",
    "        'latitude', 'longitude', 'Distance_to_MRT_km', 'Distance_to_CBD_km',\n",
    "        'Building_Class', 'Floor_Level'\n",
    "    ]\n",
    "    \n",
    "    # Only select columns that actually exist in each dataset\n",
    "    def get_available_columns(df, possible_columns):\n",
    "        return [col for col in possible_columns if col in df.columns]\n",
    "    \n",
    "    office_columns = get_available_columns(office_clean, all_possible_columns)\n",
    "    retail_columns = get_available_columns(retail_clean, all_possible_columns)\n",
    "    \n",
    "    print(f\"   Office available columns: {len(office_columns)}\")\n",
    "    print(f\"   Retail available columns: {len(retail_columns)}\")\n",
    "    \n",
    "    # 6. HANDLE MISSING GEOGRAPHIC DATA\n",
    "    print(\"\\n6. Handling missing geographic data...\")\n",
    "    \n",
    "    def fill_missing_geo_data(df, property_type):\n",
    "        df_filled = df.copy()\n",
    "        \n",
    "        # Fill missing coordinates\n",
    "        if 'latitude' in df_filled.columns:\n",
    "            missing_coords = df_filled['latitude'].isna().sum()\n",
    "            if missing_coords > 0:\n",
    "                print(f\"   {property_type}: Filling {missing_coords} missing coordinates\")\n",
    "                \n",
    "                if property_type == 'Office':\n",
    "                    default_coords = (1.280, 103.850)  # Raffles Place\n",
    "                else:  # Retail\n",
    "                    default_coords = (1.352, 103.819)  # Regional center\n",
    "                \n",
    "                df_filled['latitude'] = df_filled['latitude'].fillna(default_coords[0])\n",
    "                df_filled['longitude'] = df_filled['longitude'].fillna(default_coords[1])\n",
    "        \n",
    "        # Fill missing distances with reasonable estimates\n",
    "        if 'Distance_to_MRT_km' in df_filled.columns:\n",
    "            missing_mrt = df_filled['Distance_to_MRT_km'].isna().sum()\n",
    "            if missing_mrt > 0:\n",
    "                df_filled['Distance_to_MRT_km'] = df_filled['Distance_to_MRT_km'].fillna(2.0)\n",
    "        \n",
    "        if 'Distance_to_CBD_km' in df_filled.columns:\n",
    "            missing_cbd = df_filled['Distance_to_CBD_km'].isna().sum()\n",
    "            if missing_cbd > 0:\n",
    "                default_cbd = 3.0 if property_type == 'Office' else 6.0\n",
    "                df_filled['Distance_to_CBD_km'] = df_filled['Distance_to_CBD_km'].fillna(default_cbd)\n",
    "        \n",
    "        return df_filled\n",
    "    \n",
    "    office_clean = fill_missing_geo_data(office_clean, 'Office')\n",
    "    retail_clean = fill_missing_geo_data(retail_clean, 'Retail')\n",
    "    \n",
    "    # 7. SELECT FINAL COLUMNS AND MERGE\n",
    "    print(\"\\n7. Selecting final columns and merging...\")\n",
    "    \n",
    "    # Get intersection of columns that exist in both datasets\n",
    "    common_columns = list(set(office_columns) & set(retail_columns))\n",
    "    print(f\"   Common columns: {common_columns}\")\n",
    "    \n",
    "    # Select only common columns for merging\n",
    "    office_final = office_clean[common_columns].copy()\n",
    "    retail_final = retail_clean[common_columns].copy()\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_df = pd.concat([office_final, retail_final], axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"   Office final: {len(office_final)} rows, {len(office_final.columns)} columns\")\n",
    "    print(f\"   Retail final: {len(retail_final)} rows, {len(retail_final.columns)} columns\")\n",
    "    print(f\"   Merged total: {len(merged_df)} rows, {len(merged_df.columns)} columns\")\n",
    "    \n",
    "    return merged_df, office_final, retail_final\n",
    "\n",
    "def analyze_merged_dataset(merged_df):\n",
    "    \"\"\"Analyze the merged dataset\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MERGED DATASET ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if merged_df.empty:\n",
    "        print(\" Merged dataset is empty!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Total properties: {len(merged_df)}\")\n",
    "    print(f\"Office properties: {(merged_df['Property_Type'] == 'Office').sum()}\")\n",
    "    print(f\"Retail properties: {(merged_df['Property_Type'] == 'Retail').sum()}\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {merged_df.shape}\")\n",
    "    print(f\"Columns: {list(merged_df.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMISSING VALUES SUMMARY:\")\n",
    "    missing_data = merged_df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0]\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        for col, count in missing_data.items():\n",
    "            percentage = (count / len(merged_df)) * 100\n",
    "            print(f\"  {col}: {count} missing ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  No missing values!\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    if 'Median_PSM' in merged_df.columns:\n",
    "        print(f\"\\nPRICE STATISTICS (Median PSM):\")\n",
    "        stats = merged_df.groupby('Property_Type')['Median_PSM'].describe()\n",
    "        print(stats)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def save_datasets(merged_df, office_final, retail_final):\n",
    "    \"\"\"Save the cleaned datasets\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING DATASETS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Save merged dataset\n",
    "        merged_filename = 'office_retail_merged.csv'\n",
    "        merged_df.to_csv(merged_filename, index=False)\n",
    "        print(f\" Merged dataset saved: {merged_filename}\")\n",
    "        \n",
    "        # Save individual datasets\n",
    "        office_filename = 'office_cleaned.csv'\n",
    "        retail_filename = 'retail_cleaned.csv'\n",
    "        \n",
    "        office_final.to_csv(office_filename, index=False)\n",
    "        retail_final.to_csv(retail_filename, index=False)\n",
    "        \n",
    "        print(f\"Office dataset saved: {office_filename}\")\n",
    "        print(f\" Retail dataset saved: {retail_filename}\")\n",
    "        \n",
    "        return merged_filename, office_filename, retail_filename\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error saving files: {e}\")\n",
    "        return None, None, None\n",
    "try:\n",
    "    # 1. Clean and merge\n",
    "    merged_df, office_final, retail_final = clean_and_merge_datasets(office_enriched, retail_enriched)\n",
    "    \n",
    "    # 2. Analyze the merged dataset\n",
    "    analysis_result = analyze_merged_dataset(merged_df)\n",
    "    \n",
    "    # 3. Save the results\n",
    "    if not merged_df.empty:\n",
    "        merged_file, office_file, retail_file = save_datasets(merged_df, office_final, retail_final)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" CLEANING AND MERGING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\" FINAL DATASET: {len(merged_df)} total properties\")\n",
    "        print(f\" Office: {(merged_df['Property_Type'] == 'Office').sum()} properties\") \n",
    "        print(f\"  Retail: {(merged_df['Property_Type'] == 'Retail').sum()} properties\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\nSAMPLE OF MERGED DATA:\")\n",
    "        sample_cols = ['Property_Type', 'Location', 'Median_PSM', 'Region_Classification']\n",
    "        available_sample_cols = [col for col in sample_cols if col in merged_df.columns]\n",
    "        print(merged_df[available_sample_cols].head(8))\n",
    "        \n",
    "    else:\n",
    "        print(\" No data to save - merged dataset is empty\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {e}\")\n",
    "    print(\"Trying alternative approach...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70d64b2-19d6-4e6c-95a2-c6c5a10086c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARING DATA FOR MODELING\n",
      "============================================================\n",
      "FIXING DATA ISSUES:\n",
      "==================================================\n",
      "1. Converting Floor_Area_SQM from text to numeric...\n",
      "   Converted 1228 area values\n",
      "2. Creating enhanced features...\n",
      "   Created 4 new features\n",
      "FEATURES FOR MODELING:\n",
      "  Target: Median_PSM\n",
      "  Features: ['Floor_Area_SQM_Numeric', 'Property_Type_Code', 'Year', 'Quarter', 'Distance_to_MRT_km', 'Distance_to_CBD_km', 'latitude', 'longitude']\n",
      "  X shape: (4109, 8), y shape: (4109,)\n",
      " Target: Median_PSM (Price per Square Meter)\n",
      " Samples: 4109 properties\n",
      " Features: 8 numeric features\n"
     ]
    }
   ],
   "source": [
    "def fix_data_issues(merged_df):\n",
    "    \"\"\"Fix issues in the merged dataset before modeling\"\"\"\n",
    "    \n",
    "    df_fixed = merged_df.copy()\n",
    "    \n",
    "    print(\"FIXING DATA ISSUES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Fix Floor_Area_SQM - it's currently text ranges, not numbers\n",
    "    print(\"1. Converting Floor_Area_SQM from text to numeric...\")\n",
    "    \n",
    "    def parse_floor_area(area_text):\n",
    "        if pd.isna(area_text):\n",
    "            return np.nan\n",
    "        text = str(area_text).lower()\n",
    "        \n",
    "        if 'below' in text or '& below' in text:\n",
    "            return 50  # Assume average of small units\n",
    "        elif '>' in text and '-' in text:\n",
    "            # Extract numbers from ranges like \">100 - 200\"\n",
    "            numbers = [int(s) for s in text.split() if s.isdigit()]\n",
    "            if len(numbers) == 2:\n",
    "                return (numbers[0] + numbers[1]) / 2  # Return midpoint\n",
    "        elif '>' in text:\n",
    "            # For \">1000\", return a reasonable large value\n",
    "            numbers = [int(s) for s in text.split() if s.isdigit()]\n",
    "            if numbers:\n",
    "                return numbers[0] * 1.5  # 50% above the threshold\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    df_fixed['Floor_Area_SQM_Numeric'] = df_fixed['Floor_Area_SQM'].apply(parse_floor_area)\n",
    "    print(f\"   Converted {df_fixed['Floor_Area_SQM_Numeric'].notna().sum()} area values\")\n",
    "    \n",
    "    # 2. Create better features\n",
    "    print(\"2. Creating enhanced features...\")\n",
    "    \n",
    "    # Property type as numeric\n",
    "    df_fixed['Property_Type_Code'] = df_fixed['Property_Type'].map({'Office': 1, 'Retail': 0})\n",
    "    \n",
    "    # Extract year and quarter from Reference_Period\n",
    "    df_fixed['Year'] = df_fixed['Reference_Period'].str[:4].astype(int)\n",
    "    df_fixed['Quarter'] = df_fixed['Reference_Period'].str[-1].astype(int)\n",
    "    \n",
    "    # Region classification as numeric\n",
    "    region_mapping = {\n",
    "        'Central Core': 4,\n",
    "        'Rest Central': 3, \n",
    "        'City Fringe': 2,\n",
    "        'Outside Central': 1,\n",
    "        'Unknown': 0\n",
    "    }\n",
    "    # df_fixed['Region_Code'] = df_fixed['Region_Classification'].map(region_mapping)\n",
    "    \n",
    "    print(f\"   Created {df_fixed.shape[1] - merged_df.shape[1]} new features\")\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# Prepare features for modeling\n",
    "def prepare_modeling_features(df_fixed, target_column='Median_PSM'):\n",
    "    \"\"\"Prepare features for machine learning\"\"\"\n",
    "    \n",
    "    # Select numeric features for modeling\n",
    "    feature_columns = [\n",
    "        'Floor_Area_SQM_Numeric', 'Property_Type_Code', 'Region_Code',\n",
    "        'Year', 'Quarter', 'Distance_to_MRT_km', 'Distance_to_CBD_km',\n",
    "        'latitude', 'longitude'\n",
    "    ]\n",
    "    \n",
    "    # Only use features that exist and have data\n",
    "    available_features = [col for col in feature_columns if col in df_fixed.columns and df_fixed[col].notna().any()]\n",
    "    \n",
    "    X = df_fixed[available_features].fillna(df_fixed[available_features].median())\n",
    "    y = df_fixed[target_column]\n",
    "    \n",
    "    print(f\"FEATURES FOR MODELING:\")\n",
    "    print(f\"  Target: {target_column}\")\n",
    "    print(f\"  Features: {available_features}\")\n",
    "    print(f\"  X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    return X, y, available_features\n",
    "\n",
    "# Execute the fixes\n",
    "print(\"PREPARING DATA FOR MODELING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fix data issues\n",
    "df_fixed = fix_data_issues(merged_df)\n",
    "\n",
    "# Prepare features\n",
    "X, y, feature_names = prepare_modeling_features(df_fixed, 'Median_PSM')\n",
    "\n",
    "\n",
    "print(f\" Target: Median_PSM (Price per Square Meter)\")\n",
    "print(f\" Samples: {len(X)} properties\")\n",
    "print(f\" Features: {len(feature_names)} numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf07a8b-1897-4a7c-9920-53b5300fcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column='Median_PSM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ecc8594-ac28-4244-b447-991846fcfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in merged_df.columns]\n",
    "categorical_columns = merged_df[feature_columns].select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = merged_df[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_encoded = pd.get_dummies(merged_df[feature_columns], columns=categorical_columns, drop_first=True)\n",
    "y = merged_df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4bd3ee-b04c-4b5f-abbe-9dde68e67327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking target column data types:\n",
      "Target column: Median_PSM\n",
      "Data type: object\n",
      "Sample values:\n",
      "0     115.3\n",
      "1    121.47\n",
      "2    121.63\n",
      "3    113.98\n",
      "4    126.22\n",
      "5     70.11\n",
      "6     83.96\n",
      "7     82.72\n",
      "8     91.49\n",
      "9     54.36\n",
      "Name: Median_PSM, dtype: object\n",
      "Unique values: [115.3 121.47 121.63 113.98 126.22 70.11 83.96 82.72 91.49 54.36]\n",
      "Found 1 non-numeric values in target column\n",
      "Non-numeric values:\n",
      "['1,017.50']\n",
      "\n",
      "After conversion:\n",
      "Target stats - Min: $12.75, Max: $964.19, Mean: $151.35\n",
      "Missing values after conversion: 1\n",
      "\n",
      "After cleaning:\n",
      "X_clean shape: (4108, 3675)\n",
      "y_clean shape: (4108,)\n",
      "\n",
      "Training set: (3286, 3675)\n",
      "Testing set: (822, 3675)\n",
      "\n",
      "Baseline Model (Mean Prediction):\n",
      "MAE: $80.15\n",
      "RMSE: $109.96\n",
      "\n",
      "Training Random Forest model...\n",
      "Random Forest Model:\n",
      "MAE: $21.39\n",
      "RMSE: $37.89\n",
      "R²: 0.8812\n",
      "Improvement over baseline: +73.3%\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what's actually in your target column\n",
    "print(\"Checking target column data types:\")\n",
    "print(f\"Target column: {target_column}\")\n",
    "print(f\"Data type: {merged_df[target_column].dtype}\")\n",
    "print(f\"Sample values:\")\n",
    "print(merged_df[target_column].head(10))\n",
    "print(f\"Unique values: {merged_df[target_column].unique()[:10]}\")  # First 10 unique values\n",
    "\n",
    "# Convert target column to numeric, handling any non-numeric values\n",
    "def safe_numeric_conversion(series):\n",
    "    \"\"\"Safely convert series to numeric, handling errors\"\"\"\n",
    "    # First try direct conversion\n",
    "    converted = pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Check if we have any non-numeric values\n",
    "    non_numeric_count = series.size - converted.notna().sum()\n",
    "    if non_numeric_count > 0:\n",
    "        print(f\"Found {non_numeric_count} non-numeric values in target column\")\n",
    "        print(\"Non-numeric values:\")\n",
    "        non_numeric_mask = pd.to_numeric(series, errors='coerce').isna()\n",
    "        print(series[non_numeric_mask].unique())\n",
    "    \n",
    "    return converted\n",
    "\n",
    "# Apply conversion\n",
    "y_numeric = safe_numeric_conversion(merged_df[target_column])\n",
    "\n",
    "print(f\"\\nAfter conversion:\")\n",
    "print(f\"Target stats - Min: ${y_numeric.min():.2f}, Max: ${y_numeric.max():.2f}, Mean: ${y_numeric.mean():.2f}\")\n",
    "print(f\"Missing values after conversion: {y_numeric.isna().sum()}\")\n",
    "\n",
    "# Remove rows where target is NaN after conversion\n",
    "valid_mask = y_numeric.notna()\n",
    "X_clean = X_encoded[valid_mask]\n",
    "y_clean = y_numeric[valid_mask]\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"X_clean shape: {X_clean.shape}\")\n",
    "print(f\"y_clean shape: {y_clean.shape}\")\n",
    "\n",
    "# Now proceed with train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "# Create a simple baseline (predict mean) - THIS SHOULD WORK NOW\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "\n",
    "print(f\"\\nBaseline Model (Mean Prediction):\")\n",
    "print(f\"MAE: ${baseline_mae:,.2f}\")\n",
    "print(f\"RMSE: ${baseline_rmse:,.2f}\")\n",
    "\n",
    "# Now train a simple model to test\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(f\"\\nTraining Random Forest model...\")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "model_mae = mean_absolute_error(y_test, y_pred)\n",
    "model_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "model_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Random Forest Model:\")\n",
    "print(f\"MAE: ${model_mae:,.2f}\")\n",
    "print(f\"RMSE: ${model_rmse:,.2f}\")\n",
    "print(f\"R²: {model_r2:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = ((baseline_mae - model_mae) / baseline_mae) * 100\n",
    "print(f\"Improvement over baseline: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a279092c-334f-42e8-81a8-14806fc2e227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      "0\n",
      "Missing values in target:\n",
      "0\n",
      "Missing values after imputation - Train: 0\n",
      "Missing values after imputation - Test: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Check for missing values\n",
    "print(\"Missing values in features:\")\n",
    "print(X_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in target:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(X, strategy='mean'):\n",
    "    \"\"\"Handle missing values in the feature matrix\"\"\"\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "# Apply to training and test data\n",
    "X_train_imputed = handle_missing_values(X_train)\n",
    "X_test_imputed = handle_missing_values(X_test)\n",
    "\n",
    "print(f\"Missing values after imputation - Train: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after imputation - Test: {X_test_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e3b5f-2240-4dbe-a24d-3713cec768ee",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ada2a374-36c8-4114-aa70-8368d8e31018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in y: 356\n",
      "NaN values in y_train: 0\n",
      "Removed NaN values from y\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Results:\n",
      "  MAE: $21.39\n",
      "  RMSE: $37.89\n",
      "  R²: 0.8812\n",
      "  Improvement over baseline: 73.3%\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "Hist Gradient Boosting Results:\n",
      "  MAE: $22.98\n",
      "  RMSE: $37.02\n",
      "  R²: 0.8866\n",
      "  Improvement over baseline: 71.3%\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost Results:\n",
      "  MAE: $22.45\n",
      "  RMSE: $38.15\n",
      "  R²: 0.8796\n",
      "  Improvement over baseline: 72.0%\n",
      "\n",
      "Training CatBoost...\n",
      "Error training CatBoost: Found input variables with inconsistent numbers of samples: [4109, 3753]\n",
      "\n",
      "Best Model: Random Forest\n",
      "Best MAE: $21.39\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Check if y needs cleaning (only if it's string type)\n",
    "if y.dtype == 'object':\n",
    "    y = y.str.replace(',', '').astype(float)\n",
    "else:\n",
    "    print(f\"y is already numeric (dtype: {y.dtype})\")\n",
    "\n",
    "# Check for NaN values\n",
    "print(f\"NaN values in y: {y.isna().sum()}\")\n",
    "print(f\"NaN values in y_train: {y_train.isna().sum()}\")\n",
    "\n",
    "# Remove NaN values if any\n",
    "if y.isna().sum() > 0:\n",
    "    y = y.dropna()\n",
    "    print(f\"Removed NaN values from y\")\n",
    "\n",
    "if y_train.isna().sum() > 0:\n",
    "    y_train = y_train.dropna()\n",
    "    print(f\"Removed NaN values from y_train\")\n",
    "\n",
    "# Convert feature data to numpy arrays to avoid XGBoost dtype issues\n",
    "X_train_array = np.array(X_train_imputed)\n",
    "X_test_array = np.array(X_test_imputed)\n",
    "\n",
    "# Make sure y_train and y_test align with the feature arrays\n",
    "# If you need to recreate splits to ensure alignment:\n",
    "# X_train_array, X_test_array, y_train, y_test = train_test_split(\n",
    "#     X_imputed, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# Models definition\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingRegressor(random_state=42, max_iter=100),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(n_estimators=100, random_seed=42, verbose=False, allow_writing_files=False)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "        if name == 'CatBoost':\n",
    "            # For CatBoost, use categorical features if available\n",
    "            if 'categorical_columns' in locals() and 'feature_columns' in locals():\n",
    "                cat_features_list = list(categorical_columns)\n",
    "                X_cat = merged_df[feature_columns].copy()\n",
    "                \n",
    "                # Convert categorical columns to string for CatBoost\n",
    "                for col in cat_features_list:\n",
    "                    if col in X_cat.columns:\n",
    "                        X_cat[col] = X_cat[col].astype(str)\n",
    "                \n",
    "                # Split for CatBoost\n",
    "                X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "                    X_cat, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "                \n",
    "                # Remove NaN from CatBoost target\n",
    "                nan_mask = ~y_train_cat.isna()\n",
    "                X_train_cat = X_train_cat[nan_mask]\n",
    "                y_train_cat = y_train_cat[nan_mask]\n",
    "                \n",
    "                # Identify categorical feature indices for CatBoost\n",
    "                cat_features_indices = [i for i, col in enumerate(X_cat.columns) if col in cat_features_list]\n",
    "                \n",
    "                model.fit(X_train_cat, y_train_cat, cat_features=cat_features_indices)\n",
    "                y_pred = model.predict(X_test_cat)\n",
    "                y_test_compare = y_test_cat\n",
    "            else:\n",
    "                # Fallback: use numpy arrays if categorical features not defined\n",
    "                model.fit(X_train_array, y_train)\n",
    "                y_pred = model.predict(X_test_array)\n",
    "                y_test_compare = y_test\n",
    "            \n",
    "        else:\n",
    "            # Use numpy arrays for other models\n",
    "            model.fit(X_train_array, y_train)\n",
    "            y_pred = model.predict(X_test_array)\n",
    "            y_test_compare = y_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test_compare, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_compare, y_pred))\n",
    "        r2 = r2_score(y_test_compare, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'predictions': y_pred,\n",
    "            'y_test': y_test_compare\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  MAE: ${mae:,.2f}\")\n",
    "        print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        if 'baseline_mae' in locals():\n",
    "            print(f\"  Improvement over baseline: {((baseline_mae - mae) / baseline_mae * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Find best model\n",
    "if results:\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mae'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    print(f\"Best MAE: ${results[best_model_name]['mae']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fad49742-4f9e-4e9f-80a8-8d0a709f1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced training pipeline...\n",
      "Starting Enhanced Model Training Pipeline\n",
      "============================================================\n",
      "Training enhanced models with improved configurations...\n",
      "============================================================\n",
      "\n",
      "--- Training Hist Gradient Boosting ---\n",
      "Test MAE: $23.16\n",
      "Test RMSE: $37.04\n",
      "Test R²: 0.8864\n",
      "CV MAE: $25.06 ± $0.85\n",
      "Improvement over baseline: +71.1%\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Error training XGBoost: 'DataFrame' object has no attribute 'dtype'\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $21.60\n",
      "Test RMSE: $36.82\n",
      "Test R²: 0.8878\n",
      "CV MAE: $22.95 ± $0.57\n",
      "Improvement over baseline: +73.1%\n",
      "\n",
      "--- Training CatBoost ---\n",
      "Error training CatBoost: catboost/libs/data/features_layout.cpp:124: All feature names should be different, but 'Median_PSM_102.22' used more than once.\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Model Ranking (by MAE):\n",
      " 1. Gradient Boosting         | MAE: $21.60 | R²: 0.8878 | CV: $22.95 ± $0.57\n",
      " 2. Hist Gradient Boosting    | MAE: $23.16 | R²: 0.8864 | CV: $25.06 ± $0.85\n",
      "\n",
      "--- Error Analysis for Gradient Boosting ---\n",
      "Mean Absolute Error: $21.60\n",
      "Median Absolute Error: $12.50\n",
      "Max Error: $278.09\n",
      "Error Std: $29.84\n",
      "\n",
      "Error Percentage Stats:\n",
      "  Mean: 14.5%\n",
      "  Median: 11.2%\n",
      "  95th percentile: 37.6%\n",
      "\n",
      "Top 5 Largest Errors:\n",
      "  Actual: $639 | Predicted: $438 | Error: 31.6%\n",
      "  Actual: $410 | Predicted: $174 | Error: 57.7%\n",
      "  Actual: $648 | Predicted: $394 | Error: 39.3%\n",
      "  Actual: $511 | Predicted: $246 | Error: 51.9%\n",
      "  Actual: $353 | Predicted: $75 | Error: 78.7%\n",
      "\n",
      "Creating ensemble from: ['Gradient Boosting', 'Hist Gradient Boosting']\n",
      "Ensemble weights:\n",
      "  Gradient Boosting: 0.517\n",
      "  Hist Gradient Boosting: 0.483\n",
      "\n",
      "Ensemble Results:\n",
      "MAE: $21.91\n",
      "RMSE: $36.14\n",
      "R²: 0.8919\n",
      "Ensemble vs Best Single Model: -1.5% improvement\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      " RECOMMENDATION: USE Gradient Boosting\n",
      "   MAE: $21.60\n",
      "   R²: 0.8878\n",
      "   CV Consistency: $22.95 ± $0.57\n",
      "Final model type: GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Enhanced model training with better validation and ensemble options\"\"\"\n",
    "    \n",
    "    print(\"Training enhanced models with improved configurations...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Enhanced models with optimized hyperparameters\n",
    "    models = {\n",
    "        # 'Random Forest': RandomForestRegressor(\n",
    "        #     n_estimators=200,\n",
    "        #     max_depth=20,\n",
    "        #     min_samples_split=10,\n",
    "        #     min_samples_leaf=4,\n",
    "        #     max_features='sqrt',\n",
    "        #     bootstrap=True,\n",
    "        #     random_state=42,\n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "        'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_leaf=20,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add CatBoost if you have categorical features\n",
    "    try:\n",
    "        models['CatBoost'] = CatBoostRegressor(\n",
    "            iterations=200,\n",
    "            depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    except:\n",
    "        print(\"CatBoost not available, skipping...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_imputed, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RMSE: ${rmse:,.2f}\")\n",
    "            print(f\"Test R²: {r2:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} ± ${cv_std:,.2f}\")\n",
    "            \n",
    "            if baseline_mae:\n",
    "                improvement = ((baseline_mae - mae) / baseline_mae * 100)\n",
    "                print(f\"Improvement over baseline: {improvement:+.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Create intelligent ensemble from best performing models\"\"\"\n",
    "    \n",
    "    # Get top 3 models by MAE\n",
    "    valid_models = {k: v for k, v in results.items() if 'mae' in v}\n",
    "    if len(valid_models) < 2:\n",
    "        print(\"Not enough models for ensemble\")\n",
    "        return None, None\n",
    "    \n",
    "    top_models = sorted(valid_models.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    \n",
    "    print(f\"\\nCreating ensemble from: {[name for name, _ in top_models]}\")\n",
    "    \n",
    "    # Create weighted ensemble based on performance\n",
    "    ensemble_models = []\n",
    "    weights = []\n",
    "    \n",
    "    for name, result in top_models:\n",
    "        ensemble_models.append((name, result['model']))\n",
    "        # Weight inversely proportional to MAE (better models get higher weight)\n",
    "        weight = 1.0 / result['mae']\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w/total_weight for w in weights]\n",
    "    \n",
    "    print(\"Ensemble weights:\")\n",
    "    for (name, _), weight in zip(ensemble_models, normalized_weights):\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "    \n",
    "    # Create voting regressor with weights\n",
    "    ensemble = VotingRegressor(\n",
    "        estimators=ensemble_models,\n",
    "        weights=normalized_weights\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.fit(X_train_imputed, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test_imputed)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    mae_ens = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "    rmse_ens = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "    r2_ens = r2_score(y_test, y_pred_ensemble)\n",
    "    \n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"MAE: ${mae_ens:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse_ens:,.2f}\")\n",
    "    print(f\"R²: {r2_ens:.4f}\")\n",
    "    \n",
    "    # Compare with best single model\n",
    "    best_single_mae = top_models[0][1]['mae']\n",
    "    improvement = ((best_single_mae - mae_ens) / best_single_mae) * 100\n",
    "    \n",
    "    print(f\"Ensemble vs Best Single Model: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    return ensemble, mae_ens\n",
    "\n",
    "def analyze_model_performance(results, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Comprehensive model performance analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort models by MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])\n",
    "    \n",
    "    print(\"\\nModel Ranking (by MAE):\")\n",
    "    for i, (name, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i:2d}. {name:25} | MAE: ${result['mae']:,.2f} | R²: {result['r2']:.4f} | CV: ${result['cv_mae']:,.2f} ± ${result['cv_std']:,.2f}\")\n",
    "    \n",
    "    best_model_name, best_result = sorted_results[0]\n",
    "    \n",
    "    # Error analysis for best model\n",
    "    print(f\"\\n--- Error Analysis for {best_model_name} ---\")\n",
    "    y_pred_best = best_result['predictions']\n",
    "    errors = np.abs(y_test - y_pred_best)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: ${errors.mean():,.2f}\")\n",
    "    print(f\"Median Absolute Error: ${np.median(errors):,.2f}\")\n",
    "    print(f\"Max Error: ${errors.max():,.2f}\")\n",
    "    print(f\"Error Std: ${errors.std():,.2f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_pct = (errors / y_test) * 100\n",
    "    print(f\"\\nError Percentage Stats:\")\n",
    "    print(f\"  Mean: {error_pct.mean():.1f}%\")\n",
    "    print(f\"  Median: {error_pct.median():.1f}%\")\n",
    "    print(f\"  95th percentile: {np.percentile(error_pct, 95):.1f}%\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    worst_indices = np.argsort(errors)[-5:]\n",
    "    print(f\"\\nTop 5 Largest Errors:\")\n",
    "    for idx in worst_indices:\n",
    "        actual = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]\n",
    "        predicted = y_pred_best[idx]\n",
    "        error = abs(actual - predicted)\n",
    "        error_pct = (error / actual) * 100\n",
    "        print(f\"  Actual: ${actual:,.0f} | Predicted: ${predicted:,.0f} | Error: {error_pct:.1f}%\")\n",
    "    \n",
    "    return best_model_name, best_result\n",
    "\n",
    "# Main execution function\n",
    "def run_enhanced_training(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Complete enhanced training pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting Enhanced Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Train individual models\n",
    "    results = train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No models trained successfully!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. Analyze performance\n",
    "    best_model_name, best_result = analyze_model_performance(results, X_test_imputed, y_test)\n",
    "    \n",
    "    # 3. Create ensemble\n",
    "    ensemble_model, ensemble_mae = create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test)\n",
    "    \n",
    "    # 4. Final recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ensemble_model and ensemble_mae < best_result['mae']:\n",
    "        print(\" RECOMMENDATION: USE ENSEMBLE MODEL\")\n",
    "        print(f\"   Ensemble MAE: ${ensemble_mae:,.2f}\")\n",
    "        print(f\"   Best Single MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Improvement: {((best_result['mae'] - ensemble_mae) / best_result['mae'] * 100):+.1f}%\")\n",
    "        final_model = ensemble_model\n",
    "    else:\n",
    "        print(f\" RECOMMENDATION: USE {best_model_name}\")\n",
    "        print(f\"   MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   R²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV Consistency: ${best_result['cv_mae']:,.2f} ± ${best_result['cv_std']:,.2f}\")\n",
    "        final_model = best_result['model']\n",
    "    \n",
    "    return final_model, results\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "# Usage - replace your existing training code with this:\n",
    "print(\"Running enhanced training pipeline...\")\n",
    "final_model, all_results = run_enhanced_training(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    ")\n",
    "\n",
    "# You can then use final_model for predictions\n",
    "if final_model:\n",
    "    #print(\"\\nTraining completed successfully!\")\n",
    "    print(f\"Final model type: {type(final_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bd28b3-2ef0-4501-bddc-57bf6a16e358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete data cleaning and model rebuilding...\n",
      "STARTING DATA CLEANING AND MODEL REBUILDING\n",
      "============================================================\n",
      "CLEANING DATASET AND REBUILDING MODELS\n",
      "============================================================\n",
      "1. Removing constant features...\n",
      "   Removed 3571 constant features\n",
      "   Remaining features: 104\n",
      "2. Removing highly correlated features...\n",
      "   Removed 45 highly correlated features\n",
      "   Final features: 59\n",
      "\n",
      "3. TRAINING MODELS ON CLEANED DATA\n",
      "============================================================\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $23.13\n",
      "Test R²: 0.8812\n",
      "Train R²: 0.9181\n",
      "Overfit Gap: 0.0370\n",
      "CV MAE: $24.74 ± $0.66\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $23.39\n",
      "Test R²: 0.8762\n",
      "Train R²: 0.9504\n",
      "Overfit Gap: 0.0743\n",
      "CV MAE: $25.41 ± $0.33\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL RESULTS\n",
      "============================================================\n",
      "Gradient Boosting    | MAE: $23.13 | R²: 0.8812 | Overfit: 0.0370\n",
      "XGBoost              | MAE: $23.39 | R²: 0.8762 | Overfit: 0.0743\n",
      "\n",
      "BEST FINAL MODEL: Gradient Boosting\n",
      "   Test MAE: $23.13\n",
      "   Test R²: 0.8812\n",
      "   CV MAE: $24.74 ± $0.66\n",
      "   Overfit Gap: 0.0370\n",
      "\n",
      "4. FEATURE IMPORTANCE ANALYSIS (Top 20)\n",
      "============================================================\n",
      "       feature  importance\n",
      "2    feature_2    0.928879\n",
      "45  feature_45    0.014064\n",
      "1    feature_1    0.009794\n",
      "0    feature_0    0.008194\n",
      "6    feature_6    0.006616\n",
      "23  feature_23    0.003383\n",
      "17  feature_17    0.003291\n",
      "50  feature_50    0.003029\n",
      "47  feature_47    0.002618\n",
      "21  feature_21    0.001715\n",
      "18  feature_18    0.001694\n",
      "43  feature_43    0.001431\n",
      "39  feature_39    0.001376\n",
      "51  feature_51    0.000991\n",
      "42  feature_42    0.000912\n",
      "57  feature_57    0.000885\n",
      "33  feature_33    0.000738\n",
      "58  feature_58    0.000714\n",
      "44  feature_44    0.000707\n",
      "34  feature_34    0.000638\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMWCAYAAADs4eXxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAl/VJREFUeJzs3Ql8lOW5//8rbJGyB0FWZd8EAooiyGJ7pIKojbSCCAKKwJFCPSxqsdCCbIdGWmkLAkZ2CwKCcEChglBkERHB0rIVLNKjWBALiCAg5P/6Xuc385/EhCQkD5mEz/v1mjPL8+See4ZpT7/PdS8xycnJyQYAAAAAAHJcgZxvEgAAAAAACKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAV11MTEymbuvXrw+0H//85z9t1KhRdvvtt1uZMmXs+uuvt7vuusvWrFmT5vknTpywvn37Wrly5axYsWL2/e9/3z788MNMvZfa1WeqXbt2msfffvvt8OdevHixBeHNN9+0kSNHZvp89blhw4aWV3322Wf+eXfu3Bn4e505c8bfK7O/WZ2X3u/+4YcfDqSPu3fv9j4eOnQokPYBAGkrlM7rAAAEZu7cuSmez5kzx0Nn6tfr168faD+WLVtmEyZMsISEBOvZs6d9++233pd27drZjBkz7LHHHgufe+nSJevYsaN99NFH9vTTT3tAnzJligfT7du3pxumI1133XV24MABe//99z3oR3r11Vf9+DfffGNBUeiePHlyloJ3XqbQrYsq1apVsyZNmgQeuvVeot9EZv3sZz+z2267LcVr6m9QoVt9VP+Ceg8AwHcRugEAV1337t1TPH/vvfc8dKd+PWiqVB8+fNgDdMh//ud/ekD75S9/mSJ0q/q8efNmW7Rokf3kJz/x1zp37mx16tSxX/3qV/bHP/4xw/erWbOmB/v58+enCN0K2kuXLvVQ//rrr+f457zW6DvWRZK8oHXr1uHfU1719ddf+8gPAEDaGF4OAIja/yE/ZMgQq1q1qsXGxlrdunXthRdesOTk5BTnaTjugAEDvFKsc1QtvvXWW23Dhg0ZvsfNN9+cInCL3uvee++1//3f/7WvvvoqRei+4YYbrFOnTuHXNMxcwVsV83PnzmXqc3Xt2tVee+21FKHwf/7nf7xSqrbSsmPHDuvQoYOVLFnSihcvbv/xH//hFyoiXbhwwauYqrjrOyhbtqy1atXKL2ZIr169vMod+s5Ct6wKfd+6+NCgQQMrWrSotWjRwnbt2uXHp02bZrVq1fI+qKKaeihzaMi6Rge0bNnS/7569eo2derU77zX0aNHrXfv3v69q734+HibPXt2inPUvvqk38aLL77oFzb0b6hRCKEKsi6ehD7vrFmz/LV3333XHnroIbvxxhv9fP3OBg0aZGfPnk3Rvr43feeffvqpj4jQY/27Dx061C5evBjug14T/RuE3isnRhRs3brV2rdvb6VKlbLvfe971rZtW9u0aVOKcz755BPr37+///71ferfXp8t8rvX59ZroYtNqadwpNdfVcT1HUS2o3P//Oc/+3uWL1/eqlSpEj7+1ltv+YUEhfASJUr4haS//e1vKdr8/PPP/d9Ef6fvvmLFivajH/2IYe8A8i0q3QCAqKNg/cADD9i6des8dKnyvHr1ah/WrfDz29/+NsX5CgAKshqqGwpcCioaxn0lc5IVChRwdIsMvrfccosVKJDyerUq1tOnT7f9+/dbo0aNMmz7kUceCc/9/cEPfuCvqUquIK0Ak5oCi0KMAvczzzxjhQsX9mCr8KrP3bx5cz9PbY4fP96eeOIJ79OpU6fsgw8+8DnnGi7fr18/H26d1jD+rFJgXb58uf30pz/153rf++67z/un715h7N///rf9+te/tscff9zeeeedFH+vY7qwoYsMugixcOFCe/LJJ61IkSJ+vij86jNqOL5CvoK5gr4CoObWP/XUUynanDlzpo8Y0Jx7/QYefPBBv2iiEQt6Td+hKOiL2tKFDr2vQqp+K7///e/9YouORVK4vueee/y7VrjXnP+JEyd6wNffK3C/9NJL/ljvG7ow07hx4wy/S/Xxiy++SPFaXFyc/870veliiy4iaTSFXtPn1O9G/wah0RLbtm3zURiaC64gq/Cq/uj705By/Y7btGnj//n43e9+Z88991x46saVTuHQv7E+t75fXSAT/a40TUPflaZt6PtVP3TxR//5CQ1p//GPf+y/64EDB/prurii36VGnTDsHUC+lAwAQC776U9/qvJ1+Pkbb7zhz8eMGZPivJ/85CfJMTExyQcOHAi/pvN0++CDD8KvffLJJ8nXXXdd8oMPPpjlvvz973/3v3300UdTvF6sWLHkxx9//Dvnr1y50t9/1apVl223bdu2yTfffLM/btasWXLv3r398b///e/kIkWKJM+ePTt53bp13taiRYvCf5eQkODHDx48GH7ts88+Sy5RokRymzZtwq/Fx8cnd+zYMUvfc0Yi+xyiv4+NjU3+xz/+EX5t2rRp/nqFChWST506FX592LBh/nrkuWpTr02cODH82rlz55KbNGmSXL58+eTz58/7ay+++KKfN2/evPB5OtaiRYvk4sWLh99Hbeu8kiVLJh89ejRFX7dt2+bHZs6c+Z3PdubMme+8Nn78eP996fcT0rNnT2/j+eefT3Fu06ZNk2+99dbw82PHjvl5v/rVr5IzI/RvndZNn+nSpUvJtWvXTr7nnnv8cWS/q1evntyuXbvLfpYtW7Z4W3PmzAm/pt+VXtN7p5Ze32+66Sb/DkL0XercVq1aJX/77bfh17/66qvk0qVLJ/fp0yfF33/++efJpUqVCr+u37v+PjExMVPfEwDkBwwvBwBEHS34VbBgQa/MRdJwc+UDDWGNpOHNqgaGaMiwhquqOh4aApwZqsxpCK6G6P73f/93imOqvKqCmpqGPYeOZ5aq3UuWLLHz58/7sHV9VlVIU1Pf//SnP/mw5ho1aoRf13BctbFx40avaEvp0qW9evj3v//dgqaqfGRFMlRtVwVTQ4pTv/7xxx+n+PtChQp55T1EFW49V8VTw85Dv4EKFSp4JTxEVX79Jk6fPu1V/kh679AQ78zQv3GIKrWqNqsKrt+XqrKpaa5/JFXOU3+uK6FKsaq8kTd9bq24rn9L/TsfP37c+6eb+qrvX9MnQlMUIj+LphnofA3x128is6vrZ1WfPn38dxuifmsEgv69Qn3VTefod6BRK6G+6t9bIz004gEArgUMLwcARB3NUa1UqVKKABc5FFbHI6W1crgWOFOIPnbsmIeYjCjganiuhuMq1Ov9IykspDVvO7TaeGTwyYjeR3OC9T6ai66h2ak/q6jv+gyaq5uavguFLm17prnpzz//vF9o0OfWkHoNr3/00UczNcQ5q3RRI5LmG4vmRaf1eupwpe829cJb6rdoaPQdd9zh/8b6d009nD+934CGn2eFhjIr8GqYfOr+nTx58jsXVlIHem0xlxOhUVMS7r777u+8Hrp4ouHa6VE/1Q9d8NEQfw091/SLyHUPUn+WnJL6+w71NzRlIjVNjxBduNLQc11A01x9/Vvr99+jR49M/ecUAPIiQjcAAP+vcrdixQoPwWkFB1WXjxw58p3XQ6+lDumXo7Y031bzgrUoVk6sWK45uwcPHvRF3VQdT0pK8rnvWqBM87xzUmSFMzOvp178LghZueihCyya5/7ll1/as88+a/Xq1fOLAAqsmjOeeuXz9D5XkEJ9SExMTHe7My3qJpobrcD9X//1Xz7qQxc7Qvt9Z3cV9/RGiqT+vkPvo3ndaYVnjW4IUT/vv/9+e+ONN3w0yogRI/yigeawN23aNFv9BYBoROgGAESdm266yRer0iJTkRXgvXv3ho9HSmtItRY20wJSmRlyrAXaFFq0+nXkcOZICj5avErhIrL6qtWl9T6hSm1madiwwrCGAGtRsbSo72p737593zmm70L9iKwuawEurQqtm4ZgK4hrgbVQ6L6S1cqDoAXdUm8zpX8vCQ1b17/xX/7yl+983+n9BtKS3ufVSut6P62ErgprSGil9yuR09+tFmkLVYjTqoRH0hQFVcR1ESdyBIaGe2e2j6qYpz5f0x/SutB0uf5qMcCM+hs6X9Vu3fSfX/3nS/2fN29ept4PAPIS5nQDAKKOQqgqbH/4wx9SvK7KrYKDVnSOtGXLlhRzVzXkWhXfH/7whxlWKVVJ1IrUWtE59YrYkbSX8r/+9S+fix2iOata6VpVu7Tme1+O2tOK1FrtW3Nc06K+6zPos0Rup6R+aMVzrQodGrarebypq6Ca1xs5JD4UclOHq9zYR1srsEeGOz3XRYbQ3Hz9BrSKvFalj/w7rTCuz6atszKS3ucN/SYiK/B6PGnSpCv+TKGV7nPqu9X3oGCq36YuoKQ19SDy86QeTaDvKXWV+nL//nqv1NvsaVX+zK6JoBXL9VscN26czytPr7+aLhGakhH53rq4ltlt9wAgr6HSDQCIOgqx2kv4F7/4hYdN7c+sIdMKnxqaGqqqhWgOs/5Hf+SWYaE9ky9n6dKlvs2V5g5rrnDqKpuGIGveaSgka/6pqsia9639vfU+CiUZvU9aNAQ4M/s4jxkzxiuwCtjapknDdBVQFVC0JVeI9szWkHWFNVW8tV2YKqDabiskFGj1Pen7UljTEOSrTUPxNa9X/7YaIaBgrYXDFPK0WJpomy99Tg331uJqqoDr82g4vkYkpDUHPjX9TjSSQEPsdb5Cpxb10nByHdO8eg0pV1jUEP/szNHWcGv9G+iz6DPp30C/yyvZsk5U3dcUAV1g0px9/e4qV67s/dWiZOqz9ncXzYnWsG79ptQHXYTSSBFthRZJ1WT9m+u711xv/WdFUylUndZoCC0WpwXp9Lv/6KOPfOh36n3s06P+aHswrSOgrfX0u9JFFM2dX7lypd15551+EU0jDLQQnLaLU1/1e9Z/DnUhKTd+iwBwVeT28ukAAKS1lZW2IBo0aFBypUqVkgsXLuzbJ2mbocjtk0R/p7/X1lI6R9tZaTuntLZFSk1bJKW3bVNaWyt9+eWXvtVX2bJlk7/3ve/59lfalupKt99KLa0tw+TDDz/0raO0VZbe9/vf/37y5s2bU5yj7dVuv/1237apaNGiyfXq1UseO3ZseAsu0RZPAwcOTC5XrpxvjZXR/wxIb8swfd+RQtt2pd4GKq3PE2pTW7xp+y9tz6Ztqf7whz985/3/9a9/JT/22GPJ119/vW+b1qhRo+9s/5Xee4csW7YsuUGDBsmFChVKsX3Y7t27k++++27/TtW+trT66KOPvrPFmLbL0nZx6f12IunfRNuIqa8ZbR+W3r91ajt27Eju1KmT/+b029Z31blz5+S1a9eGz9E2XKHvSZ9Hv5W9e/d+Z7svefnll5Nr1KiRXLBgwRS/8YsXLyY/++yz3oZ+Y2pDW/Olt2VYer97tae/1TZh+retWbNmcq9evcJb+n3xxRf++9HvU9+rzmvevHnywoULL/s9AEBeFqP/c3XiPQAAOU/DzX/6059+Zyg6opOq8RqW/9e//jW3uwIAwFXBnG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAsKcbgAAAAAAAkKlGwAAAACAgBC6AQAAAAAISKGgGsbVc+nSJfvss8+sRIkSvnUOAAAAACBYmqn91VdfWaVKlaxAgfTr2YTufECBu2rVqrndDQAAAAC45vzzn/+0KlWqpHuc0J0PqMId+scuWbJkbncHAAAAAPK9U6dOefEzlMfSQ+jOB0JDyhW4Cd0AAAAAcPVkNMWXhdQAAAAAAAgIle58pM3w+VYwtmhudwMAAAAAsm17Yg/LD6h0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAADXauhOTk62vn37WlxcnMXExNjOnTtzu0sAAAAAAOSP0L1q1SqbNWuWrVixwo4cOWINGzbMdpu9evWyhIQEiwbr16+3H/3oR1axYkUrVqyYNWnSxF599dXc7hYAAAAAIAcUsih38OBBD6QtW7a0aHPx4kWvvhcocOXXLjZv3myNGze2Z5991m644Qa/uNCjRw8rVaqU3XfffTnaXwAAAADA1RXVlW5VpAcOHGiHDx/2cFutWjW7dOmSjR8/3qpXr25Fixa1+Ph4W7x4cYog3Lt37/DxunXr2qRJk8LHR44cabNnz7Zly5Z5m7qp2qybHp84cSJ8roay67VDhw75c1XcS5cubcuXL7cGDRpYbGys9+3cuXM2dOhQq1y5slermzdv7u1lxnPPPWejR4/2iwo1a9a0p556ytq3b29LlizJ0e8SAAAAAHD1RXWlW2FZQXT69Om2bds2K1iwoAfuefPm2dSpU6127dq2YcMG6969u5UrV87atm3robxKlSq2aNEiK1u2rFeSNSdc1fLOnTt7ON6zZ4+dOnXKZs6c6e+j+eI6LzPOnDljEyZMsKSkJG+/fPnyNmDAANu9e7ctWLDAKlWqZEuXLvXgvGvXLu9jVp08edLq16+f7nGFfN1C9FkAAAAAANEnqkO3hliXKFHCw3aFChU8aI4bN87WrFljLVq08HNq1KhhGzdutGnTpnnoLly4sI0aNSrchireW7ZssYULF3roLl68uFfA1ZbazKoLFy7YlClTvMIuqnQrvOtegVsU7DUXXa+rv1mhfuoCgz5PenThIfIzAgAAAACiU1SH7tQOHDjgleZ27dqleP38+fPWtGnT8PPJkyfbjBkzPAifPXvWj2uBspxQpEgRn4Mdomq2hrTXqVMnxXkK9aqEZ8W6devsscces5dfftluvvnmdM8bNmyYDR48OEWlu2rVqll6LwAAAABA8PJU6D59+rTfr1y50udPR9L8atEQb1WaJ06c6NVwVcoTExNt69atl207tBiatiiLrGqnpiq55nlH9kmV+O3bt/t9JFXVM+vPf/6z3X///fbb3/7WF1K7HH3W0OcFAAAAAESvPBW6Ixcv01DytGzatMkXJevfv3+KFdBTV6tVnY6kOeGibcnKlCnjjzOzJ7gq7Grr6NGj1rp16yv6XFp0TSuVa6645p8DAAAAAPKHPBW6VbVWFXvQoEG+YFqrVq180TEF7ZIlS1rPnj194bI5c+bY6tWrfT733LlzfY60HodoFXQd37dvnw8B19zxWrVq+RBtrW4+duxY279/v1fLM6Jh5d26dfPqtM5XCD927JitXbvWh6F37NgxwyHlCtxatfzHP/6xff755+ELA1rgDQAAAACQd0X1lmFp0fZaI0aM8MXEtMK3VgnXcPNQqO7Xr5916tTJunTp4lt3HT9+PEXVW/r06eNbiTVr1swr3ArtWoBt/vz5tnfvXg/LqjqPGTMmU33SgmkK3UOGDPF2ExISPOjfeOONGf6tti/TPHV9Hq2wHrrpMwAAAAAA8raY5MhJzMiTtJCaqvXxA6dawdiiud0dAAAAAMi27YmXX+sqWnKYRl9r5HW+qXQDAAAAAJBXELoD1qFDB1/FPK1bVvfwBgAAAADkLXlqIbW8KCkpyfcKTwsLpQEAAABA/kboDljq/cQBAAAAANcOhpcDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBDmdOcjG8Z0vez+cAAAAACAq4tKNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBC2DMtH2gyfbwVji2b6/O2JPQLtDwAAAABc66h0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAD5MXQnJydb3759LS4uzmJiYmznzp252R0AAAAAAPJP6F61apXNmjXLVqxYYUeOHLGGDRtmu81evXpZQkKCRZsDBw5YiRIlrHTp0ile1+fXBYfI23XXXZdr/QQAAAAA5JxClosOHjxoFStWtJYtW1q0uXjxogfgAgWyf13iwoUL1rVrV2vdurVt3rz5O8dLlixp+/btCz/X+wIAAAAA8r5cq3SrIj1w4EA7fPiwh8xq1arZpUuXbPz48Va9enUrWrSoxcfH2+LFi1ME4d69e4eP161b1yZNmhQ+PnLkSJs9e7YtW7YsXDVev3693/T4xIkT4XM1lF2vHTp0KFxxVhV6+fLl1qBBA4uNjfW+nTt3zoYOHWqVK1e2YsWKWfPmzb29rBg+fLjVq1fPOnfunOZx9aNChQrh2w033HAF3ygAAAAAINrkWqVbYblmzZo2ffp027ZtmxUsWNAD97x582zq1KlWu3Zt27Bhg3Xv3t3KlStnbdu29VBepUoVW7RokZUtW9arxpoTrmq5Aq3C8Z49e+zUqVM2c+ZMfx/NF0+rupyWM2fO2IQJEywpKcnbL1++vA0YMMB2795tCxYssEqVKtnSpUutffv2tmvXLu9jRt555x3vr0L+kiVL0jzn9OnTdtNNN/nnu+WWW2zcuHF28803p9umLgToFqLPCwAAAACIPrkWukuVKuVznBW2Vd1ViFTYXLNmjbVo0cLPqVGjhm3cuNGmTZvmobtw4cI2atSocBuqeG/ZssUWLlzoobt48eJeAVdbavNKhoFPmTLFK+yiSrfCu+4VuEXBXnPR9br6eznHjx/3ir4uJGgIeVpUrZ8xY4Y1btzYTp48aS+88IIPt//b3/7mFxjSoosTkd8DAAAAACA65eqc7tQLjanS3K5duxSvnz9/3po2bRp+PnnyZA+pCsJnz571402aNMmRPhQpUsTDb4iq2RrSXqdOnRTnKdSrEp6RPn362COPPGJt2rRJ9xxdYAhdZBAF7vr16/uFhtGjR6f5N8OGDbPBgwenqHRXrVo1w/4AAAAAAK7R0K0h1rJy5UqfPx1J86tFQ7xVaZ44caIHVVXKExMTbevWrZdtO7QYmrYoi6xqp6YqeeQiZuqTKvHbt2/3+0iqqmdmaLnmiKt6HXp/DSEvVKiQD6t//PHHv/M3qubrIoMuQqRH30foOwEAAAAARK+oCd2Ri5dpKHlaNm3a5JXg/v37p1gBPXW1WtXpSJoTLtqWrEyZMv44M3uCK/yqraNHj/rK41mloe+RfdECb5ozrjnmqS8shOh8VdjvvffeLL8fAAAAACC6RE3oVtVaVexBgwZ5NbhVq1Y+x1lBW/Ohe/bs6QuXzZkzx1avXu3zuefOneuLsOlxiFZB13FtwaUh4Jo7XqtWLR9+rdXNx44da/v37/dqeUY0rLxbt27Wo0cPP18h/NixY7Z27Vofht6xY8fL/r2GiUf64IMPvOoeuR/5888/b3fccYf3Uaurq3L/ySef2BNPPHFF3yMAAAAAIHrk2pZhadEc5hEjRvhCYQqsWiVcw81Dobpfv37WqVMn69Kli2/dpYXKIqveoXnUWpysWbNmXuFWaNeQ7fnz59vevXs9LKvaPGbMmEz1SQumKXQPGTLE201ISPCgf+ONN+bIZ/73v//tfdbnVXVb87NVCVflHwAAAACQt8UkR050Rp6koK6KfvzAqVYwtmim/257Yo9A+wUAAAAA+T2HaYR2ertVRV2lGwAAAACA/ITQnQ0dOnTwVczTumW0hzcAAAAAIP+LmoXU8qKkpCTfKzwtcXFxV70/AAAAAIDoQujOhvS2/QIAAAAAQBheDgAAAABAQAjdAAAAAAAEhNANAAAAAEBAmNOdj2wY0/Wy+8MBAAAAAK4uKt0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBA2DIsH2kzfL4VjC2a7vHtiT2uan8AAAAA4FpHpRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAAuFZDd3JysvXt29fi4uIsJibGdu7cmdtdAgAAAAAgf4TuVatW2axZs2zFihV25MgRa9iwYbbb7NWrlyUkJFg0+Oabb7w/jRo1skKFCkVNvwAAAAAA2VfIotzBgwetYsWK1rJlS4s2Fy9e9Op7gQIFstVG0aJF7Wc/+5m9/vrrOdo/AAAAAEDuiupKtyrAAwcOtMOHD3u4rVatml26dMnGjx9v1atX97AaHx9vixcvThFie/fuHT5et25dmzRpUvj4yJEjbfbs2bZs2TJvU7f169f7TY9PnDgRPldD2fXaoUOH/Lkq7qVLl7bly5dbgwYNLDY21vt27tw5Gzp0qFWuXNmKFStmzZs39/YyQ+e/9NJL1qdPH6tQoUKOfn8AAAAAgNwV1ZVuheWaNWva9OnTbdu2bVawYEEP3PPmzbOpU6da7dq1bcOGDda9e3crV66ctW3b1kN5lSpVbNGiRVa2bFnbvHmzzwlXtbxz584ejvfs2WOnTp2ymTNn+vtovrjOy4wzZ87YhAkTLCkpydsvX768DRgwwHbv3m0LFiywSpUq2dKlS619+/a2a9cu72NOU8jXLUSfBQAAAAAQfaI6dJcqVcpKlCjhYVtVYAXNcePG2Zo1a6xFixZ+To0aNWzjxo02bdo0D92FCxe2UaNGhdtQxXvLli22cOFCD93Fixf3CrjaupLK8oULF2zKlCleYRdVuhXeda/ALQr2mouu19XfnKYLD5GfEQAAAAAQnaI6dKd24MABrzS3a9cuxevnz5+3pk2bhp9PnjzZZsyY4UH47NmzfrxJkyY50ociRYpY48aNw89VzdaQ9jp16qQ4T6FelfAgDBs2zAYPHpyi0l21atVA3gsAAAAAcI2E7tOnT/v9ypUrff50JM2vFg3xVqV54sSJXg1XpTwxMdG2bt162bZDi6Fpi7LIqnZqqpJrnndkn1SJ3759u99HUlU9CPqsoc8LAAAAAIheeSp0Ry5epqHkadm0aZOvdN6/f/8UK6CnrlarOh1Jc8JF25KVKVPGH2dmT3BV2NXW0aNHrXXr1lf0uQAAAAAA+VOeCt2qWquKPWjQIF8wrVWrVnby5EkP2iVLlrSePXv6wmVz5syx1atX+3zuuXPn+iJsehyiVdB1fN++fT4EXHPHa9Wq5UO0tbr52LFjbf/+/V4tz4iGlXfr1s169Ojh5yuEHzt2zNauXevD0Dt27JhhG1qETUPgv/zyS/vqq6/CYT+nhsQDAAAAAHJHngrdMnr0aK9KazGxjz/+2LfwuuWWW+y5557z4/369bMdO3ZYly5dfBh4165dver91ltvhdvQ9lza0qtZs2Y+PHzdunV211132fz58+3JJ5/0sHzbbbfZmDFj7KGHHsqwT1owTecOGTLEPv30U7v++uvtjjvusPvuuy9Tn+nee++1Tz75JPw8ND89cqg7AAAAACDviUkm2eV5WkhN1fr4gVOtYGzRdM/bntjjqvYLAAAAAPJ7DtPoa428Ts//rR4GAAAAAAByHKE7YB06dPBVzNO6BbGHNwAAAAAgeuS5Od15TVJSku8Vnpa4uLir3h8AAAAAwNVD6A5Y6v3EAQAAAADXDoaXAwAAAAAQEEI3AAAAAAABIXQDAAAAABAQ5nTnIxvGdL3s/nAAAAAAgKuLSjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAFhn+58pM3w+VYwtmiax7Yn9rjq/QEAAACAax2VbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAArtXQnZycbH379rW4uDiLiYmxnTt35naXAAAAAADIH6F71apVNmvWLFuxYoUdOXLEGjZsmO02e/XqZQkJCRYt/vKXv1jr1q3tuuuus6pVq9qvf/3r3O4SAAAAACAHFLIod/DgQatYsaK1bNnSos3Fixe9+l6gwJVfuzh16pT98Ic/tLvvvtumTp1qu3btsscff9xKly7tFX4AAAAAQN4V1ZVuVaQHDhxohw8f9nBbrVo1u3Tpko0fP96qV69uRYsWtfj4eFu8eHGKINy7d+/w8bp169qkSZPCx0eOHGmzZ8+2ZcuWeZu6rV+/3m96fOLEifC5Gsqu1w4dOuTPVXFXGF6+fLk1aNDAYmNjvW/nzp2zoUOHWuXKla1YsWLWvHlzby8zXn31VTt//rzNmDHDbr75Znv44YftZz/7mf3mN7/J0e8SAAAAAHD1RXWlW2G5Zs2aNn36dNu2bZsVLFjQA/e8efO8Kly7dm3bsGGDde/e3cqVK2dt27b1UF6lShVbtGiRlS1b1jZv3uwVY1XLO3fu7OF4z549XmGeOXOmv4/mi+u8zDhz5oxNmDDBkpKSvP3y5cvbgAEDbPfu3bZgwQKrVKmSLV261Nq3b+9Va/XxcrZs2WJt2rSxIkWKhF+75557/D3+/e9/W5kyZbL5LQIAAAAAcktUh+5SpUpZiRIlPGxXqFDBK8rjxo2zNWvWWIsWLfycGjVq2MaNG23atGkeugsXLmyjRo0Kt6GKt4LtwoULPXQXL17cK+BqS21m1YULF2zKlCleYRdVuhXeda/ALQr2mouu19Xfy/n888+9j5FuuOGG8LG0Qrf6rluILiAAAAAAAKJPVIfu1A4cOOCV5nbt2qV4XcOzmzZtGn4+efJkH66tIHz27Fk/3qRJkxzpgyrSjRs3Dj9XNVtD2uvUqZPiPIViVcKDoGp/5IUFAAAAAEB0ylOh+/Tp036/cuVKnz8dSfOrRUO8VWmeOHGiV8NVKU9MTLStW7detu3QYmjaoiyyqp2aquSa5x3ZJ1Xit2/f7veRVFXPiKrt//rXv1K8FnqeXiV+2LBhNnjw4BSVbq16DgAAAACILnkqdEcuXqah5GnZtGmTr3Tev3//FCugp65WqzodSXPCRduShYZ0Z2ZPcFXY1dbRo0d926+s0oWBX/ziFx7wNTRe3n77bV8ALr353PoOQhcZAAAAAADRK6pXL09NVWtVsQcNGuQrkCtMf/jhh/b73//en4sWLvvggw9s9erVtn//fhsxYoQvwhZJq6Brb+x9+/bZF1984YG3Vq1aXi3W6uZ///vfvZquanlGNKy8W7du1qNHD1uyZIn94x//sPfff9+HgKuNjDzyyCN+EUArrv/tb3+z1157zReQi6xkAwAAAADypjwVumX06NEepBVq69ev76uEK9yGFiPr16+fderUybp06eJbdx0/fjxF1Vv69OnjleRmzZp5hVvVcVWZ58+fb3v37vU521o9fMyYMZnqkxZMU+geMmSIt5uQkOBB/8Ybb8zUYnF/+tOfPKzfeuut3sYvf/lL9ugGAAAAgHwgJjlyEjPyJM3pVniPHzjVCsYWTfOc7Yk9rnq/AAAAACC/57CTJ09ayZIl80+lGwAAAACAvILQHbAOHTr4KuZp3TLawxsAAAAAkLflqdXL86KkpCTfKzwtcXFxV70/AAAAAICrh9AdsNT7iQMAAAAArh0MLwcAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACwkJq+ciGMV0vuyk7AAAAAODqotINAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhC3D8pE2w+dbwdiiKV7bntgj1/oDAAAAANc6Kt0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAFyroTs5Odn69u1rcXFxFhMTYzt37sztLgEAAAAAkD9C96pVq2zWrFm2YsUKO3LkiDVs2DDbbfbq1csSEhIsmi4svPDCC1anTh2LjY21ypUr29ixY3O7WwAAAACAbCpkUe7gwYNWsWJFa9mypUWbixcvevW9QIHsXbt46qmn7E9/+pMH70aNGtmXX37pNwAAAABA3hbVlW5VpAcOHGiHDx/2cFutWjW7dOmSjR8/3qpXr25Fixa1+Ph4W7x4cYog3Lt37/DxunXr2qRJk8LHR44cabNnz7Zly5Z5m7qtX7/eb3p84sSJ8Lkayq7XDh065M9VcS9durQtX77cGjRo4FVp9e3cuXM2dOhQr1AXK1bMmjdv7u1lxp49e+yll17y/jzwwAPe71tvvdXatWuXo98lAAAAAODqi+pKt8JyzZo1bfr06bZt2zYrWLCgB+558+bZ1KlTrXbt2rZhwwbr3r27lStXztq2beuhvEqVKrZo0SIrW7asbd682eeEq1reuXNnD8cKuqdOnbKZM2f6+2i+uM7LjDNnztiECRMsKSnJ2y9fvrwNGDDAdu/ebQsWLLBKlSrZ0qVLrX379rZr1y7v4+X8z//8j9WoUcOHz+tvNNT87rvvtl//+tfeLwAAAABA3hXVobtUqVJWokQJD9sVKlTwivK4ceNszZo11qJFCz9HgXXjxo02bdo0D92FCxe2UaNGhdtQ5XjLli22cOFCD93Fixf3CrjaUptZdeHCBZsyZYpX2EWVboV33Stwi4K95qLrdfX3cj7++GP75JNP/CLBnDlzvFI/aNAg+8lPfmLvvPNOmn+jvusWogsIAAAAAIDoE9WhO7UDBw54pTn10Ovz589b06ZNw88nT55sM2bM8CB89uxZP96kSZMc6UORIkWscePG4eeqZisoaxG0SArFqoRnRJV5navAHWrjlVde8SHm+/bt8+HxqanaH3lhAQAAAAAQnfJU6D59+rTfr1y50udPR9L8atEQb1WaJ06c6NVwVcoTExNt69atl207tBiahndHVrVTU5Vc87wj+6RK/Pbt2/0+kqrqGdGw90KFCqUI7fXr1/d7XTRIK3QPGzbMBg8enKLSXbVq1QzfCwAAAABwdeWp0B25eJmGkqdl06ZNvtJ5//79U6yAnrparep0JM0JF21LVqZMGX+cmT3BVWFXW0ePHrXWrVtn+TPdeeed9u2333ofNX9d9u/f7/c33XRTmn+j7yB0kQEAAAAAEL2ievXy1FS1VhVbc561ArmC6ocffmi///3v/blo4bIPPvjAVq9e7eF1xIgRvghbJK2C/pe//MWHb3/xxRde0a5Vq5ZXi7W6+d///nevpqtanhFVqLt162Y9evSwJUuW2D/+8Q97//33fQi42siIFk275ZZb7PHHH7cdO3Z4xbxfv34+hD71kHUAAAAAQN6Sp0K3jB492oO0Qq2GYWvFb4VbLZgmCqydOnWyLl26+NZdx48fT1H1lj59+viw7WbNmnmFW9VxLcA2f/5827t3r8/Z1grlY8aMyVSftGCaQveQIUO83YSEBA/6N954Y4Z/q2HtWsH8+uuvtzZt2ljHjh39c2mYPAAAAAAgb4tJjpzEjDxJc7q10nv8wKlWMLZoimPbE3vkWr8AAAAAIL/nsJMnT1rJkiXzT6UbAAAAAIC8gtAdsA4dOvgq5mndMtrDGwAAAACQt+Wp1cvzoqSkJN8rPC1xcXFXvT8AAAAAgKuH0B2w1PuJAwAAAACuHQwvBwAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAALCQmr5yIYxXS+7KTsAAAAA4Oqi0g0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0J2PtBk+3259ek5udwMAAAAA8P8QugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACA/Bi6k5OTrW/fvhYXF2cxMTG2c+fO3OwOAAAAAAD5J3SvWrXKZs2aZStWrLAjR45Yw4YNs91mr169LCEhwaLB+vXr7Uc/+pFVrFjRihUrZk2aNLFXX301xTlLliyxZs2aWenSpcPnzJ07N9f6DAAAAADIOYUsFx08eNADacuWLS3aXLx40avvBQpc+XWJzZs3W+PGje3ZZ5+1G264wS8u9OjRw0qVKmX33Xefn6Mq/y9+8QurV6+eFSlSxM957LHHrHz58nbPPffk4CcCAAAAAFwzlW5VpAcOHGiHDx/2cFutWjW7dOmSjR8/3qpXr25Fixa1+Ph4W7x4cYog3Lt37/DxunXr2qRJk8LHR44cabNnz7Zly5Z5m7qp2qybHp84cSJ8roay67VDhw75c1XcVW1evny5NWjQwGJjY71v586ds6FDh1rlypW9Et28eXNvLzOee+45Gz16tF9UqFmzpj311FPWvn17r26H3HXXXfbggw9a/fr1w+coqG/cuDGHvmkAAAAAwDVX6VZYVsicPn26bdu2zQoWLOiBe968eTZ16lSrXbu2bdiwwbp3727lypWztm3beiivUqWKLVq0yMqWLeuVZM0JV7W8c+fOHo737Nljp06dspkzZ4YryTovM86cOWMTJkywpKQkb1/V5gEDBtju3bttwYIFVqlSJVu6dKkH5127dnkfs+rkyZMesNOb4/7OO+/Yvn37vB/p0YUA3UL0eQEAAAAA0SfXQreGWJcoUcLDdoUKFTxEjhs3ztasWWMtWrTwc2rUqOEV32nTpnnoLly4sI0aNSrchireW7ZssYULF3roLl68uFfA1ZbazKoLFy7YlClTvMIuqnQrvOtegVsU7DUXXa+rv1mhfuoCgz5P6iCuSrr6re9DfWjXrl267ejiROT3AAAAAACITrk6pzvSgQMHvNKcOmyeP3/emjZtGn4+efJkmzFjhgfhs2fP+nEtPpYTNKdaQ7tDVM3WkPY6deqkOE/hWJXwrFi3bp3P1X755Zft5ptvTnFMFx803P306dO2du1aGzx4sF9w0NDztAwbNszPiax0V61aNUv9AQAAAABcQ6FbgVNWrlzpVd9Iml8tGuKtSvPEiRO9Gq6wmpiYaFu3br1s26HF0DR8O7KqnZqq5JrnHdknVZ63b9/u95FUVc+sP//5z3b//ffbb3/7W19ILa3+1apVyx/rAoKGyKuanV7o1vcR+k4AAAAAANErakJ35OJlGkqelk2bNvmiZP3790+xAnrqarWq05E0J1y0LVmZMmX8cWb2BFeFXW0dPXrUWrdufUWfS4uuaaVyzdHW/PPM0Nz1yDnbAAAAAIC8KWpCt6rWqmIPGjTIQ2erVq18rrOCdsmSJa1nz56+cNmcOXNs9erVPp9b+1lrjrQeh2gVdB3XYmQaAq6546oia/i1VjcfO3as7d+/36vlGdGw8m7dunl1WucrhB87dsyHgGsYeseOHTMcUq7ArRXJf/zjH9vnn38evjCgBd5EFW3t061F5RS033zzTf9cL730Ura/UwAAAADANbplWFq0vdaIESM8iGqFb60SruHmoVDdr18/69Spk3Xp0sW37jp+/HiKqrf06dPHtxJTkFWFW6FdC7DNnz/f9u7d62FZVecxY8Zkqk9aME2he8iQId5uQkKCB/0bb7wxw7/V9mWap67PoxXWQzd9hpCvv/7aP4Pmed955532+uuv+wruTzzxRJa/PwAAAABAdIlJjpzojDxJC6mpoh8/cKoVjC1q2xO/O28cAAAAAJDzOUwjtDU6O09UugEAAAAAyE8I3dnQoUMHX8U8rVtW9/AGAAAAAOQ/UbOQWl6UlJTke4WnJbRQGgAAAADg2kXozobU+4kDAAAAABCJ4eUAAAAAAASE0A0AAAAAQEAI3QAAAAAABIQ53fnIhjFdL7s/HAAAAADg6qLSDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQnY+0GT7fbn16Tm53AwAAAADw/xC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAID8GLqTk5Otb9++FhcXZzExMbZz587c7A4AAAAAAPkndK9atcpmzZplK1assCNHjljDhg2z3WavXr0sISHBosE333zj/WnUqJEVKlQozX7puC44pL7dfPPNudJnAAAAAEA+Cd0HDx60ihUrWsuWLa1ChQoeTKPFxYsX7dKlS9luo2jRovazn/3M7r777jTPmTRpkl9wCN3++c9/euX/oYceytZ7AwAAAACu4dCtCu/AgQPt8OHDXtmtVq2ah9zx48db9erVPazGx8fb4sWLU4TY3r17h4/XrVvXQ2vIyJEjbfbs2bZs2bJwxXj9+vV+0+MTJ06Ez9VQdr126NAhf66Ke+nSpW358uXWoEEDi42N9b6dO3fOhg4dapUrV7ZixYpZ8+bNvb3M0PkvvfSS9enTxy8qpKVUqVJ+LHT74IMP7N///rc99thj2fh2AQAAAADRINdKywrLNWvWtOnTp9u2bdusYMGCHrjnzZtnU6dOtdq1a9uGDRuse/fuVq5cOWvbtq2H8ipVqtiiRYusbNmytnnzZp8Trmp5586dPRzv2bPHTp06ZTNnzvT3UdVY52XGmTNnbMKECZaUlOTtly9f3gYMGGC7d++2BQsWWKVKlWzp0qXWvn1727Vrl/cxp73yyiteFb/pppvSPUcXAnQL0ecFAAAAAESfXAvdqvCWKFHCw7YqvAqR48aNszVr1liLFi38nBo1atjGjRtt2rRpHroLFy5so0aNCrehiveWLVts4cKFHrqLFy/uFXC1lV5l+XIuXLhgU6ZM8Qq7qNKt8K57BW5RsNdcdL2u/uakzz77zN566y374x//eNnzdHEi8nsAAAAAAESnqJlEfeDAAa80t2vXLsXr58+ft6ZNm4afT5482WbMmOFB+OzZs368SZMmOdKHIkWKWOPGjcPPVc3WkPY6deqkOE+hXpXwnKah8RrintFCcMOGDbPBgwenqHRXrVo1x/sDAAAAAMgnofv06dN+v3LlSp8/HUnzq0VDvFVpnjhxolfDVSlPTEy0rVu3XrbtAgUKhLcoi6xqp6YqueZ5R/ZJlfjt27f7fSRV1XOS+qaLCY8++qiH/8vR9xH6TgAAAAAA0StqQnfk4mUaSp6WTZs2+Urn/fv3T7ECeiQFVlWnI2lOuGh18DJlyvjjzOwJrgq72jp69Ki1bt3agvTnP//Zq/1aKA4AAAAAkD9ETehW1VpV7EGDBvmCaa1atbKTJ0960C5ZsqT17NnTFy6bM2eOrV692udzz5071xdh0+MQrYKu4/v27fMh4Jo7XqtWLR9+rdXNx44da/v37/dqeUY0rLxbt27Wo0cPP18h/NixY7Z27Vofht6xY8cM29AibBoC/+WXX9pXX30VDvuph8RrATWtjJ4Te5UDAAAAAKJD1IRuGT16tFeltVDYxx9/7PObb7nlFnvuuef8eL9+/WzHjh3WpUsXHwbetWtXr3pr8bEQbc+lLb2aNWvmw8PXrVtnd911l82fP9+efPJJD8u33XabjRkzJlN7YWvBNJ07ZMgQ+/TTT+3666+3O+64w+67775MfaZ7773XPvnkk/Dz0Pz0yKHuurjw+uuvp9j+DAAAAACQ98UkR6Y/5ElaSE0V/fiBU61gbFHbntgjt7sEAAAAANdEDlMRVaOz0/N/K4wBAAAAAIAcR+jOhg4dOvgq5mndcnoPbwAAAABA3hNVc7rzmqSkJN8rPC1xcXFXvT8AAAAAgOhC6M6G1PuJAwAAAAAQieHlAAAAAAAEhNANAAAAAEBACN0AAAAAAASEOd35yIYxXS+7PxwAAAAA4Oqi0g0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0J2PtBk+3259ek5udwMAAAAA8P8QugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACA/Bi6k5OTrW/fvhYXF2cxMTG2c+fO3OwOAAAAAAD5J3SvWrXKZs2aZStWrLAjR45Yw4YNs91mr169LCEhwaLBoUOH/GJC6tt7772X4rxFixZZvXr17LrrrrNGjRrZm2++mWt9BgAAAADkk9B98OBBq1ixorVs2dIqVKhghQoVsmhx8eJFu3TpUo60tWbNGr+oELrdeuut4WObN2+2rl27Wu/evW3Hjh1+wUC3v/71rzny3gAAAACAazB0qyI9cOBAO3z4sFd/q1Wr5iF3/PjxVr16dStatKjFx8fb4sWLUwRhhdPQ8bp169qkSZPCx0eOHGmzZ8+2ZcuWhavK69ev95senzhxInyuhrLrNVWjRRX30qVL2/Lly61BgwYWGxvrfTt37pwNHTrUKleubMWKFbPmzZt7e1lRtmxZv6gQuhUuXDh8TP1v3769Pf3001a/fn0bPXq03XLLLfaHP/whm98wAAAAACC35VppWWGzZs2aNn36dNu2bZsVLFjQA/e8efNs6tSpVrt2bduwYYN1797dypUrZ23btvVQXqVKFR+OrSCrKrHmhKta3rlzZw/He/bssVOnTtnMmTP9fTRfXOdlxpkzZ2zChAmWlJTk7ZcvX94GDBhgu3fvtgULFlilSpVs6dKlHpJ37drlfcyMBx54wL755hurU6eOPfPMM/48ZMuWLTZ48OAU599zzz32xhtvpNueLgToFqLPCwAAAACIPrkWukuVKmUlSpTwsK3qr0LkuHHjfCh2ixYt/JwaNWrYxo0bbdq0aR66VSEeNWpUuA1VvBVaFy5c6KG7ePHiXgFXW2ozqy5cuGBTpkzxCruo0q3wrnsFblGw11x0va7+Xo76M3HiRLvzzjutQIEC9vrrr/vQcQXqUPD+/PPP7YYbbkjxd3qu19OjixOR3wMAAAAAIDpFzSTqAwcOeKW5Xbt2KV4/f/68NW3aNPx88uTJNmPGDA/CZ8+e9eNNmjTJkT4UKVLEGjduHH6uaraGtKtCHUmhXpXwjFx//fUpqti33XabffbZZ5aYmJii2p1Vw4YNS9GuKt1Vq1a94vYAAAAAAPk8dJ8+fdrvV65c6fOnI2l+tWiItyrNqh6rGq5KuQLs1q1bL9u2qsyhLcoiq9qpqUqued6RfVIlfvv27X6fuop9JTQn/O233w4/V0X+X//6V4pz9PxylXp9H6HvBAAAAAAQvaImdEcuXqah5GnZtGmTr3Tev3//FCugp65WqzodSXPCRSuHlylTxh9nZk9wVdjV1tGjR61169aWE/S+moMeoosHa9eutf/6r/8Kv6ZQHhpiDwAAAADIu6ImdKtqrSr2oEGDfMG0Vq1a2cmTJz1olyxZ0nr27OkLl82ZM8dWr17t87nnzp3ri7DpcYhWQdfxffv2+RBwzR2vVauWD7/W6uZjx461/fv3e7U8IxpW3q1bN+vRo4efrxB+7NgxD8kaht6xY8fL/r1WUtdFgNDw+CVLlvjQeC3UFvLUU0/5RQa1r/ZUzf/ggw98gTkAAAAAQN6Wq/t0p6btskaMGOELhWn7LK0SruHmoVDdr18/69Spk3Xp0sWHaR8/fjxF1Vv69OnjW4k1a9bMK9wK7VqAbf78+bZ3714Py1qhfMyYMZnqkxZMU+geMmSIt6uF0BT0b7zxxkx/Ju3Lrf5qK7PXXnvNHnvssfBxVe7/+Mc/esgObZGmhdYaNmyYpe8OAAAAABB9YpIjJzojT9JCaqroxw+cagVji9r2xB653SUAAAAAuCZymEZoa3R2nqh0AwAAAACQnxC6s6FDhw6+inlat4z28AYAAAAA5H9Rs5BaXqQF0bRXeFri4uKuen8AAAAAANGF0J0NqfcTBwAAAAAgEsPLAQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgICwkFo+smFM18tuyg4AAAAAuLqodAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdOcjbYbPt1ufnpPb3QAAAAAA/D+EbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAA8mPoTk5Otr59+1pcXJzFxMTYzp07c7M7AAAAAADkn9C9atUqmzVrlq1YscKOHDliDRs2zHabvXr1soSEBIs2Bw4csBIlSljp0qVTvH7XXXf5BYfUt44dO+ZaXwEAAAAAOaOQ5aKDBw9axYoVrWXLlhZtLl686OG3QIHsX5e4cOGCde3a1Vq3bm2bN29OcWzJkiV2/vz58PPjx49bfHy8PfTQQ9l+XwAAAADANVrpVkV64MCBdvjwYQ+31apVs0uXLtn48eOtevXqVrRoUQ+fixcvThGEe/fuHT5et25dmzRpUvj4yJEjbfbs2bZs2bJwxXj9+vV+0+MTJ06Ez9VQdr126NAhf66Ku6rQy5cvtwYNGlhsbKz37dy5czZ06FCrXLmyFStWzJo3b+7tZcXw4cOtXr161rlz5+8c09D6ChUqhG9vv/22fe973yN0AwAAAEA+kGuVboXlmjVr2vTp023btm1WsGBBD9zz5s2zqVOnWu3atW3Dhg3WvXt3K1eunLVt29ZDeZUqVWzRokVWtmxZrxprTriq5Qq0Csd79uyxU6dO2cyZM8OhNnV1OT1nzpyxCRMmWFJSkrdfvnx5GzBggO3evdsWLFhglSpVsqVLl1r79u1t165d3seMvPPOO95fhXxVtTPyyiuv2MMPP+wBHwAAAACQt+Va6C5VqpTPcVbYVoVXFeVx48bZmjVrrEWLFn5OjRo1bOPGjTZt2jQP3YULF7ZRo0aF21DFe8uWLbZw4UIP3cWLF/cKuNpSm1cyDHzKlCleYRdVuhXeda/ALQr2mouu19Xfy9FQcVX0dSGhZMmSGb7/+++/b3/96189eF+OPp9uIbrIAAAAAACIPrk6pzv1QmOqNLdr1y7F65rv3LRp0/DzyZMn24wZMzwInz171o83adIkR/pQpEgRa9y4cfi5qtka0l6nTp0U5ynwqhKekT59+tgjjzxibdq0ydT7K2w3atTIbr/99suepxEBkRcfAAAAAADRKWpC9+nTp/1+5cqVPn86kuZXi4Z4q9I8ceJEr4arUp6YmGhbt269bNuhxdC0RVlkVTs1Vck1zzuyT6rEb9++3e8jqaqemaHlmiP+wgsvhN9fQ+QLFSrkw+off/zx8Llff/21f77nn38+w3aHDRtmgwcPTlHprlq1aoZ/BwAAAAC4RkN35OJlGkqelk2bNvlK5/3790+xAnrqarWq05E0J1y0LVmZMmX8cWb2BFeFXW0dPXrUVx7PKg19j+yLFnjTnHHNMU99YUHzvlVB1xz2jOh7Cl2IAAAAAABEr6gJ3apaq4o9aNAgrwa3atXKTp486UFb86F79uzpC5fNmTPHVq9e7fO5586d64uw6XGIVkHX8X379vkQcM0dr1WrlleCtbr52LFjbf/+/V4tz4iGlXfr1s169Ojh5yuEHzt2zNauXevD0DPaS7t+/fopnn/wwQdedU9rP3INLdf+4pkZtg4AAAAAyBtybcuwtIwePdpGjBjhc5YVWLVKuIabh0J1v379rFOnTtalSxffuksLlUVWvUPzqLWVWLNmzbzCrdCuBdjmz59ve/fu9bCsavOYMWMy1SctmKbQPWTIEG9XwVhB/8Ybb8yxz60LBFowTtuhAQAAAADyj5jkyInOyJM0p1sV/fiBU61gbFHbntgjt7sEAAAAANdEDtMI7cvtVhVVlW4AAAAAAPITQnc2dOjQwVcxT+uW0R7eAAAAAID8L2oWUsuLkpKSfK/wtMTFxV31/gAAAAAAoguhOxtSb/sFAAAAAEAkhpcDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAAWEhtXxkw5iul92UHQAAAABwdVHpBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLozkfaDJ+f210AAAAAAEQgdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAA0RS6Dx48aMOHD7euXbva0aNH/bW33nrL/va3v2WpneTkZOvbt6/FxcVZTEyM7dy580q6AwAAAABA/gjdf/7zn61Ro0a2detWW7JkiZ0+fdpf/+ijj+xXv/pVltpatWqVzZo1y1asWGFHjhyxhg0bWnb16tXLEhISLBqsX7/efvSjH1nFihWtWLFi1qRJE3v11VdTnKMLFT/+8Y+tWrVqfuHhxRdfzLX+AgAAAAByOXT//Oc/tzFjxtjbb79tRYoUCb/+gx/8wN57770sV8wVSFu2bGkVKlSwQoUKWbS4ePGiXbp0KVttbN682Ro3bmyvv/66/eUvf7HHHnvMevTo4RcZQs6cOWM1atSw//7v//bvAAAAAABwDYfuXbt22YMPPvid18uXL29ffPFFlirSAwcOtMOHD3uFV5Vehdzx48db9erVrWjRohYfH2+LFy9OEYR79+4dPl63bl2bNGlS+PjIkSNt9uzZtmzZMm9TN1WbddPjEydOhM/VUHa9dujQIX+uinvp0qVt+fLl1qBBA4uNjfW+nTt3zoYOHWqVK1f2anXz5s29vcx47rnnbPTo0X5RoWbNmvbUU09Z+/btfYRAyG233WaJiYn28MMP+3sCAAAAAPKPLJeWFUw1FFzBN9KOHTs8mGaWwrKC6PTp023btm1WsGBBD9zz5s2zqVOnWu3atW3Dhg3WvXt3K1eunLVt29ZDeZUqVWzRokVWtmxZryRrTriq5Z07d/ZwvGfPHjt16pTNnDnT30fzxXVeZqjqPGHCBEtKSvL2dSFhwIABtnv3bluwYIFVqlTJli5d6sFZFx/Ux6w6efKk1a9f37JDFwJ0C9HnBQAAAADkg9Ctiuyzzz7rwVeVYgXhTZs2eeDV0OnMKlWqlJUoUcLDtoZVK0SOGzfO1qxZYy1atPBzNOx648aNNm3aNA/dhQsXtlGjRoXbUPDfsmWLLVy40EN38eLFvQKutq5kqPaFCxdsypQpXmEXVboV3nWvwC36nJqLrtfV36xQP3WBQZ8nO3RxIvJ7AAAAAADkk9CtoPnTn/7Uqlat6sO9NRRb94888oivaH6lDhw44JXmdu3apXj9/Pnz1rRp0/DzyZMn24wZMzwInz171o9rgbKcoDnqmoMdomq2PludOnVSnKdQr0p4Vqxbt87ndL/88st28803Z6ufw4YNs8GDB6eodOvfAwAAAACQh0O3tvj6/PPP7Xe/+5398pe/9FCq1csViq9kqHWk0CroK1eu/M4w9dBcZw3xVqV54sSJXg1XpVzzobWS+uUUKFAg3P/IqnZqqpKreh/ZJ1Xit2/f7veRVFXPyorv999/v/32t7/N0miA9Oj7YP43AAAAAOTD0F2rVi3f5kohOyerq5GLl2koeVo0jF2LkvXv3z/FCuipq9WqTkfSnHDRXPQyZcr448zsCa6LCWpLe5G3bt36ij6XFl277777fK645p8DAAAAAK4dWQrdqhgrbB8/fjzble3UVLVWFXvQoEE+T7xVq1a+6JiCdsmSJa1nz57+nnPmzLHVq1f7fO65c+f6HOnIRd20CrqO79u3z4eAa+64LhToAoFWNx87dqzt37/fq+UZ0bDybt26eXVa5yuEHzt2zNauXevD0Dt27JjhkHIFbq1arr24NUogdGFAC7yJhsdrobbQ408//dQvCKiSrn4DAAAAAK6hLcO0n/TTTz9tf/3rX3O8M9pea8SIEb5QmFb41irhGm4eCtX9+vWzTp06WZcuXXzrLoX/yKq39OnTx7cSa9asmVe4Fdq1ANv8+fNt7969HpZVddZe45mhBdMUuocMGeLtJiQkeNC/8cYbM/xbbV+meer6PFphPXTTZwj57LPPPMzrpkr8Cy+84I+feOKJLH9/AAAAAIDoEpMcOdE5EzQ8W0Hy22+/9Yqt5kFH+vLLL3O6j8iAFlJTRT9+4FTb+bt+ud0dAAAAALhmctjJkyd9dHaOrV7+4osvZrdvAAAAAABcE7IcujW3Gv+nQ4cO9u6776Z57LnnnvMbAAAAAODaleXQrdXFLyczc53zi6SkJN8rPC2hhdIAAAAAANeuLIdurQ4euZd1aqm368rPUu8nDgAAAABAtkL3jh07Ujy/cOGCv/ab3/zGt+MCAAAAAABXGLrj4+O/85q256pUqZIlJiam2A4LAAAAAIBrWZb36U6P9rDW/tUAAAAAAOAKK93aiyyStvk+cuSIjRw50mrXrp3V5pCDNozpmttdAAAAAABkJ3SXLl36OwupKXhXrVrVFixYkNXmAAAAAADIt7IcutetW5fieYECBaxcuXJWq1YtK1Qoy80BAAAAAJBvZTklq8rdsmXL7wTsb7/91jZs2GBt2rTJyf4BAAAAAHDtLKT2/e9/37788svvvH7y5Ek/BgAAAAAArjB0a/526jndcvz4cStWrFhWmwMAAAAAIN/K9PDy0P7bCty9evWy2NjY8LGLFy/aX/7yFx92DgAAAAAAshi6S5UqFa50lyhRwooWLRo+VqRIEbvjjjusT58+mW0OAWgzfL7t/F2/3O4GAAAAACCroXvmzJl+X61aNRs6dChDyQEAAAAAyOnVy3/1q19l9U8AAAAAALgmXdHG2osXL7aFCxfa4cOH7fz58ymOffjhhznVNwAAAAAArq3Vy3/3u9/ZY489ZjfccIPt2LHDbr/9ditbtqx9/PHH1qFDh2B6CQAAAADAtRC6p0yZYtOnT7ff//73voDaM888Y2+//bb97Gc/8726AQAAAADAFYZuDSkPbQ2mFcy/+uorf/zoo4/a/Pnzs9ocAAAAAAD5VpZDd4UKFezLL7/0xzfeeKO99957/vgf//iHbycGAAAAAACuMHT/4Ac/sOXLl/tjze0eNGiQtWvXzrp06WIPPvhgVpsDAAAAACDfyvLq5ZrPfenSJX/805/+1BdR27x5sz3wwAPWr1+/LLWlyrj+Rquh//vf//aF2Zo0aZLVLgEAAAAAkD8q3QUKFLBChf7/rP7www/7iuYDBw70hdWyYtWqVTZr1ixbsWKFHTlyxBo2bGjZ1atXL0tISLBo8M0333h/GjVq5N9Zev169dVXLT4+3r73ve9ZxYoV7fHHH7fjx49f9f4CAAAAAHI5dMu7775r3bt3txYtWtinn37qr82dO9c2btyYpXYOHjzoIVMLs2mueGSYz20XL14MV/Sz04YWm9PK7nfffXea52zatMl69OhhvXv3tr/97W+2aNEie//9961Pnz7Zem8AAAAAQB4M3a+//rrdc889HiY1HPzcuXP+urYLGzduXKbbUQVY1XGthh4TE2PVqlXzkDt+/HirXr26t6/qr4aeR4ZYhdPQ8bp169qkSZPCx0eOHGmzZ8+2ZcuWeZu6rV+/3m96fOLEifC5O3fu9NcOHTrkz1VxL126tM9Xb9CggcXGxnrf9PmGDh1qlStXtmLFilnz5s29vczQ+S+99JIHaF1USMuWLVv8syuY63O1atXKh9wreAMAAAAArrHQPWbMGJs6daq9/PLLVrhw4fDrd955p3344YeZbkdh+fnnn7cqVar40PJt27Z54J4zZ463r6qvFmlTRf3Pf/6z/41Cuc5XNXj37t32y1/+0p577jlbuHChH1c47ty5s7Vv397b1C20vVlmnDlzxiZMmGBJSUn+/uXLl7cBAwZ4MF6wYIH95S9/sYceesjb//vf/245QaMF/vnPf9qbb77pc9z/9a9/+YWGe++9N92/0YWAU6dOpbgBAAAAAKJPlsdz79u3z9q0afOd10uVKpWikpwRnV+iRAkrWLCgV4EVJFUpX7NmjQdRqVGjhg9ZnzZtmrVt29ZD/qhRo8JtqDKsQKzQrbBdvHhxr4CrrfQqy5dz4cIFmzJlilfYRZXumTNn+n2lSpXCwV5z0fV6Vir76dHFCs3p1urvmgP+7bff2v3332+TJ09O9290cSLyewAAAAAA5KN9ug8cOPCd1xWOFZKvlNpUpVnbjyk8h26qfGvud4jC6K233mrlypXz41pNXaE4J2ghuMaNG4ef79q1y4e016lTJ0WfVHmP7FN2qGL/1FNPedV++/btHug15P0///M/0/2bYcOG+XD+0E2VcgAAAABAPqh0a36yQuKMGTN8TvRnn33m1WZVgEeMGHHFHTl9+rTfr1y50udPR9L8atEQb73PxIkTvRquSnliYqJt3bo1wxXXRcO3I6vaqalKrs8U2SdV4hWGdR9J4TsnqGqtavfTTz/tzxX6NRe8devWPpRfC82lpu8j9J0AAAAAAPJ46NZcZm3npfCqKqvmVv/Hf/yHV6Y11FwBUGFYC6NdqcjFyzSUPL2VvjVHu3///uHXUlecVa1WdTqSquKiOd5lypQJL6SWkaZNm3pbR48e9RAcBH2HqVdtDwX8yIsEAAAAAIB8GroVPhVYtbCYhpBr0TNVZjUkXNVgBebsVn5VtVZw1+JpCvVaxVtDpxW0S5YsaT179rTatWv7cPPVq1f7fG5tU6a+6HGIVgLXcc09L1u2rM8dr1WrllWtWtVXNx87dqzt37/fq+UZ0bDybt26+ZZeOl/fw7Fjx2zt2rVeke7YsWOmho+fP3/evvzyS/vqq6/CYb9JkyZ+r/nbGj2gVc61Kry+5//6r/+y22+/PTyPHAAAAACQj0O3ttL6xz/+4aFb840VilVRVtjOSaNHj/aqtIZcf/zxx/6+t9xyi69QLtpKS9uUadExDQPv2rWrV73feuutcBsKsNrSq1mzZn5BYN26dXbXXXfZ/Pnz7cknn/SwfNttt/nQba1EnhEtmKZzhwwZ4nuSX3/99XbHHXfYfffdl6nPpFXIP/nkk/BzBffIKra2TlMY/8Mf/uDvoc/8gx/8wFdRBwAAAADkbTHJmRjD3LdvX68wa36xhn9r267Uc5xDFJZxdWnLMFX04wdOtZ2/65fb3QEAAACAayaHnTx50kdnZ6vSrRXCO3Xq5MPJf/azn3k1WcPBAQAAAABADqxe3r59e7/XSt5avZzQbdahQwd799130zymIfGhYfEAAAAAgGtTlrcM0xxn/J+kpCQ7e/Zsmsfi4uKuen8AAAAAAHk8dOP/l3o/cQAAAAAAIhVI8QwAAAAAAOQYQjcAAAAAAAEhdAMAAAAAEBBCdz6yYUzX3O4CAAAAACACoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoTsfaTN8fm53AQAAAAAQgdANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAOTH0J2cnGx9+/a1uLg4i4mJsZ07d+ZmdwAAAAAAyD+he9WqVTZr1ixbsWKFHTlyxBo2bJjtNnv16mUJCQkWbQ4cOGAlSpSw0qVLp3h9yZIl1qxZM3+9WLFi1qRJE5s7d26u9RMAAAAAkHMKWS46ePCgVaxY0Vq2bGnR5uLFi159L1Ag+9clLly4YF27drXWrVvb5s2bUxxTlf8Xv/iF1atXz4oUKeIXIB577DErX7683XPPPdl+bwAAAADANVjpVkV64MCBdvjwYQ+31apVs0uXLtn48eOtevXqVrRoUYuPj7fFixenCMK9e/cOH69bt65NmjQpfHzkyJE2e/ZsW7Zsmbep2/r16/2mxydOnAifq6Hseu3QoUP+XBV3VZuXL19uDRo0sNjYWO/buXPnbOjQoVa5cmWvRDdv3tzby4rhw4d7qO7cufN3jt1111324IMPWv369a1mzZr21FNPWePGjW3jxo1X+M0CAAAAAOxar3QrLCtkTp8+3bZt22YFCxb0wD1v3jybOnWq1a5d2zZs2GDdu3e3cuXKWdu2bT2UV6lSxRYtWmRly5b1qrHmhKtarkCrcLxnzx47deqUzZw5M1xJTl1dTs+ZM2dswoQJlpSU5O2r2jxgwADbvXu3LViwwCpVqmRLly619u3b265du7yPGXnnnXe8vwr5Gkqe0Rx3nb9v3z7vR3p0IUC3EH1eAAAAAED0ybXQXapUKZ/jrLBdoUIFD5Hjxo2zNWvWWIsWLfycGjVqeMV32rRpHroLFy5so0aNCrehiveWLVts4cKFHrqLFy/uFXC1pTavZBj4lClTvMIuqnQrvOtegVsU7DUXXa+rv5dz/Phxr+jrQkLJkiXTPe/kyZNeSVe/9X2oD+3atUv3fF2ciPweAAAAAADRKVfndKdeaEyV5tRh8/z589a0adPw88mTJ9uMGTM8CJ89e9aPa/GxnKA51RraHaJqtoa016lTJ8V5CseqhGekT58+9sgjj1ibNm0ue54uPqgSfvr0aVu7dq0NHjzYLzho6Hlahg0b5udEVrqrVq2aiU8IAAAAALgmQ7cCp6xcudKrvpE0v1o0xFuV5okTJ3o1XGE1MTHRtm7detm2Q4uhafh2ZFU7NVXJNc87sk+qPG/fvt3vI6mqnhENFdcc8RdeeCH8/hoiX6hQIR9W//jjj4f7V6tWLX+sCwgaIq9qdnqhW99H6DsBAAAAAESvqAndkYuXaSh5WjZt2uQrnffv3z/FCuipq9WqTkfSnHDRtmRlypTxx5nZE1wVdrV19OhRX3k8qzT0PbIvWuBNc7U1xzz1hYVICuaRc7YBAAAAAHlT1IRuVa1VxR40aJCHzlatWvlcZwVtzYfu2bOnL1w2Z84cW716tc/n1n7WWoRNj0O0CrqOazEyDQHX3HFVkTX8Wqubjx071vbv3+/V8oxoWHm3bt2sR48efr5C+LFjx3wIuIahd+zY8bJ/rxXJI33wwQde1Y7cj1wVbe3TrUXlFLTffPNN/1wvvfTSFX2PAAAAAIDokWtbhqVl9OjRNmLECA+iCqxaJVzDzUOhul+/ftapUyfr0qWLb92lhcoiq96hedTaSkxBVhVuhXYtwDZ//nzbu3evh2VVm8eMGZOpPmnBNIXuIUOGeLsJCQke9G+88cYc+cxff/21f4abb77Z7rzzTnv99dd94bUnnngiR9oHAAAAAOSemOTIic7Ik7SQmir68QOn2s7f9cvt7gAAAADANZPDTp48edndqqKq0g0AAAAAQH5C6M6GDh06+Crmad0y2sMbAAAAAJD/Rc1CanlRUlKS7xWelri4uKveHwAAAABAdCF0Z8Pltv0CAAAAAIDh5QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNCdj2wY0zW3uwAAAAAAiEDoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLozkfaDJ+f210AAAAAAEQgdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAA+TF0JycnW9++fS0uLs5iYmJs586dudkdAAAAAADyT+hetWqVzZo1y1asWGFHjhyxhg0bZrvNXr16WUJCgkWDffv22fe//3274YYb7LrrrrMaNWrY8OHD7cKFC+Fz9Pj555+3mjVr+jnx8fH+vQAAAAAA8r5CufnmBw8etIoVK1rLli0t2ly8eNGr7wUKXPl1icKFC1uPHj3slltusdKlS9tHH31kffr0sUuXLtm4ceP8HIXwefPm2csvv2z16tWz1atX24MPPmibN2+2pk2b5uAnAgAAAABcM5VuVaQHDhxohw8f9nBbrVo1D6Pjx4+36tWrW9GiRb3qu3jx4hRBuHfv3uHjdevWtUmTJoWPjxw50mbPnm3Lli3zNnVbv3693/T4xIkT4XM1lF2vHTp0yJ+r4q5gvHz5cmvQoIHFxsZ6386dO2dDhw61ypUrW7Fixax58+beXmaosv3YY4/557jpppvsgQcesG7dutm7774bPmfu3Ln23HPP2b333uvnP/nkk/544sSJOfRNAwAAAACuuUq3wrKGVE+fPt22bdtmBQsW9MCtqu/UqVOtdu3atmHDBuvevbuVK1fO2rZt66G8SpUqtmjRIitbtqxXgzUnXNXyzp07ezjes2ePnTp1ymbOnOnvo/niOi8zzpw5YxMmTLCkpCRvv3z58jZgwADbvXu3LViwwCpVqmRLly619u3b265du7yPWXHgwAEfOt6pU6fwawr1GlYeSRcUNm7cmKW2AQAAAADRJ9dCd6lSpaxEiRIetitUqODhU0Ou16xZYy1atPBzVPlV+Jw2bZqHbg3XHjVqVLgNVby3bNliCxcu9NBdvHhxD6xqS21mleZXT5kyxSvTokq3wrvuFbhFwV7BWa+HhohnRMPnP/zwQ++XLhJoDnfIPffcY7/5zW+sTZs2fhFi7dq1tmTJEq/qp0ft6BaiiwwAAAAAgOiTq3O6U1eBVWlu165ditfPnz+fYm7z5MmTbcaMGR6Ez54968ebNGmSI30oUqSINW7cOPxc1WyF3zp16qQ4T4FXlfDMeu211+yrr77yOd1PP/20vfDCC/bMM8+EK/6a56353BruruCtIen6jOnRiIDIiw8AAAAAgOgUNaH79OnTfr9y5UqfPx1J86tFQ7xVadZ8Z1XDVSlPTEy0rVu3Xrbt0GJo2qIsJHIF8RBVyRV8I/ukSvz27dv9PpKq6plVtWpVv9dccYV4VbuHDBnibWro/BtvvGHffPONHT9+3CvqP//5z73Kn55hw4bZ4MGDU1S6Q+8BAAAAAIgeURO6Ixcv01DytGzatMmHavfv3z/FCuipq9Wph2Yr2Iq2JStTpow/zsye4Kqwq62jR49a69atLSdoXroCv+4jg7zmdetig469/vrrPlw+PfqeQhciAAAAAADRK2pCt6rWqmIPGjTIA2mrVq3s5MmTHrRLlixpPXv29IXL5syZ49tqaT63Vv7WImx6HKJV0HVce2RrCLjmjteqVcsrwVrdfOzYsbZ///5MrQ6uYeVabVzbful8hfBjx475vGsNQ+/YseNl//7VV1/1eeiNGjXykPzBBx94lbpLly7+uqhK/+mnn/oQed2rj/r8oeHnAAAAAIC8K2pCt4wePdqr0pqz/PHHH/sWXtrjWltqSb9+/WzHjh0eWjUMvGvXrl71fuutt8JtaH60tvRq1qyZDw9ft26d3XXXXTZ//nzfjkth+bbbbrMxY8bYQw89lGGftGCaztVwcIXi66+/3u644w677777MvzbQoUK+WroCvka2q5tw7Qaui4shGhYufbq1ufVkHVtF6aLCfrsAAAAAIC8LSY5cqIz8iTN6VZFP37gVNv5u3653R0AAAAAuGZy2MmTJ310dnr+b4UxAAAAAACQ4wjd2dChQwcfEp7WLbN7eAMAAAAA8q+omtOd1yQlJfle4WmJi4u76v0BAAAAAEQXQnc2pN5PHAAAAACASAwvBwAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6M5HNozpmttdAAAAAABEIHQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAPkxdCcnJ1vfvn0tLi7OYmJibOfOnbnZHQAAAAAA8k/oXrVqlc2aNctWrFhhR44csYYNG2a7zV69ellCQoJFg0OHDvnFhNS39957L3zO3/72N/vxj39s1apV82MvvvhirvYZAAAAAJBzClkuOnjwoFWsWNFatmxp0ebixYseggsUyP51iTVr1tjNN98cfl62bNnw4zNnzliNGjXsoYceskGDBmX7vQAAAAAA0SPXKt2qSA8cONAOHz7s4VaV3kuXLtn48eOtevXqVrRoUYuPj7fFixenCMK9e/cOH69bt65NmjQpfHzkyJE2e/ZsW7ZsWbiqvH79er/p8YkTJ8Lnaii7XlM1WlRxL126tC1fvtwaNGhgsbGx3rdz587Z0KFDrXLlylasWDFr3ry5t5cVCtkVKlQI3woXLhw+dtttt1liYqI9/PDD/p4AAAAAgPwj1yrdCss1a9a06dOn27Zt26xgwYIeuOfNm2dTp0612rVr24YNG6x79+5Wrlw5a9u2rYfyKlWq2KJFizzIbt682eeEq1reuXNnD8d79uyxU6dO2cyZM/19NF9c52WGqs4TJkywpKQkb798+fI2YMAA2717ty1YsMAqVapkS5cutfbt29uuXbu8j5nxwAMP2DfffGN16tSxZ555xp8DAAAAAPK/XAvdpUqVshIlSnjYVvVXFeVx48b5UOwWLVr4ORp2vXHjRps2bZqHblWIR40aFW5DFe8tW7bYwoULPXQXL17cK+BqS21m1YULF2zKlCleYRdVuhXeda/ALQr2mouu19Xfy1F/Jk6caHfeeacPU3/99dd9vvkbb7yRreCtz6dbiC4yAAAAAACiT67O6Y504MABrzS3a9cuxevnz5+3pk2bhp9PnjzZZsyY4UH47NmzfrxJkyY50ociRYpY48aNw89VzdaQdlWoIynwRs7LTs/1119vgwcPTjGU/LPPPvPh5NkJ3RoREHnxAQAAAAAQnaImdJ8+fdrvV65c6fOnI4XmOmuItyrNqh6rGq5KuQLs1q1bL9t2aDE0bVEWWdVOTVVyzfOO7JMq8du3b/f71FXsK6E54W+//bZlx7Bhw1KEeVW6q1atmq02AQAAAAD5OHRHLl6moeRp2bRpk6903r9//xQroKeuVqs6HUlzwkXbkpUpU8YfZ2ZPcFXY1dbRo0etdevWlhP0vpqDnh36nlh0DQAAAACiX9SEblWtVcXWtllaMK1Vq1Z28uRJD9olS5a0nj17+sJlc+bMsdWrV/t87rlz5/oibHocolXQdXzfvn0+BFxzx2vVquWVYK1uPnbsWNu/f79XyzOiYeXdunWzHj16+PkK4ceOHbO1a9f6MPSOHTte9u+1krouAoSGxy9ZssSHxmuhthANj9dCbaHHn376qQdzVdLVbwAAAABA3pVrW4alZfTo0TZixAifs1y/fn1fJVzDzUOhul+/ftapUyfr0qWLD9M+fvx4iqq39OnTx7cSa9asmVe4Fdq1ANv8+fNt7969Hpa1QvmYMWMy1SctmKbQPWTIEG9XC6Ep6N94442Z/ky33nqr91dbmb322mv22GOPhY9rjrdCuW6qxL/wwgv++IknnsjSdwcAAAAAiD4xyZETnZEnaU63KvoaGaBRAQAAAACA6MhhUVXpBgAAAAAgPyF0Z0OHDh187nVat4z28AYAAAAA5H9Rs5BaXqQF0bRXeFri4uKuen8AAAAAANGF0J0NqfcTBwAAAAAgEsPLAQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAID8GLqTk5Otb9++FhcXZzExMbZz587c7A4AAAAAAPkndK9atcpmzZplK1assCNHjljDhg2z3WavXr0sISHBos2BAwesRIkSVrp06RSvv/zyy9a6dWsrU6aM3+6++257//33c62fAAAAAIB8EroPHjxoFStWtJYtW1qFChWsUKFCFi0uXrxoly5dypG2Lly4YF27dvVwndr69ev92Lp162zLli1WtWpV++EPf2iffvppjrw3AAAAAOAaDN2qSA8cONAOHz7sQ8urVavmIXf8+PFWvXp1K1q0qMXHx9vixYtTBOHevXuHj9etW9cmTZoUPj5y5EibPXu2LVu2zNvUTaFWNz0+ceJE+FwNZddrhw4d8uequKsKvXz5cmvQoIHFxsZ6386dO2dDhw61ypUrW7Fixax58+beXlYMHz7c6tWrZ507d/7OsVdffdX69+9vTZo08XOSkpL8e1i7du0VfrMAAAAAgGiRa6VlheWaNWva9OnTbdu2bVawYEEP3PPmzbOpU6da7dq1bcOGDda9e3crV66ctW3b1sNolSpVbNGiRVa2bFnbvHmzzwlXtVyBVuF4z549durUKZs5c6a/j+aL67zMOHPmjE2YMMGDr9ovX768DRgwwHbv3m0LFiywSpUq2dKlS619+/a2a9cu72NG3nnnHe+vQv6SJUsy1QdVxtXv9OhCgG4h+rwAAAAAgOiTa6G7VKlSPsdZYVtDyxUix40bZ2vWrLEWLVr4OTVq1LCNGzfatGnTPHQXLlzYRo0aFW5DFW8NyV64cKGH7uLFi3sFXG2pzaxS2J0yZYpX2EWVboV33Stwi4K95qLrdfX3co4fP+4VfV1IKFmyZKb68Oyzz/p7aW53enRxIvJ7AAAAAABEp0LRtNCYqrzt2rVL8fr58+etadOm4eeTJ0+2GTNmeBA+e/asH9fQ7JxQpEgRa9y4cfi5qtka0l6nTp0U5ynUqxKekT59+tgjjzxibdq0ydT7//d//7dX1DV8/brrrkv3vGHDhtngwYNTVLo1FxwAAAAAEF2iJnSfPn3a71euXOnzpyNpfrUokKrSPHHiRK+Gq1KemJhoW7duvWzbBQoUCG9RFlnVTk1Vcs3zjuyTKvHbt2/3+0iqqmdmaLnmiL/wwgvh99cQeS0Yp2H1jz/+ePhcnaPQrUp/ZPBPi76P0HcCAAAAAIheURO6Ixcv01DytGzatMlXOtfCY5EroKeuVqs6HUlzwkXbkmlbLsnMnuCqsKuto0ePprnyeEY09D2yL1rgTXPGNcc88sLCr3/9axs7dqytXr3amjVrluX3AQAAAABEp6gJ3apaq4o9aNAgrwa3atXKTp486UFb86F79uzpC5fNmTPHw6nmc8+dO9cXYdPjEK2CruP79u3zIeCaO16rVi0ffq3VzRVu9+/f79XyjGhYebdu3axHjx5+vkL4sWPHfGVxVaM7dux42b+vX79+iucffPCBV90j9yNXCP/lL39pf/zjH73vn3/+ebiSnplqOgAAAAAgeuXqPt2pjR492kaMGOELhSmwapVwDTcPhep+/fpZp06drEuXLr51lxYqi6x6h+ZRaysxVYxV4VZo1wJs8+fPt71793pYVtAdM2ZMpvqkBdMUuocMGeLtJiQkeNC/8cYbc+Qzv/TSSz4v/Sc/+Ymvwh66hYakAwAAAADyrpjkyInOyJO0kJoq+hoZkNlV0gEAAAAAweewqKp0AwAAAACQnxC6s6FDhw7hudepbxnt4Q0AAAAAyP+iZiG1vCgpKcn3Ck9LXFzcVe8PAAAAACC6ELqzIfV+4gAAAAAARGJ4OQAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAA+TF0JycnW9++fS0uLs5iYmJs586dudkdAAAAAADyT+hetWqVzZo1y1asWGFHjhyxhg0bZrvNXr16WUJCgkWDQ4cO+cWE1Lf33nsvfM5dd92V5jkdO3bM1b4DAAAAALKvkOWigwcPWsWKFa1ly5YWbS5evOjht0CB7F+XWLNmjd18883h52XLlg0/XrJkiZ0/fz78/Pjx4xYfH28PPfRQtt8XAAAAAHCNVrpVkR44cKAdPnzYw221atXs0qVLNn78eKtevboVLVrUw+fixYtTBOHevXuHj9etW9cmTZoUPj5y5EibPXu2LVu2LFwxXr9+vd/0+MSJE+FzNZRdr6kaLaq4ly5d2pYvX24NGjSw2NhY79u5c+ds6NChVrlyZStWrJg1b97c28sKhewKFSqEb4ULFw4f09D6yGNvv/22fe973yN0AwAAAEA+kGuVboXlmjVr2vTp023btm1WsGBBD9zz5s2zqVOnWu3atW3Dhg3WvXt3K1eunLVt29ZDeZUqVWzRokUeZDdv3uxzwlUt79y5s4fjPXv22KlTp2zmzJnhUKvzMuPMmTM2YcIES0pK8vbLly9vAwYMsN27d9uCBQusUqVKtnTpUmvfvr3t2rXL+5gZDzzwgH3zzTdWp04de+aZZ/x5el555RV7+OGHPeCnRxcCdAvR5wUAAAAARJ9cC92lSpWyEiVKeNhWhVchcty4cT4Uu0WLFn5OjRo1bOPGjTZt2jQP3aoQjxo1KtyGKt5btmyxhQsXeuguXry4V8DVltrMqgsXLtiUKVO8wi6qdCu8616BWxTsNRddr6u/l6P+TJw40e68804fpv7666/7fPM33ngjzeD9/vvv21//+lcP3pejixOR3wMAAAAAIDrl6pzuSAcOHPBKc7t27VK8rvnOTZs2DT+fPHmyzZgxw4Pw2bNn/XiTJk1ypA9FihSxxo0bh5+rmq0h7apQR1Koj5yXnZ7rr7/eBg8eHH5+22232WeffWaJiYlphm6F7UaNGtntt99+2XaHDRuWol1VuqtWrZphfwAAAAAA12joPn36tN+vXLnS509H0vxq0RBvVZpVPVY1XJVyBditW7detu3QYmjaoiyyqp2aquSa5x3ZJ1Xit2/f7vepq9hXQnPCNW87ta+//to/3/PPP59hG/o+Qt8JAAAAACB6RU3ojly8TEPJ07Jp0yZf6bx///4pVkBPXa1WdTqS5oSLtiUrU6aMP87MnuCqsKuto0ePWuvWrS0n6H01Bz01zVNXBV1z2AEAAAAA+UPUhG5VrVXFHjRokC+Y1qpVKzt58qQH7ZIlS1rPnj194bI5c+bY6tWrfT733LlzfRE2PQ7RKug6vm/fPh8CrrnjtWrV8uHXWt187Nixtn//fq+WZ0TDyrt162Y9evTw8xXCjx07ZmvXrvVh6Bntpa2V1HURIDQ8XtuDaWi8FmpLa2i55ntnZtg6AAAAACBviJrQLaNHj/aqtBYK+/jjj30Lr1tuucWee+45P96vXz/bsWOHdenSxYeBd+3a1aveb731VriNPn36+JZezZo18+Hh69ats7vuusvmz59vTz75pIdlza0eM2ZMprbl0oJpOnfIkCH26aef+jztO+64w+67775Mf6ZPPvnEChUqZPXq1bPXXnvNfvKTn6Q4RxcItGDcn/70pyx/ZwAAAACA6BWTHDnRGXmSFlJTRV8jAzQqAAAAAAAQHTns/1YYAwAAAAAAOY7QnQ0dOnTwVczTumW0hzcAAAAAIP+LqjndeY0WRNNe4WmJi4u76v0BAAAAAEQXQnc2pN5PHAAAAACASAwvBwAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgP4bu5ORk69u3r8XFxVlMTIzt3LkzN7sDAAAAAED+Cd2rVq2yWbNm2YoVK+zIkSPWsGHDbLfZq1cvS0hIsGiwb98++/73v2833HCDXXfddVajRg0bPny4XbhwIXzOkiVLrFmzZla6dGkrVqyYNWnSxObOnZur/QYAAAAA5IxClosOHjxoFStWtJYtW1q0uXjxolffCxS48usShQsXth49etgtt9ziofqjjz6yPn362KVLl2zcuHF+jqr8v/jFL6xevXpWpEgRvwDx2GOPWfny5e2ee+7JwU8EAAAAALhmKt2qSA8cONAOHz7s4bZatWoeRsePH2/Vq1e3okWLWnx8vC1evDhFEO7du3f4eN26dW3SpEnh4yNHjrTZs2fbsmXLvE3d1q9f7zc9PnHiRPhcDWXXa4cOHfLnqrgrGC9fvtwaNGhgsbGx3rdz587Z0KFDrXLlyl6Jbt68ubeXGapsK0Drc9x00032wAMPWLdu3ezdd98Nn3PXXXfZgw8+aPXr17eaNWvaU089ZY0bN7aNGzfm0DcNAAAAALjmKt0KywqZ06dPt23btlnBggU9cM+bN8+mTp1qtWvXtg0bNlj37t2tXLly1rZtWw/lVapUsUWLFlnZsmVt8+bNPidc1fLOnTt7ON6zZ4+dOnXKZs6cGa4k67zMOHPmjE2YMMGSkpK8fVWbBwwYYLt377YFCxZYpUqVbOnSpda+fXvbtWuX9zErDhw44EPqO3XqlO4c93feeceHpasf6dGFAN1C9HkBAAAAANEn10J3qVKlrESJEh62K1So4CFSQ67XrFljLVq0CFeKVfGdNm2ah24N1x41alS4DVW8t2zZYgsXLvTQXbx4ca+Aqy21mVWaaz1lyhSvTIsq3QrvulfgFgV7BWe9HhoinhENn//www+9X7pI8Pzzz6c4fvLkSa+k67i+D/WhXbt26banixOR3wMAAAAAIDrl6pzu1FVgVZpTh83z589b06ZNw88nT55sM2bM8CB89uxZP67Fx3KC5lRraHeIqtka0l6nTp0U5ykcqxKeWa+99pp99dVXPqf76aefthdeeMGeeeaZ8HFdfNBw99OnT9vatWtt8ODBfsFBQ8/TMmzYMD8nstJdtWrVLH5aAAAAAMA1E7oVOGXlypVe9Y2k+dWiId6qNE+cONGr4QqriYmJtnXr1su2HVoMTcO3QyJXEA9RlVzzvCP7pMrz9u3b/T6SquqZFQrEmiuuEK9q95AhQ8Jtqn+1atXyx7qAoCHyqmanF7r1fYS+EwAAAABA9Iqa0B25eJmGkqdl06ZNPlS7f//+KVZAT12tVrCNpDnhom3JypQp448zsye4Kuxq6+jRo9a6dWvLCZqXrsCv+9RBPvKcyDnbAAAAAIC8KWpCt6rWqmIPGjTIQ2erVq18rrOCdsmSJa1nz56+cNmcOXNs9erVPp9b+1lrETY9DtEq6Dquxcg0BFxzx1VFVrVZq5uPHTvW9u/f79XyjGhYuVYb17ZfOl8h/NixYz4EXMPQO3bseNm/f/XVV30eeqNGjfyCwgcffOBDw7t06eKviyra2qdbi8opaL/55pv+uV566aUc+FYBAAAAALkpakK3jB492qvSCqIff/yxb+GlPa6fe+45P96vXz/bsWOHh1YNA+/atatXvd96661wG9oHW1t6KchqePi6det8mPb8+fPtySef9LB822232ZgxY+yhhx7KsE9aME3najj4p59+atdff73dcccddt9992X4t4UKFfJVyBXyNbRd24ZpNXRdWAj5+uuv/TP87//+rw9v137dWsFdnxEAAAAAkLfFJEdOdEaepIXUVNHXyACNCgAAAAAAREcO+78VxgAAAAAAQI4jdGdDhw4dfBXztG6Z3cMbAAAAAJB/RdWc7rwmKSnJ9wpPS1xc3FXvDwAAAAAguhC6syH1fuIAAAAAAERieDkAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAPkxdCcnJ1vfvn0tLi7OYmJibOfOnbnZHQAAAAAA8k/oXrVqlc2aNctWrFhhR44csYYNG2a7zV69ellCQoJFg0OHDvnFhNS39957L8V5L774otWtW9eKFi1qVatWtUGDBtk333yTa/0GAAAAAOSMQpaLDh48aBUrVrSWLVtatLl48aIH5AIFsn9dYs2aNXbzzTeHn5ctWzb8+I9//KP9/Oc/txkzZvj3sH//fr9woPf+zW9+k+33BgAAAABcg5VuBcuBAwfa4cOHPWBWq1bNLl26ZOPHj7fq1at71Tc+Pt4WL16cIgj37t07fFzV4UmTJoWPjxw50mbPnm3Lli0LV5XXr1/vNz0+ceJE+FwNZddrqkaLKu6lS5e25cuXW4MGDSw2Ntb7du7cORs6dKhVrlzZihUrZs2bN/f2skIhu0KFCuFb4cKFw8c2b95sd955pz3yyCP+Hfzwhz+0rl272vvvv5/NbxgAAAAAcM1WuhWWa9asadOnT7dt27ZZwYIFPXDPmzfPpk6darVr17YNGzZY9+7drVy5cta2bVsP5VWqVLFFixZ5kFVg1ZxwVcs7d+7s4XjPnj126tQpmzlzpr+P5ovrvMw4c+aMTZgwwZKSkrz98uXL24ABA2z37t22YMECq1Spki1dutTat29vu3bt8j5mxgMPPODDxevUqWPPPPOMPw9RdVufWSH79ttvt48//tjefPNNe/TRR6/wmwUAAAAA2LUeukuVKmUlSpTwsK3qryrK48aN86HYLVq08HNq1KhhGzdutGnTpnnoVoV41KhR4TZU8d6yZYstXLjQQ3fx4sW9Aq621GZWXbhwwaZMmeIVdlGlW+Fd9wrcomCvueh6Xf29HPVn4sSJXsnWMPXXX3/d55u/8cYb4eCtCvcXX3xhrVq18oXlvv32W/vP//xPe+6559JtV59PtxBdZAAAAAAARJ9cndMd6cCBA15pbteuXYrXz58/b02bNg0/nzx5ss9/VhA+e/asH2/SpEmO9KFIkSLWuHHj8HNVszWkXRXqSAq8kfOy03P99dfb4MGDw89vu+02++yzzywxMTEcujVUXeFdYV9D1/U9PPXUUzZ69GgbMWJEmu1qREDkxQcAAAAAQHSKmtB9+vRpv1+5cqXPn46k+dWiId6qNKt6rGq4KuUKsFu3br1s26HF0FRJjqxqp6YqueZ5R/ZJlfjt27f7feoq9pVQsH777bfDzxWsNZT8iSee8OeNGjWyr7/+2ofN/+IXv0hzIbdhw4alCPOqdGvVcwAAAABAdIma0B25eJmGkqdl06ZNPge6f//+KVZAT12tVnU6kuaEi7YlK1OmjD/OzJ7gqrCrraNHj1rr1q0tJ+h9NQc9RNX91ME6FPAjLxJE0vcUuhABAAAAAIheURO6VbVWFVt7VGvBNM1xPnnypAftkiVLWs+ePX3hsjlz5tjq1at9PvfcuXN9ETY9DtEK4Dq+b98+HwKuueO1atXySrBWNx87dqxvy6VqeUY0rLxbt27Wo0cPP18h/NixY7Z27Vofht6xY8fL/r1WUtdFgNDw+CVLlvjQeC3UFnL//ff71mA6JzS8XNVvvZ66ug4AAAAAyFuiJnSL5jGrKq05y1rFW1t43XLLLeFFxfr162c7duywLl26+DBwba2lqvdbb70VbqNPnz4+T7pZs2Y+PHzdunV211132fz58+3JJ5/0sKy51WPGjLGHHnoowz5pwTSdO2TIEPv00099nvYdd9xh9913X6Y/0yeffGKFChWyevXq2WuvvWY/+clPwseHDx/un0X3al+fX4FbFwcAAAAAAHlbTHJ6Y5iRZ2hOtyr6GhmgUQEAAAAAgOjIYd9dpQsAAAAAAOQIQnc2dOjQwVcxT+uW0R7eAAAAAID8L6rmdOc1WhBNe4WnJS4u7qr3BwAAAAAQXQjd2ZB6P3EAAAAAACIxvBwAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAAyI+hOzk52fr27WtxcXEWExNjO3fuzM3uAAAAAACQf0L3qlWrbNasWbZixQo7cuSINWzYMNtt9urVyxISEizaHDhwwEqUKGGlS5dO95wFCxb4xYdo7D8AAAAAII+F7oMHD1rFihWtZcuWVqFCBStUqJBFi4sXL9qlS5dypK0LFy5Y165drXXr1umec+jQIRs6dOhlzwEAAAAA5C25FrpVkR44cKAdPnzYq7vVqlXzkDt+/HirXr26FS1a1OLj423x4sUpgnDv3r3Dx+vWrWuTJk0KHx85cqTNnj3bli1b5m3qtn79er/p8YkTJ8Lnaii7XlPYFVXcVYVevny5NWjQwGJjY71v586d8zBcuXJlK1asmDVv3tzby4rhw4dbvXr1rHPnzmke1+fq1q2bjRo1ymrUqHEF3yYAAAAAIBrlWmlZYblmzZo2ffp027ZtmxUsWNAD97x582zq1KlWu3Zt27Bhg3Xv3t3KlStnbdu29VBepUoVW7RokZUtW9Y2b97sc8JVLVegVTjes2ePnTp1ymbOnOnvo/niOi8zzpw5YxMmTLCkpCRvv3z58jZgwADbvXu3D/2uVKmSLV261Nq3b2+7du3yPmbknXfe8f4q5C9ZsiTNc55//nl/L11QePfddzNsUxcCdAvR5wUAAAAARJ9cC92lSpXyOc4K2xparhA5btw4W7NmjbVo0cLPUdV348aNNm3aNA/dhQsX9mpwiCreW7ZssYULF3roLl68uFfA1ZbavJJh4FOmTPEKu6jSrfCuewVuUbDXXHS9rv5ezvHjx72irwsJJUuWTPMcfb5XXnklS4vI6eJE5PcAAAAAAIhOhaJpoTFVmtu1a5fi9fPnz1vTpk3DzydPnmwzZszwIHz27Fk/3qRJkxzpQ5EiRaxx48bh56pma+h3nTp1UpynUK9KeEb69OljjzzyiLVp0ybN41999ZU9+uij9vLLL9v111+f6X4OGzbMBg8enKLSXbVq1Uz/PQAAAADgGgvdp0+f9vuVK1f6/OlIml8tGuKtSvPEiRO9Gq5KeWJiom3duvWybRcoUCC8RVlkVTs1Vck1zzuyT6rEb9++3e8jqaqemaHlmiP+wgsvhN9fQ+S1YJyG1d9yyy0+p/z+++8P/01o8Tads2/fPh+Cn5q+j9B3AgAAAACIXlETuiMXL9NQ8rRs2rTJVzrv379/ihXQU1erVZ2OpDnhom3JypQp448zM5xbFXa1dfTo0StaVVxD3yP7ogXeNGdcc8x1YUEhX9X01IuuqQKuOe9UrwEAAAAgb4ua0K2qtarYgwYN8mpvq1at7OTJkx60NR+6Z8+evnDZnDlzbPXq1T6fe+7cub4Imx6HaBV0HVeVWEPANXe8Vq1aHmC1uvnYsWNt//79Xi3PiIaVa1XxHj16+PkK4ceOHbO1a9f6MPSOHTte9u/r16+f4vkHH3zgVffI/chT700e2sc7J/YsBwAAAABcw/t0pzZ69GgbMWKELxSmwKpVwjXcPBSq+/XrZ506dbIuXbr41l1aqCyy6h2aR62txJo1a+YVboV2LcA2f/5827t3r4dlVZvHjBmTqT5pwTSF7iFDhni7CQkJHvRvvPHGQL4DAAAAAED+EZMcOdEZeZIWUlNFXyMD0lslHQAAAABw9XNYVFW6AQAAAADITwjd2dChQwdfxTytW0Z7eAMAAAAA8r+oWUgtL0pKSvK9wtMSFxd31fsDAAAAAIguhO5sSL2fOAAAAAAAkRheDgAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAfgzdycnJ1rdvX4uLi7OYmBjbuXNnbnYHAAAAAID8E7pXrVpls2bNshUrVtiRI0esYcOG2W6zV69elpCQYNFg37599v3vf99uuOEGu+6666xGjRo2fPhwu3DhQprnL1iwwC8+REv/AQAAAADZU8hy0cGDB61ixYrWsmVLizYXL170AFygwJVflyhcuLD16NHDbrnlFitdurR99NFH1qdPH7t06ZKNGzcuxbmHDh2yoUOHWuvWrXOg9wAAAACAa7rSrYr0wIED7fDhwx5uq1Wr5mF0/PjxVr16dStatKjFx8fb4sWLUwTh3r17h4/XrVvXJk2aFD4+cuRImz17ti1btszb1G39+vV+0+MTJ06Ez9VQdr2msCuquCsYL1++3Bo0aGCxsbHet3PnznkYrly5shUrVsyaN2/u7WWGKtuPPfaYf46bbrrJHnjgAevWrZu9++67Kc7T59Lro0aN8r8BAAAAAOQPuVbpVliuWbOmTZ8+3bZt22YFCxb0wD1v3jybOnWq1a5d2zZs2GDdu3e3cuXKWdu2bT2UV6lSxRYtWmRly5a1zZs3+5xwVcs7d+7s4XjPnj126tQpmzlzpr+P5ovrvMw4c+aMTZgwwZKSkrz98uXL24ABA2z37t0+9LtSpUq2dOlSa9++ve3atcv7mBUHDhzwIfWdOnVK8frzzz/v76ULCqkDeVp0IUC3EH1eAAAAAED0ybXQXapUKStRooSH7QoVKniI1JDrNWvWWIsWLfwcVX03btxo06ZN89Ct4dqqBoeo4r1lyxZbuHChh+7ixYt7BVxtqc2s0lzrKVOmeGVaVOlWeNe9Arco2Cs46/XUQ8TTo+HzH374ofdLFwkUskP0+V555ZUsLSKnixOR3wMAAAAAIDrl6pzu1FVgVZrbtWuX4vXz589b06ZNw88nT55sM2bM8CB89uxZP96kSZMc6UORIkWscePG4eeqZmvod506dVKcp/CsSnhmvfbaa/bVV1/5nO6nn37aXnjhBXvmmWf8tUcffdRefvllu/766zPd3rBhw2zw4MEpKt1Vq1bN9N8DAAAAAK6x0H369Gm/X7lypc+fjqT51aIh3qo0T5w40avhqpQnJiba1q1bL9t2aDE0bVEWktYK4qqSa553ZJ9Uid++fbvfR1JVPbNCgVhzxRXiVe0eMmSILySnOeX3339/+FwNoZdChQr56ucagp+avo/QdwIAAAAAiF5RE7ojFy/TUPK0bNq0yYdq9+/fP/yagmvqarWCbSTNCRdtS1amTBl/nJnh3Kqwq62jR4/m2KriCtUK/LqvV6+eV9MjaUsxVcA1553qNQAAAADkbVETulW1VhV70KBBHkhbtWplJ0+e9KBdsmRJ69mzpy9cNmfOHFu9erXP5547d64vwqbHIVoFXcdVJdYQcM0dr1WrlgdYrW4+duxY279/v1fLM6Jh5VpVXNt+6XyF8GPHjtnatWt9GHrHjh0v+/evvvqqz0Nv1KiRX1D44IMPfGh4ly5d/HXdUu9NrhXUJSf2LAcAAAAA5K6oCd0yevRor0probCPP/7YA6j2uH7uuef8eL9+/WzHjh0eWjUMvGvXrl71fuutt8JtaB9sbenVrFkzHx6+bt06u+uuu2z+/Pn25JNPeli+7bbbbMyYMfbQQw9l2CctmKZzNRz8008/9bnXd9xxh913330Z/q2GiGs1dIV8DW3XtmFaDV0XFgAAAAAA+V9McuREZ+RJWkhNFX2NDNCoAAAAAABAdOSw/1thDAAAAAAA5DhCdzZ06NDBVzFP65bZPbwBAAAAAPlXVM3pzmuSkpJ8r/C0xMXFXfX+AAAAAACiC6E7G1LvJw4AAAAAQCSGlwMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAFhy7B8IDk52e9PnTqV210BAAAAgGtCKH+F8lh6CN35wPHjx/2+atWqud0VAAAAALimfPXVV1aqVKl0jxO684G4uDi/P3z48GX/sYFov1KoC0f//Oc/rWTJkrndHSDL+A0jP+B3jLyO3zCuJlW4FbgrVap02fMI3flAgQL/NzVfgZv/ckFep98wv2PkZfyGkR/wO0Zex28YV0tmip4spAYAAAAAQEAI3QAAAAAABITQnQ/Exsbar371K78H8ip+x8jr+A0jP+B3jLyO3zCiUUxyRuubAwAAAACAK0KlGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELrziMmTJ1u1atXsuuuus+bNm9v7779/2fMXLVpk9erV8/MbNWpkb7755lXrK5ATv+OXX37ZWrdubWXKlPHb3XffneHvHoi2/y4OWbBggcXExFhCQkLgfQRy+nd84sQJ++lPf2oVK1b0xanq1KnD/65AnvoNv/jii1a3bl0rWrSoVa1a1QYNGmTffPPNVesvQOjOA1577TUbPHiwr8T44YcfWnx8vN1zzz129OjRNM/fvHmzde3a1Xr37m07duzw/5Gn21//+ter3nfgSn/H69ev99/xunXrbMuWLf7/JH/4wx/ap59+etX7DlzJbzjk0KFDNnToUL+IBOS13/H58+etXbt2/jtevHix7du3zy+KVq5c+ar3HbiS3/Af//hH+/nPf+7n79mzx1555RVv47nnnrvqfce1i9XL8wBdwbvtttvsD3/4gz+/dOmSB5CBAwf6f4mk1qVLF/v6669txYoV4dfuuOMOa9KkiU2dOvWq9h240t9xahcvXvSKt/6+R48eV6HHQPZ/w/rdtmnTxh5//HF79913vWL4xhtvXOWeA1f+O9b/bkhMTLS9e/da4cKFc6HHQPZ+wwMGDPCwvXbt2vBrQ4YMsa1bt9rGjRuvat9x7aLSHeV0hXn79u0+tDakQIEC/lzVv7To9cjzRVcA0zsfiMbfcWpnzpyxCxcuWFxcXIA9BXL2N/z8889b+fLlfeQRkBd/x8uXL7cWLVr48PIbbrjBGjZsaOPGjfMLSkBe+A23bNnS/yY0BP3jjz/26RH33nvvVes3UCi3O4DL++KLL/z/sen/0UXSc111Tsvnn3+e5vl6Hcgrv+PUnn32WatUqdJ3LigB0fobVgVFwxh37tx5lXoJ5PzvWAHlnXfesW7dunlQOXDggPXv398vgmq4LhDtv+FHHnnE/65Vq1amAb7ffvv/tXfvoTX/fwDHX2MXt2nYGJLLzN2Qe6MIEbZRCJmRmaVp5hLaMLcsVi5/GKMsiqnZXKaYCUUo18ZmmDRpY64N5fr59nrXWZsf2vDZ5Xeejzodn8s55/05fZrz+rxe79fnq0RGRlJejmpFphtArZeQkGAaUWVkZJimKUBtV1paKqGhoWbuq7e3d00PB/hjWrqr1RrJycnSv39/M4UtNjaW6WqoM7RHjFZn7Nq1y8wBT09Pl1OnTsmGDRtqemhwImS6azn9sVa/fn15/vx5hfW67Ovr+9PX6Pqq7A/UxvPYITEx0QTd2dnZEhAQYPNIgX9zDhcUFJjGU0FBQRWCF+Xq6mqaUfn5+VXDyIG/+1usHct1Lre+zqF79+6mek5Lfd3d3W0fN/A35/Dq1avNRdDw8HCzrHf10d5HERER5gKSlqcDduMsq+X0PzO9sly++YP+cNNlnWP1M7q+/P7q7Nmzv9wfqI3nsdqyZYu5En369GkZMGBANY0W+PtzWG/ZmJOTY0rLHY/g4GAZOXKk+bc2/QHqwt/iwMBAU1LuuGikHjx4YIJxAm7UhXNYe8L8GFg7LiLRTxrVRruXo3ZLTU21PDw8rJSUFCs3N9eKiIiwvLy8rOLiYrM9NDTUWrlyZdn+ly9ftlxdXa3ExEQrLy/PWrt2reXm5mbl5OTU4FHA2VX1PE5ISLDc3d2ttLQ0q6ioqOxRWlpag0cBZ1bVc/hHYWFhVkhISDWOGPj787iwsNDy9PS0oqKirPz8fCszM9Nq2bKltXHjxho8Cjizqp7D+jtYz+HDhw9bjx8/trKysiw/Pz9r2rRpNXgUcDaUl9cBOn+qpKRE1qxZY8q59NZfmvlzNJEoLCyscAVPuzTqPQnj4uJMkwh/f39zixrtOArUlfM4KSnJlC5OmTKlwvto4574+PhqHz9Q1XMY+H84j7Uq48yZMxITE2Om+Oj9uaOjo01zS6AunMP6e9jFxcU8P3v2THx8fMzUn02bNtXgUcDZcJ9uAAAAAABswiV5AAAAAABsQtANAAAAAIBNCLoBAAAAALAJQTcAAAAAADYh6AYAAAAAwCYE3QAAAAAA2ISgGwAAAAAAmxB0AwAAAABgE4JuAACcxIgRI2Tx4sU1PQwAAJyKi2VZVk0PAgAA2O/169fi5uYmnp6eUttcuHBBRo4cKW/evBEvL6+aHg4AAP+M6797KwAAUJs1b95caqMvX77U9BAAALAN5eUAADhheXmHDh1k48aNMnv2bGnSpIm0b99eTpw4ISUlJRISEmLWBQQEyPXr18ten5KSYrLQx44dE39/f2nQoIGMHTtWnj59WuFzkpKSxM/PT9zd3aVr165y8ODBCttdXFzMPsHBwdK4cWOZP3++yXKrZs2ame1z5swxy6dPn5Zhw4aZz23RooVMnDhRCgoKyt7ryZMnZv/09HTzHo0aNZI+ffrIlStXKnzm5cuXzfHrdv0MHbdm1dX3799l8+bN0rFjR2nYsKF5fVpa2j///gEAzomgGwAAJ7Vt2zYJDAyUW7duyYQJEyQ0NNQE4bNmzZKbN2+awFmXy89E+/jxo2zatEkOHDhgAtm3b9/K9OnTy7ZnZGRIdHS0LF26VO7evSsLFiyQuXPnyvnz5yt8dnx8vEyePFlycnJk3bp1cvToUbM+Pz9fioqKZMeOHWb5w4cPsmTJEhP8nzt3TurVq2dep4FyebGxsbJs2TK5ffu2dOnSRWbMmCFfv34123TdqFGjpEePHiYYv3TpkgQFBcm3b9/Mdg249Xh2794t9+7dk5iYGPMdXLx40cZvHwDgLJjTDQCAk9BMb9++fWX79u0m0z18+PCyLHRxcbG0bt1aVq9eLevXrzfrrl69KkOHDjVBsK+vr8l0awCt6wcPHmz2uX//vnTv3l2uXbsmgwYNMkF8z549JTk5uexzp02bZoLnU6dOmWXNTGvGXYP+qs7pfvnypfj4+JhgvVevXibTrRnqffv2ybx588w+ubm5Zgx5eXnSrVs3mTlzphQWFppg+0efPn0yZffZ2dnmWB3Cw8PNBYZDhw79g28eAODMyHQDAOCktHzcoVWrVua5d+/e/7PuxYsXZetcXV1l4MCBZcsa1GqQrAGu0mcNvMvTZcd2hwEDBlRqjA8fPjRZ606dOknTpk3NxQKlQfSvjkUvHpQftyPT/TOPHj0ywfWYMWNMSb3joZnv8mXsAAD8KRqpAQDgpLSTuYNmn3+17sdS7n9B53JXhpaB63zzvXv3Sps2bcxYNMP9+fPnCvv9btw6T/tX3r9/b541C9+2bdsK2zw8PKpwRAAA/ByZbgAAUGk6T7p8czWdg63zurXEXOmzzvUuT5d1PvXvaNM15ZhnrV69emXePy4uzmSq9b0dzc+qQrPgOh/8Z3RcGlxr5rxz584VHu3atavyZwEA8CMy3QAAoNI0o7xo0SLZuXOnKTWPioqSIUOGmPncavny5WYOd79+/WT06NFy8uRJ01lc50z/jmazNUOdmZkp48ePN9lp7TKuHct1friWjGtgvHLlyiqPedWqVaZsfuHChRIZGWkCfG3sNnXqVPH29jYN2LR5mmbGtVP6u3fvzIUCLWcPCwv74+8KAABFphsAAFSa3nJrxYoVpjmZztXW+c9Hjhwp2z5p0iTTeTwxMdE0M9uzZ4/s37/fNHH7HS3t1i7mGlTrXHIN5rVTeWpqqty4ccOUlGtgvHXr1iqPWbuZZ2VlyZ07d8zFAW2Ydvz4cXPRQG3YsME0kNMu5ppNHzdunCk31wZtAAD8LbqXAwCAStHu5dp1XMvJAQBA5ZDpBgAAAADAJgTdAAAAAADYhPJyAAAAAABsQqYbAAAAAACbEHQDAAAAAGATgm4AAAAAAGxC0A0AAAAAgE0IugEAAAAAsAlBNwAAAAAANiHoBgAAAADAJgTdAAAAAADYhKAbAAAAAACxx3+5BPgWn+Y27QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. FINAL REALITY CHECK\n",
      "============================================================\n",
      "Negative predictions: 0\n",
      "Prediction range: $19 - $835\n",
      "Actual range: $13 - $900\n",
      "\n",
      "6. IMPROVEMENT SUMMARY\n",
      "============================================================\n",
      "Original Model (Overfitted):\n",
      "  MAE: $62,707, R²: 0.9997, CV MAE: $668,723\n",
      "Final Model (Gradient Boosting):\n",
      "  MAE: $23, R²: 0.8812, CV MAE: $25\n",
      "Improvement: More realistic and trustworthy model\n",
      "\n",
      "CLEANING COMPLETE: From 3675 to 59 features\n",
      "REALISTIC MODEL: R² 0.8812 with proper validation\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def clean_and_rebuild_dataset(X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Clean the dataset by removing constant and problematic features\"\"\"\n",
    "    \n",
    "    print(\"CLEANING DATASET AND REBUILDING MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Remove constant features\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "    print(\"1. Removing constant features...\")\n",
    "    selector = VarianceThreshold(threshold=0.01)  # Remove features with <1% variance\n",
    "    X_train_clean = selector.fit_transform(X_train_imputed)\n",
    "    X_test_clean = selector.transform(X_test_imputed)\n",
    "    \n",
    "    n_features_original = X_train_imputed.shape[1]\n",
    "    n_features_clean = X_train_clean.shape[1]\n",
    "    print(f\"   Removed {n_features_original - n_features_clean} constant features\")\n",
    "    print(f\"   Remaining features: {n_features_clean}\")\n",
    "    \n",
    "    # 2. Remove highly correlated features\n",
    "    print(\"2. Removing highly correlated features...\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(X_train_clean, rowvar=False)\n",
    "    \n",
    "    # Find features to remove (correlation > 0.95)\n",
    "    to_drop = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i+1, len(corr_matrix)):\n",
    "            if abs(corr_matrix[i, j]) > 0.95:\n",
    "                to_drop.add(j)\n",
    "    \n",
    "    # Keep only non-redundant features\n",
    "    keep_indices = [i for i in range(X_train_clean.shape[1]) if i not in to_drop]\n",
    "    X_train_clean = X_train_clean[:, keep_indices]\n",
    "    X_test_clean = X_test_clean[:, keep_indices]\n",
    "    \n",
    "    print(f\"   Removed {len(to_drop)} highly correlated features\")\n",
    "    print(f\"   Final features: {X_train_clean.shape[1]}\")\n",
    "    \n",
    "    # 3. Create meaningful feature names for the cleaned dataset\n",
    "    feature_names_clean = [f'feature_{i}' for i in range(X_train_clean.shape[1])]\n",
    "    \n",
    "    return X_train_clean, X_test_clean, feature_names_clean\n",
    "\n",
    "def train_final_models(X_train_clean, y_train, X_test_clean, y_test):\n",
    "    \"\"\"Train models on cleaned data\"\"\"\n",
    "    \n",
    "    print(\"\\n3. TRAINING MODELS ON CLEANED DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_split=15,\n",
    "            min_samples_leaf=10,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        # ),\n",
    "        # 'Random Forest': RandomForestRegressor(\n",
    "        #     n_estimators=150,\n",
    "        #     max_depth=15,\n",
    "        #     min_samples_split=15,\n",
    "        #     min_samples_leaf=5,\n",
    "        #     max_features='sqrt',\n",
    "        #     bootstrap=True,\n",
    "        #     random_state=42,\n",
    "        #     n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_clean, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_clean)\n",
    "            \n",
    "            # Ensure no negative predictions\n",
    "            y_pred = np.maximum(y_pred, 0)  # Clip negative predictions to 0\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_clean, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Train score for overfitting check\n",
    "            y_pred_train = model.predict(X_train_clean)\n",
    "            train_r2 = r2_score(y_train, y_pred_train)\n",
    "            overfit_gap = train_r2 - r2\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'train_r2': train_r2,\n",
    "                'overfit_gap': overfit_gap,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test R²: {r2:.4f}\")\n",
    "            print(f\"Train R²: {train_r2:.4f}\")\n",
    "            print(f\"Overfit Gap: {overfit_gap:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} ± ${cv_std:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, X_train_clean, top_n=20):\n",
    "    \"\"\"Analyze which features are most important\"\"\"\n",
    "    \n",
    "    print(f\"\\n4. FEATURE IMPORTANCE ANALYSIS (Top {top_n})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(top_n))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=importance_df.head(top_n), x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Most Important Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model doesn't support feature importance analysis\")\n",
    "        return None\n",
    "\n",
    "def run_complete_cleaning_pipeline(X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Complete pipeline for cleaning data and rebuilding models\"\"\"\n",
    "    \n",
    "    print(\"STARTING DATA CLEANING AND MODEL REBUILDING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Clean the dataset\n",
    "    X_train_clean, X_test_clean, feature_names_clean = clean_and_rebuild_dataset(\n",
    "        X_train_imputed, y_train, X_test_imputed, y_test\n",
    "    )\n",
    "\n",
    "    # Step 2: Train models on cleaned data\n",
    "    final_results = train_final_models(X_train_clean, y_train, X_test_clean, y_test)\n",
    "\n",
    "    if final_results:\n",
    "        # Step 3: Analyze results\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL MODEL RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for name, result in final_results.items():\n",
    "            print(f\"{name:20} | MAE: ${result['mae']:,.2f} | R²: {result['r2']:.4f} | Overfit: {result['overfit_gap']:.4f}\")\n",
    "        \n",
    "        # Step 4: Find best model\n",
    "        best_final_name = min(final_results.keys(), key=lambda x: final_results[x]['mae'])\n",
    "        best_final_model = final_results[best_final_name]['model']\n",
    "        best_final_result = final_results[best_final_name]\n",
    "        \n",
    "        print(f\"\\nBEST FINAL MODEL: {best_final_name}\")\n",
    "        print(f\"   Test MAE: ${best_final_result['mae']:,.2f}\")\n",
    "        print(f\"   Test R²: {best_final_result['r2']:.4f}\")\n",
    "        print(f\"   CV MAE: ${best_final_result['cv_mae']:,.2f} ± ${best_final_result['cv_std']:,.2f}\")\n",
    "        print(f\"   Overfit Gap: {best_final_result['overfit_gap']:.4f}\")\n",
    "        \n",
    "        # Step 5: Feature importance analysis\n",
    "        importance_df = analyze_feature_importance(\n",
    "            best_final_model, feature_names_clean, X_train_clean\n",
    "        )\n",
    "        \n",
    "        # Step 6: Final reality check\n",
    "        print(f\"\\n5. FINAL REALITY CHECK\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        y_pred_final = best_final_result['predictions']\n",
    "        \n",
    "        # Check for negative predictions\n",
    "        negative_count = (y_pred_final < 0).sum()\n",
    "        print(f\"Negative predictions: {negative_count}\")\n",
    "        \n",
    "        # Check prediction range\n",
    "        print(f\"Prediction range: ${y_pred_final.min():,.0f} - ${y_pred_final.max():,.0f}\")\n",
    "        print(f\"Actual range: ${y_test.min():,.0f} - ${y_test.max():,.0f}\")\n",
    "        \n",
    "        # Compare with original problematic model\n",
    "        print(f\"\\n6. IMPROVEMENT SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Original Model (Overfitted):\")\n",
    "        print(f\"  MAE: $62,707, R²: 0.9997, CV MAE: $668,723\")\n",
    "        print(f\"Final Model ({best_final_name}):\")\n",
    "        print(f\"  MAE: ${best_final_result['mae']:,.0f}, R²: {best_final_result['r2']:.4f}, CV MAE: ${best_final_result['cv_mae']:,.0f}\")\n",
    "        print(f\"Improvement: More realistic and trustworthy model\")\n",
    "        \n",
    "        # Save the cleaned data and final model\n",
    "        final_model_data = {\n",
    "            'model': best_final_model,\n",
    "            'feature_names': feature_names_clean,\n",
    "            'X_train_clean': X_train_clean,\n",
    "            'X_test_clean': X_test_clean,\n",
    "            'results': final_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCLEANING COMPLETE: From {X_train_imputed.shape[1]} to {X_train_clean.shape[1]} features\")\n",
    "        print(f\"REALISTIC MODEL: R² {best_final_result['r2']:.4f} with proper validation\")\n",
    "        \n",
    "        return final_model_data\n",
    "    else:\n",
    "        print(\"No models trained successfully\")\n",
    "        return None\n",
    "\n",
    "# Run the complete cleaning pipeline\n",
    "print(\"Running complete data cleaning and model rebuilding...\")\n",
    "final_model_data = run_complete_cleaning_pipeline(X_train_imputed, y_train, X_test_imputed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b77f5a4-6093-444d-991f-1827a56abca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating dominant feature and creating balanced models...\n",
      "INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\n",
      "============================================================\n",
      "1. Analyzing Feature 0...\n",
      "Error in main analysis: cannot reindex on an axis with duplicate labels\n",
      "Using safe alternative...\n",
      "INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\n",
      "============================================================\n",
      "1. Analyzing dominant feature in cleaned data...\n",
      "Top 5 features by correlation with target:\n",
      "   feature_2: correlation = 0.9127\n",
      "   feature_23: correlation = 0.2266\n",
      "   feature_0: correlation = 0.2073\n",
      "   feature_44: correlation = 0.2068\n",
      "   feature_45: correlation = 0.2021\n",
      "   Dominant feature is: feature_2 (index 2)\n",
      "\n",
      "2. Feature importance distribution in cleaned model:\n",
      "Top 10 features by importance:\n",
      "       feature  importance\n",
      "2    feature_2    0.928879\n",
      "45  feature_45    0.014064\n",
      "1    feature_1    0.009794\n",
      "0    feature_0    0.008194\n",
      "6    feature_6    0.006616\n",
      "23  feature_23    0.003383\n",
      "17  feature_17    0.003291\n",
      "50  feature_50    0.003029\n",
      "47  feature_47    0.002618\n",
      "21  feature_21    0.001715\n",
      "\n",
      "3. Creating balanced models without the dominant feature...\n",
      "   Training with 58 features (excluding feature_2)\n",
      "\n",
      "--- Training Gradient Boosting (Balanced) ---\n",
      "Test MAE: $22.50\n",
      "Test R²: 0.8902\n",
      "CV MAE: $24.27 ± $0.92\n",
      "\n",
      "--- Training XGBoost (Balanced) ---\n",
      "Test MAE: $22.50\n",
      "Test R²: 0.8870\n",
      "CV MAE: $24.47 ± $1.15\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "CLEANED Models:\n",
      "  Gradient Boosting: MAE: $23, R²: 0.8812\n",
      "  XGBoost: MAE: $23, R²: 0.8762\n",
      "\n",
      "BALANCED Models (no dominant feature):\n",
      "  Gradient Boosting (Balanced): MAE: $23, R²: 0.8902\n",
      "  XGBoost (Balanced): MAE: $22, R²: 0.8870\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      "BEST OVERALL MODEL: XGBoost (Balanced)\n",
      "   Test MAE: $22.50\n",
      "   Test R²: 0.8870\n",
      "   CV MAE: $24.47\n",
      "   CV/Test Ratio: 1.09x\n",
      "   GOOD: Reasonable CV-Test consistency\n",
      "\n",
      "DEPLOYMENT RECOMMENDATION:\n",
      "Use the BALANCED model - it's more robust and doesn't rely on a single dominant feature\n",
      "\n",
      "FINAL MODEL SELECTED: XGBoost (Balanced)\n",
      "PERFORMANCE: MAE $22, R² 0.8870\n",
      "MODEL SAVED: rental_model_final.pkl\n",
      "READY FOR DEPLOYMENT!\n",
      "\n",
      "FINAL FEATURE IMPORTANCE (Top 10):\n",
      "       feature  importance\n",
      "1    feature_2    0.191799\n",
      "22  feature_23    0.081179\n",
      "25  feature_26    0.075853\n",
      "50  feature_51    0.043353\n",
      "13  feature_14    0.042889\n",
      "17  feature_18    0.033448\n",
      "0    feature_1    0.033182\n",
      "43  feature_44    0.031871\n",
      "44  feature_45    0.031675\n",
      "11  feature_12    0.029730\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "def investigate_feature_0_and_improve(final_model_data, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Investigate the dominant feature and create even better models\"\"\"\n",
    "    \n",
    "    print(\"INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check what Feature 0 might be by looking at correlations with original data\n",
    "    print(\"1. Analyzing Feature 0...\")\n",
    "    \n",
    "    # FIX: Create a temporary dataframe with unique column names\n",
    "    n_features = X_train_imputed.shape[1]\n",
    "    \n",
    "    # Generate unique column names to avoid duplicate labels\n",
    "    unique_columns = []\n",
    "    for i in range(n_features):\n",
    "        unique_columns.append(f'feature_{i}')\n",
    "    \n",
    "    temp_df = pd.DataFrame(X_train_imputed, columns=unique_columns)\n",
    "    temp_df['target'] = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    \n",
    "    # Find which original feature has highest correlation with target\n",
    "    correlations = []\n",
    "    for i in range(n_features):\n",
    "        col_name = f'feature_{i}'\n",
    "        corr = np.corrcoef(temp_df[col_name], temp_df['target'])[0, 1]\n",
    "        correlations.append((i, abs(corr)))\n",
    "    \n",
    "    # Sort by correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 5 features by correlation with target:\")\n",
    "    for i, (feature_idx, corr) in enumerate(correlations[:5]):\n",
    "        print(f\"   Feature {feature_idx}: correlation = {corr:.4f}\")\n",
    "    \n",
    "    dominant_feature_idx = correlations[0][0]\n",
    "    print(f\"   Dominant feature is likely feature {dominant_feature_idx}\")\n",
    "    \n",
    "    # 2. Check what Feature 0 in our cleaned data represents\n",
    "    print(f\"\\n2. Feature importance distribution in cleaned model:\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': final_model_data['feature_names'],\n",
    "        'importance': final_model_data['results']['Gradient Boosting']['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features by importance:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # 3. Create models without the overly dominant feature\n",
    "    print(f\"\\n3. Creating balanced models without the dominant feature...\")\n",
    "    \n",
    "    # Check if we have multiple features to work with\n",
    "    if len(final_model_data['feature_names']) > 1:\n",
    "        # Remove the dominant feature (feature_0)\n",
    "        X_train_balanced = final_model_data['X_train_clean'][:, 1:]  # Remove feature_0\n",
    "        X_test_balanced = final_model_data['X_test_clean'][:, 1:]   # Remove feature_0\n",
    "        feature_names_balanced = final_model_data['feature_names'][1:]\n",
    "        \n",
    "        print(f\"   Training with {X_train_balanced.shape[1]} features (excluding dominant feature_0)\")\n",
    "        \n",
    "        # Train balanced models\n",
    "        balanced_models = {\n",
    "            'Gradient Boosting (Balanced)': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=15,\n",
    "                subsample=0.7,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'XGBoost (Balanced)': XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        balanced_results = {}\n",
    "        \n",
    "        for name, model in balanced_models.items():\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train_balanced, y_train)\n",
    "                y_pred = model.predict(X_test_balanced)\n",
    "                y_pred = np.maximum(y_pred, 0)  # No negative prices\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_balanced, y_train, \n",
    "                                          cv=5, scoring='neg_mean_absolute_error')\n",
    "                cv_mae = -cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                balanced_results[name] = {\n",
    "                    'model': model,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2,\n",
    "                    'cv_mae': cv_mae,\n",
    "                    'cv_std': cv_std,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "                print(f\"Test MAE: ${mae:,.2f}\")\n",
    "                print(f\"Test R²: {r2:.4f}\")\n",
    "                print(f\"CV MAE: ${cv_mae:,.2f} ± ${cv_std:,.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        print(\"   Not enough features to create balanced models (only 1 feature remaining)\")\n",
    "        balanced_results = {}\n",
    "        X_train_balanced = None\n",
    "        X_test_balanced = None\n",
    "        feature_names_balanced = None\n",
    "    \n",
    "    return balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced\n",
    "\n",
    "def compare_all_models(original_results, cleaned_results, balanced_results):\n",
    "    \"\"\"Compare performance across all model versions\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nCLEANED Models:\")\n",
    "    for name, result in cleaned_results.items():\n",
    "        print(f\"  {name}: MAE: ${result['mae']:,.0f}, R²: {result['r2']:.4f}\")\n",
    "    \n",
    "    if balanced_results:\n",
    "        print(\"\\nBALANCED Models (no dominant feature):\")\n",
    "        for name, result in balanced_results.items():\n",
    "            print(f\"  {name}: MAE: ${result['mae']:,.0f}, R²: {result['r2']:.4f}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_results = {}\n",
    "    if cleaned_results:\n",
    "        all_results.update(cleaned_results)\n",
    "    if balanced_results:\n",
    "        all_results.update(balanced_results)\n",
    "    \n",
    "    if all_results:\n",
    "        best_overall = min(all_results.items(), key=lambda x: x[1]['mae'])\n",
    "        best_name, best_result = best_overall\n",
    "        \n",
    "        cv_test_ratio = best_result['cv_mae'] / best_result['mae']\n",
    "        \n",
    "        print(f\"BEST OVERALL MODEL: {best_name}\")\n",
    "        print(f\"   Test MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Test R²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV MAE: ${best_result['cv_mae']:,.2f}\")\n",
    "        print(f\"   CV/Test Ratio: {cv_test_ratio:.2f}x\")\n",
    "        \n",
    "        if cv_test_ratio > 2.0:\n",
    "            print(\"   WARNING: High CV-Test gap suggests some overfitting remains\")\n",
    "        else:\n",
    "            print(\"   GOOD: Reasonable CV-Test consistency\")\n",
    "        \n",
    "        return best_name, best_result\n",
    "    else:\n",
    "        print(\"No valid models to compare\")\n",
    "        return None, None\n",
    "\n",
    "# Alternative safe version if the main function still has issues\n",
    "def investigate_feature_0_and_improve_safe(final_model_data, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Safer version that uses the cleaned data directly\"\"\"\n",
    "    \n",
    "    print(\"INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"1. Analyzing dominant feature in cleaned data...\")\n",
    "    \n",
    "    # Use the already cleaned data from final_model_data\n",
    "    X_train_clean = final_model_data['X_train_clean']\n",
    "    feature_names = final_model_data['feature_names']\n",
    "    \n",
    "    # Calculate correlations using cleaned data\n",
    "    correlations = []\n",
    "    for i in range(X_train_clean.shape[1]):\n",
    "        corr = np.corrcoef(X_train_clean[:, i], y_train)[0, 1]\n",
    "        correlations.append((i, abs(corr), feature_names[i]))\n",
    "    \n",
    "    # Sort by correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 5 features by correlation with target:\")\n",
    "    for i, (feature_idx, corr, feature_name) in enumerate(correlations[:5]):\n",
    "        print(f\"   {feature_name}: correlation = {corr:.4f}\")\n",
    "    \n",
    "    dominant_feature_idx, dominant_corr, dominant_name = correlations[0]\n",
    "    print(f\"   Dominant feature is: {dominant_name} (index {dominant_feature_idx})\")\n",
    "    \n",
    "    # 2. Feature importance distribution\n",
    "    print(f\"\\n2. Feature importance distribution in cleaned model:\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': final_model_data['results']['Gradient Boosting']['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features by importance:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # 3. Create balanced models\n",
    "    print(f\"\\n3. Creating balanced models without the dominant feature...\")\n",
    "    \n",
    "    if len(feature_names) > 1:\n",
    "        # Remove the dominant feature\n",
    "        X_train_balanced = X_train_clean[:, 1:]  # Remove first feature\n",
    "        X_test_balanced = final_model_data['X_test_clean'][:, 1:]   # Remove first feature\n",
    "        feature_names_balanced = feature_names[1:]\n",
    "        \n",
    "        print(f\"   Training with {X_train_balanced.shape[1]} features (excluding {dominant_name})\")\n",
    "        \n",
    "        # Train balanced models\n",
    "        balanced_models = {\n",
    "            'Gradient Boosting (Balanced)': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=15,\n",
    "                subsample=0.7,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'XGBoost (Balanced)': XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        balanced_results = {}\n",
    "        \n",
    "        for name, model in balanced_models.items():\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train_balanced, y_train)\n",
    "                y_pred = model.predict(X_test_balanced)\n",
    "                y_pred = np.maximum(y_pred, 0)  # No negative prices\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_balanced, y_train, \n",
    "                                          cv=5, scoring='neg_mean_absolute_error')\n",
    "                cv_mae = -cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                balanced_results[name] = {\n",
    "                    'model': model,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2,\n",
    "                    'cv_mae': cv_mae,\n",
    "                    'cv_std': cv_std,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "                print(f\"Test MAE: ${mae:,.2f}\")\n",
    "                print(f\"Test R²: {r2:.4f}\")\n",
    "                print(f\"CV MAE: ${cv_mae:,.2f} ± ${cv_std:,.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        print(\"   Not enough features to create balanced models (only 1 feature remaining)\")\n",
    "        balanced_results = {}\n",
    "        X_train_balanced = None\n",
    "        X_test_balanced = None\n",
    "        feature_names_balanced = None\n",
    "    \n",
    "    return balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced\n",
    "\n",
    "# Run the investigation and improvement\n",
    "print(\"Investigating dominant feature and creating balanced models...\")\n",
    "\n",
    "try:\n",
    "    balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced = investigate_feature_0_and_improve(\n",
    "        final_model_data, X_train_imputed, y_train, X_test_imputed, y_test\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error in main analysis: {e}\")\n",
    "    print(\"Using safe alternative...\")\n",
    "    balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced = investigate_feature_0_and_improve_safe(\n",
    "        final_model_data, X_train_imputed, y_train, X_test_imputed, y_test\n",
    "    )\n",
    "\n",
    "# Compare all models\n",
    "best_name, best_result = compare_all_models(\n",
    "    {'Gradient Boosting': {'mae': 62707, 'r2': 0.9997}},  # Original overfitted\n",
    "    final_model_data['results'],  # Cleaned models\n",
    "    balanced_results  # Balanced models\n",
    ")\n",
    "\n",
    "# Final model selection and deployment\n",
    "if best_result:\n",
    "    print(f\"\\nDEPLOYMENT RECOMMENDATION:\")\n",
    "    \n",
    "    if balanced_results and best_name in balanced_results:\n",
    "        print(\"Use the BALANCED model - it's more robust and doesn't rely on a single dominant feature\")\n",
    "        final_deployment_model = balanced_results[best_name]['model']\n",
    "        deployment_features = feature_names_balanced\n",
    "        model_type = \"balanced\"\n",
    "    else:\n",
    "        print(\"Use the CLEANED model - it provides the best performance\")\n",
    "        final_deployment_model = final_model_data['results'][best_name]['model']\n",
    "        deployment_features = final_model_data['feature_names']\n",
    "        model_type = \"cleaned\"\n",
    "    \n",
    "    print(f\"\\nFINAL MODEL SELECTED: {best_name}\")\n",
    "    print(f\"PERFORMANCE: MAE ${best_result['mae']:,.0f}, R² {best_result['r2']:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    deployment_package = {\n",
    "        'model': final_deployment_model,\n",
    "        'feature_names': deployment_features,\n",
    "        'performance': best_result,\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'model_type': model_type,\n",
    "        'data_shape': {\n",
    "            'n_features': len(deployment_features),\n",
    "            'n_train_samples': X_train_imputed.shape[0],\n",
    "            'n_test_samples': X_test_imputed.shape[0]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('rental_model_final.pkl', 'wb') as f:\n",
    "        pickle.dump(deployment_package, f)\n",
    "    \n",
    "    print(f\"MODEL SAVED: rental_model_final.pkl\")\n",
    "    print(f\"READY FOR DEPLOYMENT!\")\n",
    "    \n",
    "    # Show final feature importance for the deployed model\n",
    "    if hasattr(final_deployment_model, 'feature_importances_'):\n",
    "        print(f\"\\nFINAL FEATURE IMPORTANCE (Top 10):\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': deployment_features,\n",
    "            'importance': final_deployment_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(10))\n",
    "else:\n",
    "    print(\"No valid model found for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd328a5-5399-47c4-9155-14398325cffa",
   "metadata": {},
   "source": [
    "## hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334e18d8-f7d6-406a-8992-6a8d872f8f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost Hyperparameter Tuning...\n",
      "This will tune the model that achieved MAE: $23.16, R²: 0.8757\n",
      "\n",
      "==================================================\n",
      "RUNNING QUICK TUNING...\n",
      "==================================================\n",
      "QUICK XGBOOST TUNING\n",
      "========================================\n",
      "Original Performance: MAE $23.39, R² 0.8762\n",
      "Running quick tuning...\n",
      "Fitting 5 folds for each of 2187 candidates, totalling 10935 fits\n",
      "\n",
      " Best Parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 5, 'subsample': 0.8}\n",
      " Tuned Performance:\n",
      "   Test MAE: $22.36\n",
      "   Test R²: 0.8886\n",
      " Improvement: +4.4%\n",
      " CV MAE: $23.81 ± $0.91\n",
      "\n",
      " Saving improved model...\n",
      " Tuned model saved: rental_model_final.pkl\n",
      "\n",
      " Top 10 Feature Importances (Tuned Model):\n",
      "       feature  importance\n",
      "26  feature_26    0.323347\n",
      "2    feature_2    0.243839\n",
      "23  feature_23    0.049280\n",
      "51  feature_51    0.032860\n",
      "12  feature_12    0.031132\n",
      "6    feature_6    0.022145\n",
      "1    feature_1    0.022063\n",
      "45  feature_45    0.021221\n",
      "47  feature_47    0.018164\n",
      "21  feature_21    0.014273\n",
      "\n",
      " Hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "RUNNING COMPREHENSIVE TUNING...\n",
      "==================================================\n",
      "XGBOOST HYPERPARAMETER TUNING\n",
      "==================================================\n",
      "Data shapes - X_train: (3286, 59), y_train: (3286,)\n",
      "Original XGBoost Performance:\n",
      "  Test MAE: $23.39\n",
      "  Test R²: 0.8762\n",
      "Base XGBoost (default):\n",
      "  Test MAE: $24.43\n",
      "  Test R²: 0.8577\n",
      "\n",
      "1. Quick Grid Search...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best parameters (Quick Search): {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Quick Search Performance:\n",
      "  Test MAE: $22.26\n",
      "  Test R²: 0.8923\n",
      "\n",
      "2. Comprehensive Randomized Search...\n",
      "Running randomized search...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best parameters (Randomized Search): {'subsample': 0.7, 'reg_lambda': 0.5, 'reg_alpha': 0, 'n_estimators': 200, 'min_child_weight': 7, 'max_depth': 7, 'learning_rate': 0.02, 'gamma': 0.1, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.6}\n",
      "Randomized Search Performance:\n",
      "  Test MAE: $22.06\n",
      "  Test R²: 0.8944\n",
      "\n",
      "==================================================\n",
      "HYPERPARAMETER TUNING RESULTS SUMMARY\n",
      "==================================================\n",
      "Original XGBoost     | MAE: $  23.39 | R²: 0.8762 | Baseline\n",
      "Base XGBoost         | MAE: $  24.43 | R²: 0.8577 | Improvement: -4.5%\n",
      "Quick Search         | MAE: $  22.26 | R²: 0.8923 | Improvement: +4.8%\n",
      "Randomized Search    | MAE: $  22.06 | R²: 0.8944 | Improvement: +5.7%\n",
      "\n",
      "Cross-Validation Performance:\n",
      "Base XGBoost         | CV MAE: $  26.70 ± $1.46\n",
      "Quick Search         | CV MAE: $  23.88 ± $0.70\n",
      "Randomized Search    | CV MAE: $  24.03 ± $0.79\n",
      "\n",
      " BEST TUNED MODEL: Randomized Search\n",
      "   Test MAE: $22.06\n",
      "   Test R²: 0.8944\n",
      "\n",
      "Top 10 Feature Importances (Tuned Model):\n",
      "       feature  importance\n",
      "2    feature_2    0.158318\n",
      "24  feature_24    0.144556\n",
      "23  feature_23    0.107913\n",
      "26  feature_26    0.046976\n",
      "1    feature_1    0.043932\n",
      "45  feature_45    0.039326\n",
      "44  feature_44    0.038647\n",
      "12  feature_12    0.032555\n",
      "6    feature_6    0.026596\n",
      "51  feature_51    0.024067\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def hyperparameter_tuning_xgboost(final_model_data, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Comprehensive hyperparameter tuning for XGBoost using your existing data\"\"\"\n",
    "    \n",
    "    print(\"XGBOOST HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Use the cleaned data from final_model_data\n",
    "    X_train_clean = final_model_data['X_train_clean']\n",
    "    X_test_clean = final_model_data['X_test_clean']\n",
    "    feature_names = final_model_data['feature_names']\n",
    "    \n",
    "    print(f\"Data shapes - X_train: {X_train_clean.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Define scoring metric\n",
    "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "    \n",
    "    # Get original model performance for comparison\n",
    "    original_mae = final_model_data['results']['XGBoost']['mae']\n",
    "    original_r2 = final_model_data['results']['XGBoost']['r2']\n",
    "    \n",
    "    print(f\"Original XGBoost Performance:\")\n",
    "    print(f\"  Test MAE: ${original_mae:,.2f}\")\n",
    "    print(f\"  Test R²: {original_r2:.4f}\")\n",
    "    \n",
    "    # Base model for comparison (with same parameters as original)\n",
    "    base_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    base_model.fit(X_train_clean, y_train)\n",
    "    base_pred = base_model.predict(X_test_clean)\n",
    "    base_mae = mean_absolute_error(y_test, base_pred)\n",
    "    base_r2 = r2_score(y_test, base_pred)\n",
    "    \n",
    "    print(f\"Base XGBoost (default):\")\n",
    "    print(f\"  Test MAE: ${base_mae:,.2f}\")\n",
    "    print(f\"  Test R²: {base_r2:.4f}\")\n",
    "    \n",
    "    # Quick Grid Search for key parameters\n",
    "    print(\"\\n1. Quick Grid Search...\")\n",
    "    param_grid_quick = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    xgb_quick = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    grid_search_quick = GridSearchCV(\n",
    "        xgb_quick, param_grid_quick, \n",
    "        scoring=mae_scorer,\n",
    "        cv=5, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search_quick.fit(X_train_clean, y_train)\n",
    "    \n",
    "    print(f\"Best parameters (Quick Search): {grid_search_quick.best_params_}\")\n",
    "    quick_model = grid_search_quick.best_estimator_\n",
    "    quick_pred = quick_model.predict(X_test_clean)\n",
    "    quick_mae = mean_absolute_error(y_test, quick_pred)\n",
    "    quick_r2 = r2_score(y_test, quick_pred)\n",
    "    \n",
    "    print(f\"Quick Search Performance:\")\n",
    "    print(f\"  Test MAE: ${quick_mae:,.2f}\")\n",
    "    print(f\"  Test R²: {quick_r2:.4f}\")\n",
    "    \n",
    "    # Comprehensive Randomized Search\n",
    "    print(\"\\n2. Comprehensive Randomized Search...\")\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5, 1, 2, 5],\n",
    "        'reg_lambda': [0, 0.1, 0.5, 1, 2, 5],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "        'min_child_weight': [1, 3, 5, 7]\n",
    "    }\n",
    "    \n",
    "    xgb_rand = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_rand, param_dist, \n",
    "        n_iter=30,  # Reduced for faster execution\n",
    "        scoring=mae_scorer,\n",
    "        cv=5, n_jobs=-1, random_state=42, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Running randomized search...\")\n",
    "    random_search.fit(X_train_clean, y_train)\n",
    "    \n",
    "    print(f\"Best parameters (Randomized Search): {random_search.best_params_}\")\n",
    "    rand_model = random_search.best_estimator_\n",
    "    rand_pred = rand_model.predict(X_test_clean)\n",
    "    rand_mae = mean_absolute_error(y_test, rand_pred)\n",
    "    rand_r2 = r2_score(y_test, rand_pred)\n",
    "    \n",
    "    print(f\"Randomized Search Performance:\")\n",
    "    print(f\"  Test MAE: ${rand_mae:,.2f}\")\n",
    "    print(f\"  Test R²: {rand_r2:.4f}\")\n",
    "    \n",
    "    # Compare all models\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results_comparison = {\n",
    "        'Original XGBoost': {'mae': original_mae, 'r2': original_r2},\n",
    "        'Base XGBoost': {'mae': base_mae, 'r2': base_r2, 'model': base_model},\n",
    "        'Quick Search': {'mae': quick_mae, 'r2': quick_r2, 'model': quick_model},\n",
    "        'Randomized Search': {'mae': rand_mae, 'r2': rand_r2, 'model': rand_model}\n",
    "    }\n",
    "    \n",
    "    for name, results in results_comparison.items():\n",
    "        if name != 'Original XGBoost':\n",
    "            improvement = ((original_mae - results['mae']) / original_mae) * 100\n",
    "            print(f\"{name:20} | MAE: ${results['mae']:7.2f} | R²: {results['r2']:6.4f} | Improvement: {improvement:+.1f}%\")\n",
    "        else:\n",
    "            print(f\"{name:20} | MAE: ${results['mae']:7.2f} | R²: {results['r2']:6.4f} | Baseline\")\n",
    "    \n",
    "    # Cross-validation comparison\n",
    "    print(\"\\nCross-Validation Performance:\")\n",
    "    for name, results in results_comparison.items():\n",
    "        if name != 'Original XGBoost' and 'model' in results:\n",
    "            cv_scores = cross_val_score(results['model'], X_train_clean, y_train, \n",
    "                                      cv=5, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            print(f\"{name:20} | CV MAE: ${cv_mae:7.2f} ± ${cv_std:.2f}\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = min([(k, v) for k, v in results_comparison.items() if k != 'Original XGBoost'], \n",
    "                         key=lambda x: x[1]['mae'])[0]\n",
    "    best_model = results_comparison[best_model_name]['model']\n",
    "    best_mae = results_comparison[best_model_name]['mae']\n",
    "    best_r2 = results_comparison[best_model_name]['r2']\n",
    "    \n",
    "    print(f\"\\n BEST TUNED MODEL: {best_model_name}\")\n",
    "    print(f\"   Test MAE: ${best_mae:,.2f}\")\n",
    "    print(f\"   Test R²: {best_r2:.4f}\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Feature Importances (Tuned Model):\")\n",
    "        print(importance_df.head(10))\n",
    "    \n",
    "    return best_model, results_comparison\n",
    "\n",
    "# Quick version for faster results\n",
    "def quick_xgboost_tuning(final_model_data, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Quick XGBoost tuning for immediate improvements\"\"\"\n",
    "    \n",
    "    print(\"QUICK XGBOOST TUNING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Use the cleaned data\n",
    "    X_train_clean = final_model_data['X_train_clean']\n",
    "    X_test_clean = final_model_data['X_test_clean']\n",
    "    feature_names = final_model_data['feature_names']\n",
    "    \n",
    "    # Original performance\n",
    "    original_mae = final_model_data['results']['XGBoost']['mae']\n",
    "    original_r2 = final_model_data['results']['XGBoost']['r2']\n",
    "    \n",
    "    print(f\"Original Performance: MAE ${original_mae:,.2f}, R² {original_r2:.4f}\")\n",
    "    \n",
    "    # Focused parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [4, 5, 6],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.5, 1],\n",
    "        'reg_lambda': [1, 2, 5]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, param_grid,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5, n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Running quick tuning...\")\n",
    "    grid_search.fit(X_train_clean, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_clean)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    improvement = ((original_mae - mae) / original_mae) * 100\n",
    "    \n",
    "    print(f\"\\n Best Parameters: {best_params}\")\n",
    "    print(f\" Tuned Performance:\")\n",
    "    print(f\"   Test MAE: ${mae:,.2f}\")\n",
    "    print(f\"   Test R²: {r2:.4f}\")\n",
    "    print(f\" Improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_model, X_train_clean, y_train, \n",
    "                              cv=5, scoring='neg_mean_absolute_error')\n",
    "    cv_mae = -cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    print(f\" CV MAE: ${cv_mae:,.2f} ± ${cv_std:.2f}\")\n",
    "    \n",
    "    return best_model, {'mae': mae, 'r2': r2, 'params': best_params, 'improvement': improvement}\n",
    "\n",
    "# Run the hyperparameter tuning\n",
    "print(\"Starting XGBoost Hyperparameter Tuning...\")\n",
    "print(\"This will tune the model that achieved MAE: $23.16, R²: 0.8757\")\n",
    "\n",
    "# Choose which version to run:\n",
    "# Option 1: Quick tuning (faster)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING QUICK TUNING...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "quick_tuned_model, quick_results = quick_xgboost_tuning(\n",
    "    final_model_data, X_train_imputed, y_train, X_test_imputed, y_test\n",
    ")\n",
    "\n",
    "# Save the tuned model if it's better\n",
    "if quick_results['improvement'] > 0:\n",
    "    print(f\"\\n Saving improved model...\")\n",
    "    \n",
    "    deployment_package_tuned = {\n",
    "        'model': quick_tuned_model,\n",
    "        'feature_names': final_model_data['feature_names'],\n",
    "        'performance': {\n",
    "            'mae': quick_results['mae'],\n",
    "            'r2': quick_results['r2'],\n",
    "            'improvement': quick_results['improvement'],\n",
    "            'params': quick_results['params']\n",
    "        },\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'model_type': 'xgboost_tuned',\n",
    "        'original_performance': {\n",
    "            'mae': final_model_data['results']['XGBoost']['mae'],\n",
    "            'r2': final_model_data['results']['XGBoost']['r2']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('rental_model_final.pkl', 'wb') as f:\n",
    "        pickle.dump(deployment_package_tuned, f)\n",
    "    \n",
    "    print(\" Tuned model saved: rental_model_final.pkl\")\n",
    "    \n",
    "    # Compare feature importance\n",
    "    if hasattr(quick_tuned_model, 'feature_importances_'):\n",
    "        print(f\"\\n Top 10 Feature Importances (Tuned Model):\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': final_model_data['feature_names'],\n",
    "            'importance': quick_tuned_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(10))\n",
    "else:\n",
    "    print(\"\\nℹ  Original model performs better. Keeping original model.\")\n",
    "\n",
    "print(\"\\n Hyperparameter tuning completed!\")\n",
    "\n",
    "# Optional: Run comprehensive tuning (uncomment if you want more thorough tuning)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING COMPREHENSIVE TUNING...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_tuned_model, tuning_results = hyperparameter_tuning_xgboost(\n",
    "    final_model_data, X_train_imputed, y_train, X_test_imputed, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af3e4-bab2-4b8c-bf66-3ad21321a534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfaf91-cb4b-4710-9f09-5108fe4dfbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b514c1b-9d7e-4f51-8cf9-a7a60c1fdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import joblib\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5410682-02bc-4996-9c63-78468e6bc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Month Year</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Planning Area</th>\n",
       "      <th>Type of Sale</th>\n",
       "      <th>Price</th>\n",
       "      <th>$psm</th>\n",
       "      <th>Area</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Contract Date</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Type Of Area</th>\n",
       "      <th>Floor Level</th>\n",
       "      <th>Region</th>\n",
       "      <th>Postal Sector</th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41711.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>WAVE9</td>\n",
       "      <td>WOODLANDS INDUSTRIAL PARK E9</td>\n",
       "      <td>Woodlands</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$1,108,888</td>\n",
       "      <td>$3,772</td>\n",
       "      <td>294.0</td>\n",
       "      <td>30 yrs from 05/06/2014</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>North Region</td>\n",
       "      <td>75.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41712.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>CATTEL BUILDING</td>\n",
       "      <td>ALEXANDRA TERRACE</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$3,800,000</td>\n",
       "      <td>$3,007</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Land</td>\n",
       "      <td>-</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41713.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>LAM SOON INDUSTRIAL BUILDING</td>\n",
       "      <td>HILLVIEW AVENUE</td>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$1,510,900</td>\n",
       "      <td>$6,243</td>\n",
       "      <td>242.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>West Region</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41714.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>SYNERGY @ KB</td>\n",
       "      <td>KAKI BUKIT ROAD 4</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$458,000</td>\n",
       "      <td>$3,368</td>\n",
       "      <td>136.0</td>\n",
       "      <td>30 yrs from 20/01/2012</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>East Region</td>\n",
       "      <td>41.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41715.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>WCEGA TOWER</td>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$800,000</td>\n",
       "      <td>$4,372</td>\n",
       "      <td>183.0</td>\n",
       "      <td>60 yrs from 13/03/1997</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>West Region</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID Month Year                  Project Name  \\\n",
       "0  41711.0    2021-01                         WAVE9   \n",
       "1  41712.0    2021-01               CATTEL BUILDING   \n",
       "2  41713.0    2021-01  LAM SOON INDUSTRIAL BUILDING   \n",
       "3  41714.0    2021-01                  SYNERGY @ KB   \n",
       "4  41715.0    2021-01                   WCEGA TOWER   \n",
       "\n",
       "                    Street Name Planning Area Type of Sale       Price  \\\n",
       "0  WOODLANDS INDUSTRIAL PARK E9     Woodlands       Resale  $1,108,888   \n",
       "1             ALEXANDRA TERRACE    Queenstown       Resale  $3,800,000   \n",
       "2               HILLVIEW AVENUE   Bukit Batok       Resale  $1,510,900   \n",
       "3             KAKI BUKIT ROAD 4         Bedok       Resale    $458,000   \n",
       "4          BUKIT BATOK CRESCENT   Bukit Batok       Resale    $800,000   \n",
       "\n",
       "     $psm    Area                  Tenure Contract Date  \\\n",
       "0  $3,772   294.0  30 yrs from 05/06/2014    2021-01-04   \n",
       "1  $3,007  1264.0                Freehold    2021-01-04   \n",
       "2  $6,243   242.0                Freehold    2021-01-04   \n",
       "3  $3,368   136.0  30 yrs from 20/01/2012    2021-01-04   \n",
       "4  $4,372   183.0  60 yrs from 13/03/1997    2021-01-04   \n",
       "\n",
       "           Property Type Type Of Area      Floor Level          Region  \\\n",
       "0  Multiple-User Factory       Strata      First Floor    North Region   \n",
       "1  Multiple-User Factory         Land                -  Central Region   \n",
       "2  Multiple-User Factory       Strata  Non-First Floor     West Region   \n",
       "3  Multiple-User Factory       Strata  Non-First Floor     East Region   \n",
       "4  Multiple-User Factory       Strata  Non-First Floor     West Region   \n",
       "\n",
       "   Postal Sector  Postal District  Unnamed: 17  Unnamed: 18  \n",
       "0           75.0             27.0          NaN          NaN  \n",
       "1           11.0              5.0          NaN          NaN  \n",
       "2           66.0             23.0          NaN          NaN  \n",
       "3           41.0             14.0          NaN          NaN  \n",
       "4           65.0             23.0          NaN          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_df=pd.read_csv(\"industrial_data.csv\")\n",
    "industrial_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daefe2f-4a58-4b9d-bea3-c2b69d3ea5e2",
   "metadata": {},
   "source": [
    "## cleaning dataset & removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da712e4-b1d3-4611-84ce-d6419589f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\AppData\\Local\\Temp\\ipykernel_31776\\104824214.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  industrial_df = industrial_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "industrial_df = industrial_df.drop(['ID', 'Unnamed: 17','Unnamed: 18'], axis=1)\n",
    "industrial_df = industrial_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a2c685-d64a-4668-9d88-f74630a97219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing empty rows: (8221, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\AppData\\Local\\Temp\\ipykernel_31776\\3471636226.py:105: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(date_str)\n"
     ]
    }
   ],
   "source": [
    "def clean_industrial_data(df):\n",
    "    \"\"\"Clean and prepare industrial property data with proper error handling\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove completely empty rows\n",
    "    df_clean = df_clean.dropna(how='all')\n",
    "    \n",
    "    print(f\"Data after removing empty rows: {df_clean.shape}\")\n",
    "    \n",
    "    # Clean price columns with better error handling\n",
    "    def clean_price_column(price_series):\n",
    "        \"\"\"Clean price column with comprehensive handling\"\"\"\n",
    "        cleaned_prices = []\n",
    "        for price in price_series:\n",
    "            if pd.isna(price) or price in ['', ' ', 'NaN', 'nan']:\n",
    "                cleaned_prices.append(np.nan)\n",
    "            else:\n",
    "                # Convert to string and clean\n",
    "                price_str = str(price).strip()\n",
    "                # Remove $ and commas\n",
    "                price_str = price_str.replace('$', '').replace(',', '')\n",
    "                # Remove any extra spaces\n",
    "                price_str = price_str.replace(' ', '')\n",
    "                try:\n",
    "                    cleaned_prices.append(float(price_str))\n",
    "                except (ValueError, TypeError):\n",
    "                    cleaned_prices.append(np.nan)\n",
    "        return cleaned_prices\n",
    "    \n",
    "    df_clean['Price'] = clean_price_column(df_clean['Price'])\n",
    "    df_clean['$psm'] = clean_price_column(df_clean['$psm'])\n",
    "    \n",
    "    # Remove rows with invalid prices\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Price', '$psm'])\n",
    "    \n",
    "    # Rename columns to match commercial model format\n",
    "    df_clean = df_clean.rename(columns={\n",
    "        '$psm': 'Unit Price ($ PSM)',\n",
    "        'Area': 'Area (SQM)',\n",
    "        'Price': 'Transacted Price ($)',\n",
    "        'Property Type': 'Property Type',\n",
    "        'Type Of Area': 'Type of Area',\n",
    "        'Floor Level': 'Floor Level',\n",
    "        'Postal District': 'Postal District',\n",
    "        'Planning Area': 'Planning Area',\n",
    "        'Region': 'Region'\n",
    "    })\n",
    "    \n",
    "    # Handle Floor Level\n",
    "    def parse_industrial_floor_level(floor_str):\n",
    "        if pd.isna(floor_str) or floor_str in ['-', ' ', '']:\n",
    "            return 0, 0, 0, 0, 1  # ground/first floor\n",
    "        elif str(floor_str).strip() == 'First Floor':\n",
    "            return 0, 0, 0, 0, 1  # first floor\n",
    "        elif str(floor_str).strip() == 'Non-First Floor':\n",
    "            return 1, 1, 1, 0, 0  # non-first floor\n",
    "        else:\n",
    "            # Try to extract numbers if available\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', str(floor_str))\n",
    "            if numbers:\n",
    "                floor_num = int(numbers[0])\n",
    "                return floor_num, floor_num, floor_num, 0, 0\n",
    "            else:\n",
    "                return 0, 0, 0, 0, 1  # default to ground\n",
    "    \n",
    "    floor_data = df_clean['Floor Level'].apply(parse_industrial_floor_level)\n",
    "    df_clean['Floor_Low'] = [x[0] for x in floor_data]\n",
    "    df_clean['Floor_High'] = [x[1] for x in floor_data]\n",
    "    df_clean['Floor_Midpoint'] = [x[2] for x in floor_data]\n",
    "    df_clean['Is_Basement'] = [x[3] for x in floor_data]\n",
    "    df_clean['Is_Ground'] = [x[4] for x in floor_data]\n",
    "    \n",
    "    # Create floor categories for industrial\n",
    "    def create_industrial_floor_category(midpoint):\n",
    "        if midpoint == 0: \n",
    "            return 'ground_floor'\n",
    "        elif midpoint == 1:\n",
    "            return 'upper_floor'\n",
    "        else:\n",
    "            return f'level_{int(midpoint)}'\n",
    "    \n",
    "    df_clean['Floor_Category'] = df_clean['Floor_Midpoint'].apply(create_industrial_floor_category)\n",
    "    \n",
    "    # Handle Tenure\n",
    "    def categorize_tenure(tenure_str):\n",
    "        if pd.isna(tenure_str) or tenure_str in ['', ' ']:\n",
    "            return 'Unknown'\n",
    "        tenure_str = str(tenure_str).lower()\n",
    "        if 'freehold' in tenure_str:\n",
    "            return 'Freehold'\n",
    "        elif any(x in tenure_str for x in ['30 yrs', '60 yrs', '99 yrs']):\n",
    "            return 'Leasehold'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    df_clean['Tenure_Type'] = df_clean['Tenure'].apply(categorize_tenure)\n",
    "    \n",
    "    # Convert date with error handling\n",
    "    def parse_contract_date(date_str):\n",
    "        if pd.isna(date_str) or date_str in ['', ' ']:\n",
    "            return pd.NaT\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    df_clean['Contract Date'] = df_clean['Contract Date'].apply(parse_contract_date)\n",
    "    df_clean['Contract Date'].unique()\n",
    "    \n",
    "    df_clean['Contract_date_missing'] = df_clean['Contract Date'].isna().astype(int)\n",
    "    \n",
    "    df_clean['Contract_year'] = df_clean['Contract Date'].dt.year\n",
    "    df_clean['Contract_month'] = df_clean['Contract Date'].dt.month\n",
    "    df_clean['Contract_quarter'] = df_clean['Contract Date'].dt.quarter\n",
    "    df_clean['Contract_dayofweek'] = df_clean['Contract Date'].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "    \n",
    "    # continuous trend feature\n",
    "    #df['days_since_first_Contract'] = (df['Contract Date'] - df['Contract Date'].min()).dt.days\n",
    "    \n",
    "    df_clean.drop(columns=['Contract Date'], inplace=True)\n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean industrial data with proper error handling\n",
    "industrial_df = clean_industrial_data(industrial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2828d0-92d3-4806-82f4-293162e37390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts per row:\n",
      " 0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "8216    0\n",
      "8217    0\n",
      "8218    0\n",
      "8219    0\n",
      "8220    0\n",
      "Length: 8220, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = industrial_df.isnull().sum(axis=1)\n",
    "print(\"Missing counts per row:\\n\", missing_counts)\n",
    "\n",
    "missing_counts = industrial_df.isna().sum(axis=1)  # Count missing values per row\n",
    "industrial_df.drop(industrial_df[missing_counts >= 4].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757cc2e-e4ec-4033-b38b-9dbad456f60e",
   "metadata": {},
   "source": [
    "### fill in project name null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c9a7f4-0b63-45a9-a663-ee9bf4a995d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING PROJECT NAME FILLING PIPELINE\n",
      "==================================================\n",
      "ðŸ— COMPREHENSIVE PROJECT NAME FILLING\n",
      "==================================================\n",
      "\n",
      "1.  Pattern matching...\n",
      "Found 187 known street-project mappings\n",
      "Filled 174 missing project names using pattern matching\n",
      "\n",
      "2.  Pattern extraction...\n",
      "Filled 127 missing project names using pattern extraction\n",
      "\n",
      "3.  Geographic clustering...\n",
      "Filled 0 missing project names using geographic clustering\n",
      "\n",
      "4.  Fuzzy matching...\n",
      "Known properties: 8077\n",
      "Properties with missing project names: 142\n",
      "Filled 46 missing project names using fuzzy matching\n",
      "\n",
      " FILLING SUMMARY:\n",
      "Initial missing: 443\n",
      "Pattern matched: 174\n",
      "Pattern extracted: 127\n",
      "Cluster matched: 0\n",
      "Fuzzy matched: 46\n",
      "Final missing: 96\n",
      "Success rate: 78.3%\n",
      "Applied 1654 custom project name mappings\n",
      " VERIFYING PROJECT NAME FILLING\n",
      "========================================\n",
      "Missing project names:\n",
      "  Before filling: 443\n",
      "  After filling: 95\n",
      "  Filled: 348\n",
      "  Success rate: 78.6%\n",
      "\n",
      " SAMPLE FILLED PROJECT NAMES (first 10):\n",
      "  Street: kian teck way -> Project: JURONG INDUSTRIAL ESTATE\n",
      "  Street: tuas avenue 8 -> Project: Tuas\n",
      "  Street: woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "  Street:  playfair road -> Project: KAPO FACTORY BUILDING\n",
      "  Street:  woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  macpherson road -> Project: BETIME BUILDING\n",
      "  Street: woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "  Street: woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "\n",
      " REMAINING MISSING PROJECT NAMES (sample):\n",
      "  Street:  neythal road\n",
      "  Street:  gul lane\n",
      "  Street: woodlands terrace\n",
      "  Street: chang charn road\n",
      "  Street: neythal road\n",
      "\n",
      " FINAL DATA QUALITY:\n",
      "Total properties: 8219\n",
      "Properties with project names: 8124\n",
      "Properties without project names: 95\n",
      "\n",
      " SAMPLE RESULTS:\n",
      "  woodlands industrial park e9 -> WOODLANDS INDUSTRIAL PARK (woodlands)\n",
      "  alexandra terrace -> CATTEL BUILDING (queenstown)\n",
      "  hillview avenue -> LAM SOON INDUSTRIAL BUILDING (bukit batok)\n",
      "  kaki bukit road 4 -> KAKI BUKIT INDUSTRIAL ESTATE (bedok)\n",
      "  bukit batok crescent -> BUKIT BATOK INDUSTRIAL ESTATE (bukit batok)\n",
      "  woodlands link -> woodlands east industrial estate (sembawang)\n",
      "  jalan pemimpin -> mapex (bishan)\n",
      "  jalan pemimpin -> m38 (bishan)\n",
      "  boon lay way -> tradehub 21 (clementi)\n",
      "  boon lay way -> 8 @ tradehub 21 (clementi)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# First, define all the individual functions\n",
    "\n",
    "def fill_project_names_pattern_based(df):\n",
    "    \"\"\"Fill missing project names based on street name patterns\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Create a mapping from known project names to street names\n",
    "    known_mappings = {}\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.notna(row['Project Name']) and pd.notna(row['Street Name']):\n",
    "            project = str(row['Project Name']).strip().upper()\n",
    "            street = str(row['Street Name']).strip().upper()\n",
    "            if project and street:\n",
    "                known_mappings[street] = project\n",
    "    \n",
    "    print(f\"Found {len(known_mappings)} known street-project mappings\")\n",
    "    \n",
    "    # Fill missing project names using the mapping\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            street_name = str(row['Street Name']).strip().upper() if pd.notna(row['Street Name']) else ''\n",
    "            if street_name in known_mappings:\n",
    "                df_filled.at[idx, 'Project Name'] = known_mappings[street_name]\n",
    "                filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using pattern matching\")\n",
    "    return df_filled, known_mappings\n",
    "\n",
    "def extract_project_from_street_patterns(df):\n",
    "    \"\"\"Extract project names from street names using common patterns\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Common industrial estate patterns in Singapore\n",
    "    industrial_patterns = [\n",
    "        (r'(.*)\\s(INDUSTRIAL|INDUSTRIAL PARK|BIZHUB|TECHNOPARK|TECH PARK)', 1),  # \"WOODLANDS INDUSTRIAL PARK\" -> \"WOODLANDS\"\n",
    "        (r'(.*)\\s(ROAD|STREET|AVENUE|DRIVE|LANE|TERRACE)\\s*(\\d+[A-Z]?)', 1),    # \"KAKI BUKIT ROAD 4\" -> \"KAKI BUKIT\"\n",
    "        (r'(.*)\\s(CRESCENT|CIRCLE|PLACE|GARDENS|VIEW)', 1),                     # \"BUKIT BATOK CRESCENT\" -> \"BUKIT BATOK\"\n",
    "        (r'(.*)\\s(COMPLEX|CENTRE|HUB|BUILDING|TOWER)', 1),                      # \"CATTEL BUILDING\" -> \"CATTEL\"\n",
    "    ]\n",
    "    \n",
    "    filled_count = 0\n",
    "    \n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            street_name = str(row['Street Name']) if pd.notna(row['Street Name']) else ''\n",
    "            \n",
    "            if street_name:\n",
    "                # Try each pattern\n",
    "                for pattern, group_num in industrial_patterns:\n",
    "                    match = re.search(pattern, street_name, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        extracted_name = match.group(group_num).strip()\n",
    "                        # Clean up the extracted name\n",
    "                        extracted_name = extracted_name.title()\n",
    "                        df_filled.at[idx, 'Project Name'] = extracted_name\n",
    "                        filled_count += 1\n",
    "                        break\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using pattern extraction\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_geographic_clusters(df):\n",
    "    \"\"\"Fill project names based on geographic clustering\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Group by Planning Area and Street Name patterns\n",
    "    geographic_clusters = {}\n",
    "    \n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.notna(row['Project Name']) and str(row['Project Name']).strip() not in ['', 'NaN', 'nan']:\n",
    "            planning_area = str(row['Planning Area']).upper() if pd.notna(row['Planning Area']) else 'UNKNOWN'\n",
    "            street_name = str(row['Street Name']).upper() if pd.notna(row['Street Name']) else 'UNKNOWN'\n",
    "            \n",
    "            cluster_key = f\"{planning_area}_{street_name}\"\n",
    "            if cluster_key not in geographic_clusters:\n",
    "                geographic_clusters[cluster_key] = []\n",
    "            \n",
    "            geographic_clusters[cluster_key].append(row['Project Name'])\n",
    "    \n",
    "    # For each cluster, find the most common project name\n",
    "    cluster_project_names = {}\n",
    "    for cluster_key, projects in geographic_clusters.items():\n",
    "        if projects:\n",
    "            # Get the most frequent project name in this cluster\n",
    "            most_common = Counter(projects).most_common(1)\n",
    "            if most_common:\n",
    "                cluster_project_names[cluster_key] = most_common[0][0]\n",
    "    \n",
    "    # Fill missing project names using clusters\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            planning_area = str(row['Planning Area']).upper() if pd.notna(row['Planning Area']) else 'UNKNOWN'\n",
    "            street_name = str(row['Street Name']).upper() if pd.notna(row['Street Name']) else 'UNKNOWN'\n",
    "            \n",
    "            cluster_key = f\"{planning_area}_{street_name}\"\n",
    "            if cluster_key in cluster_project_names:\n",
    "                df_filled.at[idx, 'Project Name'] = cluster_project_names[cluster_key]\n",
    "                filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using geographic clustering\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_fuzzy(df, similarity_threshold=80):\n",
    "    \"\"\"Fill missing project names using fuzzy matching on similar properties\"\"\"\n",
    "    try:\n",
    "        from fuzzywuzzy import fuzz\n",
    "    except ImportError:\n",
    "        print(\"Fuzzywuzzy not installed, skipping fuzzy matching\")\n",
    "        return df\n",
    "        \n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Get properties with known project names\n",
    "    known_properties = df_filled[df_filled['Project Name'].notna() & \n",
    "                                (df_filled['Project Name'].str.strip() != '')].copy()\n",
    "    \n",
    "    # Get properties with missing project names\n",
    "    missing_properties = df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')].copy()\n",
    "    \n",
    "    print(f\"Known properties: {len(known_properties)}\")\n",
    "    print(f\"Properties with missing project names: {len(missing_properties)}\")\n",
    "    \n",
    "    filled_count = 0\n",
    "    \n",
    "    for idx, missing_row in missing_properties.iterrows():\n",
    "        missing_street = str(missing_row['Street Name']).upper() if pd.notna(missing_row['Street Name']) else ''\n",
    "        missing_planning_area = str(missing_row['Planning Area']).upper() if pd.notna(missing_row['Planning Area']) else ''\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for _, known_row in known_properties.iterrows():\n",
    "            known_street = str(known_row['Street Name']).upper() if pd.notna(known_row['Street Name']) else ''\n",
    "            known_planning_area = str(known_row['Planning Area']).upper() if pd.notna(known_row['Planning Area']) else ''\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            street_similarity = fuzz.ratio(missing_street, known_street)\n",
    "            area_similarity = fuzz.ratio(missing_planning_area, known_planning_area)\n",
    "            \n",
    "            # Combined score (weight street name more heavily)\n",
    "            total_score = (street_similarity * 0.7) + (area_similarity * 0.3)\n",
    "            \n",
    "            if total_score > best_score and total_score >= similarity_threshold:\n",
    "                best_score = total_score\n",
    "                best_match = known_row['Project Name']\n",
    "        \n",
    "        if best_match:\n",
    "            df_filled.at[idx, 'Project Name'] = best_match\n",
    "            filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using fuzzy matching\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_comprehensive(df):\n",
    "    \"\"\"Comprehensive approach combining multiple methods\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    print(\"ðŸ— COMPREHENSIVE PROJECT NAME FILLING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Track filling statistics\n",
    "    initial_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                   (df_filled['Project Name'].str.strip() == '')])\n",
    "    \n",
    "    filling_stats = {\n",
    "        'initial_missing': initial_missing,\n",
    "        'pattern_matched': 0,\n",
    "        'extracted': 0,\n",
    "        'cluster_matched': 0,\n",
    "        'fuzzy_matched': 0\n",
    "    }\n",
    "    \n",
    "    # Method 1: Direct pattern matching\n",
    "    print(\"\\n1.  Pattern matching...\")\n",
    "    df_filled, mappings = fill_project_names_pattern_based(df_filled)\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['pattern_matched'] = filling_stats['initial_missing'] - current_missing\n",
    "    \n",
    "    # Method 2: Pattern extraction\n",
    "    print(\"\\n2.  Pattern extraction...\")\n",
    "    df_filled = extract_project_from_street_patterns(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['extracted'] = prev_missing - current_missing\n",
    "    \n",
    "    # Method 3: Geographic clustering\n",
    "    print(\"\\n3.  Geographic clustering...\")\n",
    "    df_filled = fill_project_names_geographic_clusters(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['cluster_matched'] = prev_missing - current_missing\n",
    "    \n",
    "    # Method 4: Try fuzzy matching if available\n",
    "    print(\"\\n4.  Fuzzy matching...\")\n",
    "    df_filled_fuzzy = fill_project_names_fuzzy(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    final_missing = len(df_filled_fuzzy[df_filled_fuzzy['Project Name'].isna() | \n",
    "                                      (df_filled_fuzzy['Project Name'].str.strip() == '')])\n",
    "    filling_stats['fuzzy_matched'] = prev_missing - final_missing\n",
    "    df_filled = df_filled_fuzzy\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n FILLING SUMMARY:\")\n",
    "    print(f\"Initial missing: {filling_stats['initial_missing']}\")\n",
    "    print(f\"Pattern matched: {filling_stats['pattern_matched']}\")\n",
    "    print(f\"Pattern extracted: {filling_stats['extracted']}\")\n",
    "    print(f\"Cluster matched: {filling_stats['cluster_matched']}\")\n",
    "    print(f\"Fuzzy matched: {filling_stats['fuzzy_matched']}\")\n",
    "    print(f\"Final missing: {final_missing}\")\n",
    "    \n",
    "    if filling_stats['initial_missing'] > 0:\n",
    "        success_rate = (filling_stats['initial_missing'] - final_missing) / filling_stats['initial_missing'] * 100\n",
    "        print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "def add_custom_project_rules(df):\n",
    "    \"\"\"Add custom rules for specific known industrial estates\"\"\"\n",
    "    df_custom = df.copy()\n",
    "    \n",
    "    # Singapore industrial estate mappings\n",
    "    custom_mappings = {\n",
    "        # Woodlands area\n",
    "        'WOODLANDS INDUSTRIAL PARK E9': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS INDUSTRIAL PARK E5': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS INDUSTRIAL PARK E1': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS CLOSE': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        \n",
    "        # Kaki Bukit area\n",
    "        'KAKI BUKIT ROAD 1': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        'KAKI BUKIT ROAD 2': 'KAKI BUKIT INDUSTRIAL ESTATE', \n",
    "        'KAKI BUKIT ROAD 3': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        'KAKI BUKIT ROAD 4': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Bukit Batok area\n",
    "        'BUKIT BATOK CRESCENT': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        'BUKIT BATOK STREET 23': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        'BUKIT BATOK EAST AVENUE 6': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Tuas area\n",
    "        'TUAS SOUTH AVENUE 3': 'TUAS INDUSTRIAL ESTATE',\n",
    "        'TUAS AVENUE 11': 'TUAS INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Pioneer area\n",
    "        'PIONEER ROAD': 'PIONEER INDUSTRIAL ESTATE',\n",
    "        'PIONEER SECTOR 1': 'PIONEER INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Specific known buildings\n",
    "        'ALEXANDRA TERRACE': 'CATTEL BUILDING',\n",
    "        'HILLVIEW AVENUE': 'LAM SOON INDUSTRIAL BUILDING',\n",
    "    }\n",
    "    \n",
    "    filled_count = 0\n",
    "    for idx, row in df_custom.iterrows():\n",
    "        street_name = str(row['Street Name']).upper().strip() if pd.notna(row['Street Name']) else ''\n",
    "        \n",
    "        # Check if this street matches any custom mapping\n",
    "        for street_pattern, project_name in custom_mappings.items():\n",
    "            if street_pattern in street_name:\n",
    "                df_custom.at[idx, 'Project Name'] = project_name\n",
    "                filled_count += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Applied {filled_count} custom project name mappings\")\n",
    "    return df_custom\n",
    "\n",
    "def verify_project_name_filling(df_before, df_after):\n",
    "    \"\"\"Verify the quality of project name filling\"\"\"\n",
    "    \n",
    "    print(\" VERIFYING PROJECT NAME FILLING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Count missing project names\n",
    "    missing_before = len(df_before[df_before['Project Name'].isna() | \n",
    "                                 (df_before['Project Name'].str.strip() == '')])\n",
    "    missing_after = len(df_after[df_after['Project Name'].isna() | \n",
    "                               (df_after['Project Name'].str.strip() == '')])\n",
    "    \n",
    "    print(f\"Missing project names:\")\n",
    "    print(f\"  Before filling: {missing_before}\")\n",
    "    print(f\"  After filling: {missing_after}\")\n",
    "    print(f\"  Filled: {missing_before - missing_after}\")\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        success_rate = (missing_before - missing_after) / missing_before * 100\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Show sample of filled project names\n",
    "    filled_indices = []\n",
    "    for idx in df_after.index:\n",
    "        if idx in df_before.index:\n",
    "            before_name = df_before.loc[idx, 'Project Name']\n",
    "            after_name = df_after.loc[idx, 'Project Name']\n",
    "            if (pd.isna(before_name) or str(before_name).strip() in ['', 'NaN', 'nan']) and pd.notna(after_name):\n",
    "                filled_indices.append(idx)\n",
    "    \n",
    "    if filled_indices:\n",
    "        print(f\"\\n SAMPLE FILLED PROJECT NAMES (first 10):\")\n",
    "        for idx in filled_indices[:10]:\n",
    "            street = df_after.loc[idx, 'Street Name']\n",
    "            project = df_after.loc[idx, 'Project Name']\n",
    "            print(f\"  Street: {street} -> Project: {project}\")\n",
    "    \n",
    "    # Show remaining missing ones\n",
    "    if missing_after > 0:\n",
    "        missing_samples = df_after[df_after['Project Name'].isna() | \n",
    "                                 (df_after['Project Name'].str.strip() == '')].head(5)\n",
    "        \n",
    "        print(f\"\\n REMAINING MISSING PROJECT NAMES (sample):\")\n",
    "        for idx, row in missing_samples.iterrows():\n",
    "            print(f\"  Street: {row['Street Name']}\")\n",
    "\n",
    "# Now run the complete pipeline\n",
    "print(\"STARTING PROJECT NAME FILLING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make a copy of your original data to preserve it\n",
    "industrial_clean_filled = industrial_df.copy()\n",
    "\n",
    "# Step 1: Comprehensive filling\n",
    "industrial_complete = fill_project_names_comprehensive(industrial_clean_filled)\n",
    "\n",
    "# Step 2: Apply custom rules\n",
    "industrial_final = add_custom_project_rules(industrial_complete)\n",
    "\n",
    "# Step 3: Verify results\n",
    "verify_project_name_filling(industrial_df, industrial_final)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n FINAL DATA QUALITY:\")\n",
    "print(f\"Total properties: {len(industrial_final)}\")\n",
    "print(f\"Properties with project names: {len(industrial_final[industrial_final['Project Name'].notna() & (industrial_final['Project Name'].str.strip() != '')])}\")\n",
    "print(f\"Properties without project names: {len(industrial_final[industrial_final['Project Name'].isna() | (industrial_final['Project Name'].str.strip() == '')])}\")\n",
    "\n",
    "# Show some examples of the results\n",
    "print(f\"\\n SAMPLE RESULTS:\")\n",
    "sample_data = industrial_final[['Street Name', 'Project Name', 'Planning Area']].head(10)\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"  {row['Street Name']} -> {row['Project Name']} ({row['Planning Area']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2924d5-90a9-441a-9b6e-89a55b8a445e",
   "metadata": {},
   "source": [
    "## fill in remaining values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8c283b-af94-477b-8c37-ba184fb33b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining null Project Names: 95\n",
      "Direct mapping filled: 1\n",
      "Pattern extraction filled: -261\n",
      "Initial missing: 95\n",
      "Final missing: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "Final null counts:\n",
      "Street Name        1\n",
      "Floor Level        1\n",
      "Postal Sector      2\n",
      "Postal District    1\n",
      "dtype: int64\n",
      "\n",
      "Sample filled project names:\n",
      "  woodlands industrial park e9 -> WOODLANDS INDUSTRIAL PARK\n",
      "  alexandra terrace -> CATTEL BUILDING\n",
      "  hillview avenue -> LAM SOON INDUSTRIAL BUILDING\n",
      "  kaki bukit road 4 -> KAKI BUKIT INDUSTRIAL ESTATE\n",
      "  bukit batok crescent -> BUKIT BATOK INDUSTRIAL ESTATE\n",
      "  woodlands link -> woodlands east industrial estate\n",
      "  jalan pemimpin -> mapex\n",
      "  jalan pemimpin -> m38\n"
     ]
    }
   ],
   "source": [
    "# Quick cleaning for remaining nulls\n",
    "def clean_remaining_nulls(df):\n",
    "    df_clean = industrial_final.copy()\n",
    "    \n",
    "    # Fill simple nulls\n",
    "    df_clean['Street Name'] = df_clean['Street Name'].fillna('Boon Lay')\n",
    "    #df_clean['Contract Date'] = df_clean['Contract Date'].fillna(df_clean['Contract Date'].mode()[0] if len(df_clean['Contract Date'].mode()) > 0 else '2021-01-01')\n",
    "    df_clean['Floor Level'] = df_clean['Floor Level'].fillna('Unknown')\n",
    "    df_clean['Postal District'] = df_clean['Postal District'].fillna(df_clean['Postal District'].median())\n",
    "    df_clean['Postal Sector'] = df_clean['Postal Sector'].fillna(df_clean['Postal Sector'].median())\n",
    "    \n",
    "    # Fill date-derived columns\n",
    "    latest_year = df_clean['Contract_year'].max()\n",
    "    df_clean['Contract_year'] = df_clean['Contract_year'].fillna(latest_year)\n",
    "    df_clean['Contract_month'] = df_clean['Contract_month'].fillna(1)\n",
    "    \n",
    "    print(f\"Remaining null Project Names: {df_clean['Project Name'].isna().sum()}\")\n",
    "    return df_clean\n",
    "\n",
    "industrial_clean = clean_remaining_nulls(industrial_final)\n",
    "\n",
    "# Efficient project name filling\n",
    "def fill_project_names_efficient(df):\n",
    "    df_filled = industrial_final.copy()\n",
    "    initial_missing = df_filled['Project Name'].isna().sum()\n",
    "    \n",
    "    # Method 1: Direct mapping from existing data\n",
    "    street_to_project = {}\n",
    "    for _, row in df_filled[df_filled['Project Name'].notna()].iterrows():\n",
    "        street = str(row['Street Name']).upper().strip()\n",
    "        project = str(row['Project Name']).strip()\n",
    "        if street and project:\n",
    "            street_to_project[street] = project\n",
    "    \n",
    "    # Apply direct mappings\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "        street = str(row['Street Name']).upper().strip()\n",
    "        if street in street_to_project:\n",
    "            df_filled.at[idx, 'Project Name'] = street_to_project[street]\n",
    "            filled_count += 1\n",
    "    \n",
    "    print(f\"Direct mapping filled: {filled_count}\")\n",
    "    \n",
    "    # Method 2: Extract from street patterns\n",
    "    patterns = [\n",
    "        (r'(.*)\\s(INDUSTRIAL|INDUSTRIAL PARK|BIZHUB)', 1),\n",
    "        (r'(.*)\\s(ROAD|STREET|AVENUE|DRIVE)\\s*(\\d+)', 1),\n",
    "        (r'(.*)\\s(CRESCENT|CIRCLE|PLACE)', 1),\n",
    "    ]\n",
    "    \n",
    "    for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "        street = str(row['Street Name'])\n",
    "        for pattern, group_num in patterns:\n",
    "            match = re.search(pattern, street, re.IGNORECASE)\n",
    "            if match:\n",
    "                df_filled.at[idx, 'Project Name'] = match.group(group_num).strip().title()\n",
    "                filled_count += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Pattern extraction filled: {filled_count - len(street_to_project)}\")\n",
    "    \n",
    "    # Method 3: Planning area + property type as fallback\n",
    "    remaining_missing = df_filled['Project Name'].isna().sum()\n",
    "    if remaining_missing > 0:\n",
    "        for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "            area = str(row['Planning Area']) if pd.notna(row['Planning Area']) else 'Industrial'\n",
    "            prop_type = str(row['Property Type']) if pd.notna(row['Property Type']) else 'Estate'\n",
    "            df_filled.at[idx, 'Project Name'] = f\"{area} {prop_type}\"\n",
    "    \n",
    "    final_missing = df_filled['Project Name'].isna().sum()\n",
    "    print(f\"Initial missing: {initial_missing}\")\n",
    "    print(f\"Final missing: {final_missing}\")\n",
    "    print(f\"Success rate: {(initial_missing - final_missing) / initial_missing * 100:.1f}%\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Apply efficient filling\n",
    "industrial_final_v2 = fill_project_names_efficient(industrial_final)\n",
    "\n",
    "# Verify all nulls are handled\n",
    "print(\"\\nFinal null counts:\")\n",
    "null_summary = industrial_final_v2.isnull().sum()\n",
    "print(null_summary[null_summary > 0])\n",
    "\n",
    "# Sample of filled project names\n",
    "print(\"\\nSample filled project names:\")\n",
    "filled_samples = industrial_final_v2[industrial_final_v2['Project Name'].notna()].head(8)[['Street Name', 'Project Name']]\n",
    "for _, row in filled_samples.iterrows():\n",
    "    print(f\"  {row['Street Name']} -> {row['Project Name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337eeec2-cc81-4fbf-8da8-e8bf2f8b7b05",
   "metadata": {},
   "source": [
    "## predict rental price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6e2541-b80d-4ed0-981b-7a11dba98c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_rental_methods(df, annual_yield=0.055, market_rental_rates=None):\n",
    "    \"\"\"\n",
    "    Calculate rental prices using all three methods for comparison\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Method 1: Standard Yield Method\n",
    "    df['annual_rental_income_yield'] = df['Transacted Price ($)'] * annual_yield\n",
    "    df['monthly_rental_price_yield'] = df['annual_rental_income_yield'] / 12\n",
    "    df['rental_rate_psm_yield'] = df['monthly_rental_price_yield'] / df['Area (SQM)']\n",
    "    \n",
    "    # Method 2: Tenure-Based Yield Method\n",
    "    freehold_yield = 0.05    # 5% for freehold\n",
    "    leasehold_yield = 0.06   # 6% for leasehold\n",
    "    \n",
    "    df['tenure_based_yield'] = df['Tenure_Type'].map({\n",
    "        'Freehold': freehold_yield,\n",
    "        'Leasehold': leasehold_yield\n",
    "    })\n",
    "    df['annual_rental_income_tenure'] = df['Transacted Price ($)'] * df['tenure_based_yield']\n",
    "    df['monthly_rental_price_tenure'] = df['annual_rental_income_tenure'] / 12\n",
    "    df['rental_rate_psm_tenure'] = df['monthly_rental_price_tenure'] / df['Area (SQM)']\n",
    "    \n",
    "    # Method 3: Market Rates Method\n",
    "    if market_rental_rates is None:\n",
    "        # Default market rates if not provided\n",
    "        market_rental_rates = {\n",
    "            'woodlands': 18.0,\n",
    "            'queenstown': 25.0,\n",
    "            'bukit batok': 20.0,\n",
    "            'bedok': 19.0,\n",
    "            'toa payoh': 22.0,\n",
    "            'jurong east': 19.0,\n",
    "            'jurong west': 18.0,\n",
    "            'kallang': 23.0,\n",
    "            'clementi': 21.0,\n",
    "            'serangoon': 20.0\n",
    "        }\n",
    "    \n",
    "    df['planning_area_lower'] = df['Planning Area'].str.lower()\n",
    "    df['market_rent_rate_psm'] = df['planning_area_lower'].map(market_rental_rates)\n",
    "    \n",
    "    # Fill missing rates with overall median\n",
    "    if df['market_rent_rate_psm'].isna().any():\n",
    "        median_rate = df['market_rent_rate_psm'].median()\n",
    "        df['market_rent_rate_psm'] = df['market_rent_rate_psm'].fillna(median_rate)\n",
    "    \n",
    "    df['monthly_rental_price_market'] = df['market_rent_rate_psm'] * df['Area (SQM)']\n",
    "    df['annual_rental_income_market'] = df['monthly_rental_price_market'] * 12\n",
    "    df['implied_yield_market'] = df['annual_rental_income_market'] / df['Transacted Price ($)']\n",
    "    \n",
    "    # Calculate averages across all methods\n",
    "    df['monthly_rental_price_avg'] = df[[\n",
    "        'monthly_rental_price_yield', \n",
    "        'monthly_rental_price_tenure', \n",
    "        'monthly_rental_price_market'\n",
    "    ]].mean(axis=1)\n",
    "    \n",
    "    df['rental_rate_psm_avg'] = df[[\n",
    "        'rental_rate_psm_yield', \n",
    "        'rental_rate_psm_tenure', \n",
    "        'market_rent_rate_psm'\n",
    "    ]].mean(axis=1)\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop('planning_area_lower', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply all three methods\n",
    "industrial_final_v2 = calculate_all_rental_methods(industrial_df, annual_yield=0.055)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c6a5b-1476-4370-9217-d4760958fb9c",
   "metadata": {},
   "source": [
    "## Cordinates loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaeecda9-5ef7-4c05-9de7-3917bdc8a06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Industrial data: (8219, 27)\n",
      "Postal codes: (121154, 4)\n",
      "City coordinates: (332, 9)\n",
      "Street coordinates: (589, 3)\n",
      "Train stations: (209, 4)\n",
      "Postal district mapping: (28, 3)\n"
     ]
    }
   ],
   "source": [
    "postal_codes = pd.read_csv('SG_postal.csv')\n",
    "city_coordinates = pd.read_csv('singapore_city_coordinates_improved.csv')\n",
    "street_coordinates = pd.read_csv('street_coordinates.csv')\n",
    "train_stations = pd.read_csv('mrt_lrt_data.csv')\n",
    "postal_district_mapping = pd.read_csv('sg_postal_districts.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Industrial data: {industrial_df.shape}\")\n",
    "print(f\"Postal codes: {postal_codes.shape}\")\n",
    "print(f\"City coordinates: {city_coordinates.shape}\")\n",
    "print(f\"Street coordinates: {street_coordinates.shape}\")\n",
    "print(f\"Train stations: {train_stations.shape}\")\n",
    "print(f\"Postal district mapping: {postal_district_mapping.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff4f8c2-bd58-4fba-ab70-0df089fdf2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Month Year                  Project Name                   Street Name  \\\n",
      "0    2021-01                         wave9  woodlands industrial park e9   \n",
      "1    2021-01               cattel building             alexandra terrace   \n",
      "2    2021-01  lam soon industrial building               hillview avenue   \n",
      "3    2021-01                  synergy @ kb             kaki bukit road 4   \n",
      "4    2021-01                   wcega tower          bukit batok crescent   \n",
      "\n",
      "  Planning Area Type of Sale  Transacted Price ($)  Unit Price ($ PSM)  \\\n",
      "0     woodlands       resale             1108888.0              3772.0   \n",
      "1    queenstown       resale             3800000.0              3007.0   \n",
      "2   bukit batok       resale             1510900.0              6243.0   \n",
      "3         bedok       resale              458000.0              3368.0   \n",
      "4   bukit batok       resale              800000.0              4372.0   \n",
      "\n",
      "   Area (SQM)                  Tenure          Property Type  ...  \\\n",
      "0       294.0  30 yrs from 05/06/2014  multiple-user factory  ...   \n",
      "1      1264.0                freehold  multiple-user factory  ...   \n",
      "2       242.0                freehold  multiple-user factory  ...   \n",
      "3       136.0  30 yrs from 20/01/2012  multiple-user factory  ...   \n",
      "4       183.0  60 yrs from 13/03/1997  multiple-user factory  ...   \n",
      "\n",
      "  tenure_based_yield annual_rental_income_tenure monthly_rental_price_tenure  \\\n",
      "0               0.06                    66533.28                 5544.440000   \n",
      "1               0.05                   190000.00                15833.333333   \n",
      "2               0.05                    75545.00                 6295.416667   \n",
      "3               0.06                    27480.00                 2290.000000   \n",
      "4               0.06                    48000.00                 4000.000000   \n",
      "\n",
      "   rental_rate_psm_tenure  market_rent_rate_psm  monthly_rental_price_market  \\\n",
      "0               18.858639                  18.0                       5292.0   \n",
      "1               12.526371                  25.0                      31600.0   \n",
      "2               26.014118                  20.0                       4840.0   \n",
      "3               16.838235                  19.0                       2584.0   \n",
      "4               21.857923                  20.0                       3660.0   \n",
      "\n",
      "   annual_rental_income_market  implied_yield_market  \\\n",
      "0                      63504.0              0.057268   \n",
      "1                     379200.0              0.099789   \n",
      "2                      58080.0              0.038441   \n",
      "3                      31008.0              0.067703   \n",
      "4                      43920.0              0.054900   \n",
      "\n",
      "   monthly_rental_price_avg  rental_rate_psm_avg  \n",
      "0               5306.281111            18.048575  \n",
      "1              21616.666667            17.101793  \n",
      "2               6020.125000            24.876550  \n",
      "3               2324.388889            17.091095  \n",
      "4               3775.555556            20.631451  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(industrial_final_v2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa2c3f84-eda1-4044-8519-60fddac1a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding geographic features with enhanced coverage...\n",
      "Starting geographic feature engineering for 8219 properties...\n",
      " Strategy 1A: Street name matching â†’ 5549 properties\n",
      "Strategy 1B: Postal district centroids â†’ 2670 properties\n",
      " Strategy 1C: Planning Area matching â†’ 108 properties\n",
      " Strategy 1D: Region-based fallback â†’ 68 properties\n",
      "FINAL COORDINATE COVERAGE: 8219/8219 properties (100.0%)\n",
      " Added general location for 8218 properties\n",
      "Calculating MRT distances for 8219 properties...\n",
      " Added MRT distances for 8219 properties\n",
      " Added region classification for 8218 properties\n",
      "Calculating CBD distances...\n",
      " Added CBD distances for 8219 properties\n",
      " Added urban classification\n",
      "\n",
      "============================================================\n",
      " GEOGRAPHIC FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "FINAL COVERAGE REPORT:\n",
      "   â€¢ Coordinates: 8219/8219\n",
      "   â€¢ MRT Distances: 8219/8219\n",
      "   â€¢ Region Classification: 8218/8219\n",
      "   â€¢ General Location: 8218/8219\n",
      "   â€¢ CBD Distances: 8219/8219\n"
     ]
    }
   ],
   "source": [
    "def add_geographic_features(main_df, postal_df, street_df, city_df, stations_df, district_df):\n",
    "    \"\"\"Enrich industrial data with geographic and proximity features - FIXED VERSION\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from rapidfuzz import process\n",
    "\n",
    "    df_enriched = main_df.copy()\n",
    "    original_count = len(df_enriched)\n",
    "    industrial_final_v2.rename(columns={'Postal_District_Str  ': 'Postal_District'}, inplace=True)\n",
    "    print(f\"Starting geographic feature engineering for {original_count} properties...\")\n",
    "\n",
    "    # --- 1. MULTI-STRATEGY COORDINATE MATCHING ---\n",
    "    \n",
    "    # Strategy 1A: Direct street name matching (your current approach)\n",
    "    if 'Street Name' in df_enriched.columns:\n",
    "        street_df['street_name_clean'] = street_df['street_name'].str.upper().str.strip()\n",
    "        df_enriched['Street_Name_Clean'] = df_enriched['Street Name'].str.upper().str.strip()\n",
    "\n",
    "        street_choices = street_df['street_name_clean'].unique().tolist()\n",
    "\n",
    "        def match_street(name):\n",
    "            if pd.isna(name): return None\n",
    "            match = process.extractOne(name, street_choices, score_cutoff=80)  # Lowered threshold\n",
    "            return match[0] if match else None\n",
    "\n",
    "        df_enriched['Matched_Street'] = df_enriched['Street_Name_Clean'].apply(match_street)\n",
    "\n",
    "        df_enriched = df_enriched.merge(\n",
    "            street_df[['street_name_clean', 'latitude', 'longitude']],\n",
    "            left_on='Matched_Street', right_on='street_name_clean', how='left'\n",
    "        )\n",
    "        street_matches = df_enriched['latitude'].notna().sum()\n",
    "        print(f\" Strategy 1A: Street name matching â†’ {street_matches} properties\")\n",
    "\n",
    "    # Strategy 1B: Postal code matching for missing coordinates\n",
    "    if 'Postal District' in df_enriched.columns and 'postal_code' in postal_df.columns:\n",
    "        # Convert postal codes to district (first 2 digits for Singapore)\n",
    "        postal_df['Postal_District'] = postal_df['postal_code'].astype(str).str[:2]\n",
    "        \n",
    "        # Get centroid coordinates for each postal district\n",
    "        district_coords = postal_df.groupby('Postal_District').agg({\n",
    "            'lat': 'mean',\n",
    "            'lon': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Merge district centroids for properties missing coordinates\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            df_enriched.loc[missing_coords_mask, 'Postal_District_Str'] = df_enriched.loc[missing_coords_mask, 'Postal District'].apply(\n",
    "                lambda x: str(int(x)) if pd.notna(x) else np.nan\n",
    "            )\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                district_coords.rename(columns={'lat': 'lat_district', 'lon': 'lon_district'}),\n",
    "                left_on='Postal_District_Str', right_on='Postal_District', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with district centroids\n",
    "            district_fill_mask = df_enriched['latitude'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            df_enriched.loc[district_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lat_district']\n",
    "            df_enriched.loc[district_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lon_district']\n",
    "            \n",
    "            district_matches = district_fill_mask.sum()\n",
    "            print(f\"Strategy 1B: Postal district centroids â†’ {district_matches} properties\")\n",
    "\n",
    "    # Strategy 1C: Planning Area matching from city coordinates\n",
    "    if 'Planning Area' in df_enriched.columns and 'Place' in city_df.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            city_df['Place_Clean'] = city_df['Place'].str.upper().str.strip()\n",
    "            df_enriched['Planning_Area_Clean'] = df_enriched['Planning Area'].str.upper().str.strip()\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                city_df[['Place_Clean', 'latitude', 'longitude']].rename(\n",
    "                    columns={'latitude': 'lat_city', 'longitude': 'lon_city'}\n",
    "                ),\n",
    "                left_on='Planning_Area_Clean', right_on='Place_Clean', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with city coordinates\n",
    "            city_fill_mask = df_enriched['latitude'].isna() & df_enriched['Planning_Area_Clean'].notna()\n",
    "            df_enriched.loc[city_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lat_city']\n",
    "            df_enriched.loc[city_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lon_city']\n",
    "            \n",
    "            city_matches = city_fill_mask.sum()\n",
    "            print(f\" Strategy 1C: Planning Area matching â†’ {city_matches} properties\")\n",
    "\n",
    "    # Strategy 1D: Region-based fallback coordinates\n",
    "    if 'Region' in df_enriched.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            # Define approximate coordinates for major regions\n",
    "            region_coords = {\n",
    "                'CENTRAL REGION': (1.2923, 103.8536),  # Singapore central\n",
    "                'EAST REGION': (1.3443, 103.9645),     # East area\n",
    "                'WEST REGION': (1.3526, 103.7584),     # West area\n",
    "                'NORTH REGION': (1.4180, 103.8200),    # North area\n",
    "                'NORTH-EAST REGION': (1.3691, 103.8975) # North-East\n",
    "            }\n",
    "            \n",
    "            def get_region_coords(region):\n",
    "                if pd.isna(region): return (np.nan, np.nan)\n",
    "                region_upper = str(region).upper()\n",
    "                for key, coords in region_coords.items():\n",
    "                    if key in region_upper:\n",
    "                        return coords\n",
    "                return (np.nan, np.nan)\n",
    "            \n",
    "            region_coords_df = df_enriched[missing_coords_mask]['Region'].apply(get_region_coords)\n",
    "            region_fill_mask = df_enriched['latitude'].isna() & df_enriched['Region'].notna()\n",
    "            \n",
    "            df_enriched.loc[region_fill_mask, 'latitude'] = region_coords_df.apply(lambda x: x[0])\n",
    "            df_enriched.loc[region_fill_mask, 'longitude'] = region_coords_df.apply(lambda x: x[1])\n",
    "            \n",
    "            region_matches = region_fill_mask.sum()\n",
    "            print(f\" Strategy 1D: Region-based fallback â†’ {region_matches} properties\")\n",
    "\n",
    "    # Final coordinate coverage report\n",
    "    final_coverage = df_enriched['latitude'].notna().sum()\n",
    "    print(f\"FINAL COORDINATE COVERAGE: {final_coverage}/{original_count} properties ({final_coverage/original_count*100:.1f}%)\")\n",
    "\n",
    "    # --- 2. ENHANCED POSTAL DISTRICT FEATURES ---\n",
    "    \n",
    "    if 'Postal District' in df_enriched.columns:\n",
    "        # Create proper postal district string (handle NaNs)\n",
    "        df_enriched['Postal_District_Str'] = df_enriched['Postal District'].apply(\n",
    "            lambda x: str(int(x)) if pd.notna(x) else 'Unknown'\n",
    "        )\n",
    "        \n",
    "        district_df['Postal District'] = district_df['Postal District'].astype(str)\n",
    "        \n",
    "        df_enriched = df_enriched.merge(\n",
    "            district_df[['Postal District', 'General Location']].rename(columns={'General Location': 'General_Location'}),\n",
    "            left_on='Postal_District_Str', right_on='Postal District', how='left'\n",
    "        )\n",
    "        print(f\" Added general location for {df_enriched['General_Location'].notna().sum()} properties\")\n",
    "\n",
    "    # --- 3. FIXED MRT DISTANCE CALCULATION ---\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate Haversine distance between two points in kilometers\"\"\"\n",
    "        R = 6371  # Earth radius in kilometers\n",
    "        \n",
    "        lat1_rad = np.radians(lat1)\n",
    "        lon1_rad = np.radians(lon1)\n",
    "        lat2_rad = np.radians(lat2)\n",
    "        lon2_rad = np.radians(lon2)\n",
    "        \n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        \n",
    "        return R * c\n",
    "\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        station_coords = stations_df[['Latitude', 'Longitude']].dropna()\n",
    "        \n",
    "        if len(valid_coords) > 0 and len(station_coords) > 0:\n",
    "            print(f\"Calculating MRT distances for {len(valid_coords)} properties...\")\n",
    "            \n",
    "            # Vectorized distance calculation\n",
    "            min_dists = []\n",
    "            for idx, prop_row in valid_coords.iterrows():\n",
    "                prop_lat, prop_lon = prop_row['latitude'], prop_row['longitude']\n",
    "                \n",
    "                # Calculate distances to all stations\n",
    "                distances = []\n",
    "                for _, station_row in station_coords.iterrows():\n",
    "                    dist = haversine_distance(\n",
    "                        prop_lat, prop_lon,\n",
    "                        station_row['Latitude'], station_row['Longitude']\n",
    "                    )\n",
    "                    distances.append(dist)\n",
    "                \n",
    "                min_dists.append(min(distances) if distances else np.nan)\n",
    "            \n",
    "            # Assign distances back to dataframe\n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_MRT_km'] = min_dists\n",
    "            print(f\" Added MRT distances for {len([x for x in min_dists if not np.isnan(x)])} properties\")\n",
    "            \n",
    "            # For properties without coordinates, use district average\n",
    "            missing_mrt_mask = df_enriched['Distance_to_MRT_km'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            if missing_mrt_mask.any():\n",
    "                district_mrt_avg = df_enriched.groupby('Postal_District_Str')['Distance_to_MRT_km'].mean()\n",
    "                df_enriched.loc[missing_mrt_mask, 'Distance_to_MRT_km'] = df_enriched.loc[missing_mrt_mask, 'Postal_District_Str'].map(district_mrt_avg)\n",
    "                print(f\" Added district-average MRT distances for {missing_mrt_mask.sum()} properties\")\n",
    "\n",
    "    # --- 4. ENHANCED REGION CLASSIFICATION ---\n",
    "    \n",
    "    def classify_region(d):\n",
    "        if pd.isna(d) or d == 'Unknown': return 'Unknown'\n",
    "        try:\n",
    "            d_int = int(d)\n",
    "            if d_int <= 9: return 'Central Core'\n",
    "            elif d_int <= 16: return 'Rest Central'\n",
    "            elif d_int <= 21: return 'City Fringe'\n",
    "            elif d_int <= 28: return 'Outside Central'\n",
    "            else: return 'Unknown'\n",
    "        except:\n",
    "            return 'Unknown'\n",
    "\n",
    "    df_enriched['Region_Classification'] = df_enriched['Postal_District_Str'].apply(classify_region)\n",
    "    region_coverage = (df_enriched['Region_Classification'] != 'Unknown').sum()\n",
    "    print(f\" Added region classification for {region_coverage} properties\")\n",
    "\n",
    "    # --- 5. ADDITIONAL GEOGRAPHIC FEATURES ---\n",
    "    \n",
    "    # CBD proximity (distance to Raffles Place)\n",
    "    cbd_coords = (1.2833, 103.8515)  # Raffles Place\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        if len(valid_coords) > 0:\n",
    "            print(\"Calculating CBD distances...\")\n",
    "            cbd_distances = []\n",
    "            for idx, row in valid_coords.iterrows():\n",
    "                dist = haversine_distance(\n",
    "                    row['latitude'], row['longitude'],\n",
    "                    cbd_coords[0], cbd_coords[1]\n",
    "                )\n",
    "                cbd_distances.append(dist)\n",
    "            \n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_CBD_km'] = cbd_distances\n",
    "            print(f\" Added CBD distances for {len(cbd_distances)} properties\")\n",
    "\n",
    "    # Urban vs Suburban classification\n",
    "    def classify_urban_rural(distance_to_cbd):\n",
    "        if pd.isna(distance_to_cbd): return 'Unknown'\n",
    "        if distance_to_cbd <= 5: return 'CBD'\n",
    "        elif distance_to_cbd <= 10: return 'Urban'\n",
    "        elif distance_to_cbd <= 20: return 'Suburban'\n",
    "        else: return 'Rural'\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        df_enriched['Urban_Classification'] = df_enriched['Distance_to_CBD_km'].apply(classify_urban_rural)\n",
    "        print(f\" Added urban classification\")\n",
    "\n",
    "    # --- FINAL REPORT ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" GEOGRAPHIC FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"FINAL COVERAGE REPORT:\")\n",
    "    print(f\"   â€¢ Coordinates: {df_enriched['latitude'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ MRT Distances: {df_enriched['Distance_to_MRT_km'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ Region Classification: {(df_enriched['Region_Classification'] != 'Unknown').sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ General Location: {df_enriched['General_Location'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        print(f\"   â€¢ CBD Distances: {df_enriched['Distance_to_CBD_km'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = [col for col in df_enriched.columns if col in ['Matched_Street', 'street_name_clean', 'Postal_District_Str', 'Postal District_y']]\n",
    "    df_enriched = df_enriched.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "# Call the improved function\n",
    "print(\"\\nAdding geographic features with enhanced coverage...\")\n",
    "industrial_enriched = add_geographic_features(\n",
    "    industrial_final_v2, \n",
    "    postal_codes, \n",
    "    street_coordinates, \n",
    "    city_coordinates, \n",
    "    train_stations, \n",
    "    postal_district_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2813fab6-2480-4339-97e5-2f51e7499dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "industrial_enriched[\"Unit Price ($ PSF)\"] = industrial_enriched[\"Unit Price ($ PSM)\"] / 10.7639"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e6b3fad-35cf-4293-bda0-d592fc3f635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month Year</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Planning Area</th>\n",
       "      <th>Type of Sale</th>\n",
       "      <th>Transacted Price ($)</th>\n",
       "      <th>Unit Price ($ PSM)</th>\n",
       "      <th>Area (SQM)</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Street_Name_Clean</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Planning_Area_Clean</th>\n",
       "      <th>General_Location</th>\n",
       "      <th>Distance_to_MRT_km</th>\n",
       "      <th>Region_Classification</th>\n",
       "      <th>Distance_to_CBD_km</th>\n",
       "      <th>Urban_Classification</th>\n",
       "      <th>Unit Price ($ PSF)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>wave9</td>\n",
       "      <td>woodlands industrial park e9</td>\n",
       "      <td>woodlands</td>\n",
       "      <td>resale</td>\n",
       "      <td>1108888.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>30 yrs from 05/06/2014</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>WOODLANDS INDUSTRIAL PARK E9</td>\n",
       "      <td>1.440807</td>\n",
       "      <td>103.771032</td>\n",
       "      <td>WOODLANDS</td>\n",
       "      <td>Yishun, Sembawang</td>\n",
       "      <td>0.982056</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>19.666080</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>350.430606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>cattel building</td>\n",
       "      <td>alexandra terrace</td>\n",
       "      <td>queenstown</td>\n",
       "      <td>resale</td>\n",
       "      <td>3800000.0</td>\n",
       "      <td>3007.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>ALEXANDRA TERRACE</td>\n",
       "      <td>1.291067</td>\n",
       "      <td>103.819738</td>\n",
       "      <td>QUEENSTOWN</td>\n",
       "      <td>Pasir Panjang, Hong Leong Garden, Clementi New...</td>\n",
       "      <td>0.365268</td>\n",
       "      <td>Central Core</td>\n",
       "      <td>3.634989</td>\n",
       "      <td>CBD</td>\n",
       "      <td>279.359712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>lam soon industrial building</td>\n",
       "      <td>hillview avenue</td>\n",
       "      <td>bukit batok</td>\n",
       "      <td>resale</td>\n",
       "      <td>1510900.0</td>\n",
       "      <td>6243.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>HILLVIEW AVENUE</td>\n",
       "      <td>1.355502</td>\n",
       "      <td>103.761731</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...</td>\n",
       "      <td>0.826301</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>12.807806</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>579.994240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>synergy @ kb</td>\n",
       "      <td>kaki bukit road 4</td>\n",
       "      <td>bedok</td>\n",
       "      <td>resale</td>\n",
       "      <td>458000.0</td>\n",
       "      <td>3368.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>30 yrs from 20/01/2012</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>KAKI BUKIT ROAD 4</td>\n",
       "      <td>1.341884</td>\n",
       "      <td>103.956072</td>\n",
       "      <td>BEDOK</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.334138</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>13.325616</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>312.897742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>wcega tower</td>\n",
       "      <td>bukit batok crescent</td>\n",
       "      <td>bukit batok</td>\n",
       "      <td>resale</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>4372.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>60 yrs from 13/03/1997</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>1.348442</td>\n",
       "      <td>103.746379</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...</td>\n",
       "      <td>0.360454</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>13.748672</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>406.172484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8214</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>henderson industrial park</td>\n",
       "      <td>henderson road</td>\n",
       "      <td>bukit merah</td>\n",
       "      <td>resale</td>\n",
       "      <td>1788000.0</td>\n",
       "      <td>9824.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>HENDERSON ROAD</td>\n",
       "      <td>1.282146</td>\n",
       "      <td>103.819361</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>Queenstown, Tiong Bahru</td>\n",
       "      <td>0.871885</td>\n",
       "      <td>Central Core</td>\n",
       "      <td>3.575114</td>\n",
       "      <td>CBD</td>\n",
       "      <td>912.680348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8215</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>trivex</td>\n",
       "      <td>burn road</td>\n",
       "      <td>toa payoh</td>\n",
       "      <td>resale</td>\n",
       "      <td>726000.0</td>\n",
       "      <td>8067.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60 yrs from 20/05/2008</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>BURN ROAD</td>\n",
       "      <td>1.301155</td>\n",
       "      <td>103.786307</td>\n",
       "      <td>TOA PAYOH</td>\n",
       "      <td>Macpherson, Braddell</td>\n",
       "      <td>0.201015</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>7.514296</td>\n",
       "      <td>Urban</td>\n",
       "      <td>749.449549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>excalibur centre</td>\n",
       "      <td>ubi crescent</td>\n",
       "      <td>geylang</td>\n",
       "      <td>resale</td>\n",
       "      <td>708000.0</td>\n",
       "      <td>5168.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>60 yrs from 05/07/1997</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>UBI CRESCENT</td>\n",
       "      <td>1.296707</td>\n",
       "      <td>103.803661</td>\n",
       "      <td>GEYLANG</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.321296</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>5.523142</td>\n",
       "      <td>Urban</td>\n",
       "      <td>480.123375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>vertex</td>\n",
       "      <td>ubi avenue 3</td>\n",
       "      <td>geylang</td>\n",
       "      <td>resale</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>6061.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>60 yrs from 01/01/2007</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>...</td>\n",
       "      <td>UBI AVENUE 3</td>\n",
       "      <td>1.331528</td>\n",
       "      <td>103.866315</td>\n",
       "      <td>GEYLANG</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.305073</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>5.609937</td>\n",
       "      <td>Urban</td>\n",
       "      <td>563.085870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>primz bizhub</td>\n",
       "      <td>woodlands close</td>\n",
       "      <td>woodlands</td>\n",
       "      <td>resale</td>\n",
       "      <td>775000.0</td>\n",
       "      <td>6458.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60 yrs from 27/09/2011</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>WOODLANDS CLOSE</td>\n",
       "      <td>1.442340</td>\n",
       "      <td>103.796633</td>\n",
       "      <td>WOODLANDS</td>\n",
       "      <td>Kranji, Woodgrove</td>\n",
       "      <td>0.521949</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>18.706665</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>599.968413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8219 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Month Year                  Project Name                   Street Name  \\\n",
       "0       2021-01                         wave9  woodlands industrial park e9   \n",
       "1       2021-01               cattel building             alexandra terrace   \n",
       "2       2021-01  lam soon industrial building               hillview avenue   \n",
       "3       2021-01                  synergy @ kb             kaki bukit road 4   \n",
       "4       2021-01                   wcega tower          bukit batok crescent   \n",
       "...         ...                           ...                           ...   \n",
       "8214    2025-01     henderson industrial park                henderson road   \n",
       "8215    2025-01                        trivex                     burn road   \n",
       "8216    2025-01              excalibur centre                  ubi crescent   \n",
       "8217    2025-01                        vertex                  ubi avenue 3   \n",
       "8218    2025-01                  primz bizhub               woodlands close   \n",
       "\n",
       "     Planning Area Type of Sale  Transacted Price ($)  Unit Price ($ PSM)  \\\n",
       "0        woodlands       resale             1108888.0              3772.0   \n",
       "1       queenstown       resale             3800000.0              3007.0   \n",
       "2      bukit batok       resale             1510900.0              6243.0   \n",
       "3            bedok       resale              458000.0              3368.0   \n",
       "4      bukit batok       resale              800000.0              4372.0   \n",
       "...            ...          ...                   ...                 ...   \n",
       "8214   bukit merah       resale             1788000.0              9824.0   \n",
       "8215     toa payoh       resale              726000.0              8067.0   \n",
       "8216       geylang       resale              708000.0              5168.0   \n",
       "8217       geylang       resale              800000.0              6061.0   \n",
       "8218     woodlands       resale              775000.0              6458.0   \n",
       "\n",
       "      Area (SQM)                  Tenure          Property Type  ...  \\\n",
       "0          294.0  30 yrs from 05/06/2014  multiple-user factory  ...   \n",
       "1         1264.0                freehold  multiple-user factory  ...   \n",
       "2          242.0                freehold  multiple-user factory  ...   \n",
       "3          136.0  30 yrs from 20/01/2012  multiple-user factory  ...   \n",
       "4          183.0  60 yrs from 13/03/1997  multiple-user factory  ...   \n",
       "...          ...                     ...                    ...  ...   \n",
       "8214       182.0                freehold  multiple-user factory  ...   \n",
       "8215        90.0  60 yrs from 20/05/2008  multiple-user factory  ...   \n",
       "8216       137.0  60 yrs from 05/07/1997  multiple-user factory  ...   \n",
       "8217       132.0  60 yrs from 01/01/2007              warehouse  ...   \n",
       "8218       120.0  60 yrs from 27/09/2011  multiple-user factory  ...   \n",
       "\n",
       "                 Street_Name_Clean  latitude   longitude  Planning_Area_Clean  \\\n",
       "0     WOODLANDS INDUSTRIAL PARK E9  1.440807  103.771032            WOODLANDS   \n",
       "1                ALEXANDRA TERRACE  1.291067  103.819738           QUEENSTOWN   \n",
       "2                  HILLVIEW AVENUE  1.355502  103.761731          BUKIT BATOK   \n",
       "3                KAKI BUKIT ROAD 4  1.341884  103.956072                BEDOK   \n",
       "4             BUKIT BATOK CRESCENT  1.348442  103.746379          BUKIT BATOK   \n",
       "...                            ...       ...         ...                  ...   \n",
       "8214                HENDERSON ROAD  1.282146  103.819361          BUKIT MERAH   \n",
       "8215                     BURN ROAD  1.301155  103.786307            TOA PAYOH   \n",
       "8216                  UBI CRESCENT  1.296707  103.803661              GEYLANG   \n",
       "8217                  UBI AVENUE 3  1.331528  103.866315              GEYLANG   \n",
       "8218               WOODLANDS CLOSE  1.442340  103.796633            WOODLANDS   \n",
       "\n",
       "                                       General_Location  Distance_to_MRT_km  \\\n",
       "0                                     Yishun, Sembawang            0.982056   \n",
       "1     Pasir Panjang, Hong Leong Garden, Clementi New...            0.365268   \n",
       "2     Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...            0.826301   \n",
       "3                                        Geylang, Eunos            0.334138   \n",
       "4     Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...            0.360454   \n",
       "...                                                 ...                 ...   \n",
       "8214                            Queenstown, Tiong Bahru            0.871885   \n",
       "8215                               Macpherson, Braddell            0.201015   \n",
       "8216                                     Geylang, Eunos            0.321296   \n",
       "8217                                     Geylang, Eunos            0.305073   \n",
       "8218                                  Kranji, Woodgrove            0.521949   \n",
       "\n",
       "      Region_Classification  Distance_to_CBD_km  Urban_Classification  \\\n",
       "0           Outside Central           19.666080              Suburban   \n",
       "1              Central Core            3.634989                   CBD   \n",
       "2           Outside Central           12.807806              Suburban   \n",
       "3              Rest Central           13.325616              Suburban   \n",
       "4           Outside Central           13.748672              Suburban   \n",
       "...                     ...                 ...                   ...   \n",
       "8214           Central Core            3.575114                   CBD   \n",
       "8215           Rest Central            7.514296                 Urban   \n",
       "8216           Rest Central            5.523142                 Urban   \n",
       "8217           Rest Central            5.609937                 Urban   \n",
       "8218        Outside Central           18.706665              Suburban   \n",
       "\n",
       "      Unit Price ($ PSF)  \n",
       "0             350.430606  \n",
       "1             279.359712  \n",
       "2             579.994240  \n",
       "3             312.897742  \n",
       "4             406.172484  \n",
       "...                  ...  \n",
       "8214          912.680348  \n",
       "8215          749.449549  \n",
       "8216          480.123375  \n",
       "8217          563.085870  \n",
       "8218          599.968413  \n",
       "\n",
       "[8219 rows x 50 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec8965-c7a9-49c3-87ef-f1cbe981974a",
   "metadata": {},
   "source": [
    "## remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e09f1cc3-c189-40e7-9800-4f5b59ab1f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Handling obvious data errors...\n",
      "Removing 1 impossible values (>$100,000 PSF)\n",
      "Removing 2 suspicious low values (<$10 PSF)\n",
      "\n",
      "STEP 2: Applying domain knowledge bounds...\n",
      "Domain knowledge outliers: 269\n",
      "\n",
      "STEP 3: Applying statistical outliers within domain bounds...\n",
      "IQR bounds: $-745.83 - $1816.44\n",
      "Statistical outliers: 35\n",
      "\n",
      "FINAL RESULT:\n",
      "Total outliers removed: 269 (3.27%)\n",
      "Original: 8219 records\n",
      "Cleaned: 7947 records\n"
     ]
    }
   ],
   "source": [
    "def smart_outlier_removal(df, target_column):\n",
    "    \"\"\"\n",
    "    Hybrid approach: Combine statistical methods with domain knowledge\n",
    "    and handle clear data errors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Handle obvious data errors first\n",
    "    print(\"STEP 1: Handling obvious data errors...\")\n",
    "    \n",
    "    # Check for the extreme outlier - likely a data entry error\n",
    "    extreme_outlier_mask = df[target_column] > 100000  # $100,000 PSF is impossible\n",
    "    if extreme_outlier_mask.any():\n",
    "        print(f\"Removing {extreme_outlier_mask.sum()} impossible values (>$100,000 PSF)\")\n",
    "        df = df[~extreme_outlier_mask].copy()\n",
    "    \n",
    "    # Check for near-zero values that don't make business sense\n",
    "    near_zero_mask = df[target_column] < 10  # Less than $10 PSF is suspicious for industrial\n",
    "    if near_zero_mask.any():\n",
    "        print(f\"Removing {near_zero_mask.sum()} suspicious low values (<$10 PSF)\")\n",
    "        df = df[~near_zero_mask].copy()\n",
    "    \n",
    "    # Step 2: Apply conservative domain knowledge bounds\n",
    "    print(\"\\nSTEP 2: Applying domain knowledge bounds...\")\n",
    "    \n",
    "    # Singapore industrial realistic bounds (adjusted based on your data)\n",
    "    lower_bound = 100   # Minimum realistic PSF for industrial\n",
    "    upper_bound = 1500  # Maximum realistic PSF for premium industrial\n",
    "    \n",
    "    domain_outliers = (df[target_column] < lower_bound) | (df[target_column] > upper_bound)\n",
    "    print(f\"Domain knowledge outliers: {domain_outliers.sum()}\")\n",
    "    \n",
    "    # Step 3: Apply moderate IQR for statistical outliers within domain bounds\n",
    "    print(\"\\nSTEP 3: Applying statistical outliers within domain bounds...\")\n",
    "    \n",
    "    Q1 = df[target_column].quantile(0.10)  # Use 10th percentile for more conservative approach\n",
    "    Q3 = df[target_column].quantile(0.90)  # Use 90th percentile\n",
    "    IQR = Q3 - Q1\n",
    "    lower_iqr = Q1 - 1.5 * IQR\n",
    "    upper_iqr = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"IQR bounds: ${lower_iqr:.2f} - ${upper_iqr:.2f}\")\n",
    "    \n",
    "    iqr_outliers = (df[target_column] < lower_iqr) | (df[target_column] > upper_iqr)\n",
    "    print(f\"Statistical outliers: {iqr_outliers.sum()}\")\n",
    "    \n",
    "    # Step 4: Combine both criteria\n",
    "    final_outliers = domain_outliers | iqr_outliers\n",
    "    \n",
    "    print(f\"\\nFINAL RESULT:\")\n",
    "    print(f\"Total outliers removed: {final_outliers.sum()} ({final_outliers.sum()/len(df)*100:.2f}%)\")\n",
    "    print(f\"Original: {len(industrial_enriched)} records\")\n",
    "    print(f\"Cleaned: {len(df[~final_outliers])} records\")\n",
    "    \n",
    "    return df[~final_outliers].copy(), final_outliers\n",
    "\n",
    "# Apply the smart hybrid method\n",
    "industrial_cleaned_smart, smart_outliers_mask = smart_outlier_removal(industrial_enriched, 'Unit Price ($ PSF)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff89be21-1745-46ab-af11-a687f1e3f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: ['Month Year', 'Project Name', 'Street Name', 'Planning Area', 'Type of Sale', 'Tenure', 'Property Type', 'Type of Area', 'Floor Level', 'Region', 'Floor_Category', 'Tenure_Type', 'Street_Name_Clean', 'Planning_Area_Clean', 'General_Location', 'Region_Classification', 'Urban_Classification']\n",
      "Numerical columns: ['Area (SQM)', 'Postal Sector', 'Postal District_x', 'Floor_Low', 'Floor_High', 'Floor_Midpoint', 'Is_Basement', 'Is_Ground', 'Contract_date_missing', 'Contract_year', 'Contract_month', 'Contract_quarter', 'Contract_dayofweek', 'tenure_based_yield', 'latitude', 'longitude', 'Distance_to_MRT_km', 'Distance_to_CBD_km']\n",
      "Training set: (6575, 1776)\n",
      "Testing set: (1644, 1776)\n",
      "\n",
      "Baseline Model (Mean Prediction):\n",
      "MAE: $1,872,074.27\n",
      "RMSE: $7,018,482.58\n"
     ]
    }
   ],
   "source": [
    "target_column='Transacted Price ($)'\n",
    "\n",
    "exclude_columns = ['Transacted Price ($)', 'Unit Price ($ PSM)',\n",
    "    'Unit Price ($ PSF)',\n",
    "    'monthly_rental_price_yield',\n",
    "    'rental_rate_psm_yield',\n",
    "    'monthly_rental_price_tenure',\n",
    "    'rental_rate_psm_tenure',\n",
    "    'monthly_rental_price_market',\n",
    "    'rental_rate_psm_avg',\n",
    "    'monthly_rental_price_avg',\n",
    "    'market_rent_rate_psm',\n",
    "    'annual_rental_income_yield','implied_yield_market','annual_rental_income_tenure','annual_rental_income_market']\n",
    "\n",
    "feature_columns = [col for col in industrial_enriched.columns \n",
    "                  if col not in exclude_columns and col != target_column]\n",
    "\n",
    "categorical_columns = industrial_enriched[feature_columns].select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = industrial_enriched[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(f\"\\nCategorical columns: {list(categorical_columns)}\")\n",
    "print(f\"Numerical columns: {list(numerical_columns)}\")\n",
    "\n",
    "X_encoded = pd.get_dummies(industrial_enriched[feature_columns], columns=categorical_columns, drop_first=True)\n",
    "y = industrial_enriched[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "# Create a simple baseline (predict mean)\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "\n",
    "print(f\"\\nBaseline Model (Mean Prediction):\")\n",
    "print(f\"MAE: ${baseline_mae:,.2f}\")\n",
    "print(f\"RMSE: ${baseline_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c2c5319-b267-4954-abc9-93a6db03189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month Year                      object\n",
       "Project Name                    object\n",
       "Street Name                     object\n",
       "Planning Area                   object\n",
       "Type of Sale                    object\n",
       "Transacted Price ($)           float64\n",
       "Unit Price ($ PSM)             float64\n",
       "Area (SQM)                     float64\n",
       "Tenure                          object\n",
       "Property Type                   object\n",
       "Type of Area                    object\n",
       "Floor Level                     object\n",
       "Region                          object\n",
       "Postal Sector                  float64\n",
       "Postal District_x              float64\n",
       "Floor_Low                        int64\n",
       "Floor_High                       int64\n",
       "Floor_Midpoint                   int64\n",
       "Is_Basement                      int64\n",
       "Is_Ground                        int64\n",
       "Floor_Category                  object\n",
       "Tenure_Type                     object\n",
       "Contract_date_missing            int32\n",
       "Contract_year                  float64\n",
       "Contract_month                 float64\n",
       "Contract_quarter               float64\n",
       "Contract_dayofweek             float64\n",
       "annual_rental_income_yield     float64\n",
       "monthly_rental_price_yield     float64\n",
       "rental_rate_psm_yield          float64\n",
       "tenure_based_yield             float64\n",
       "annual_rental_income_tenure    float64\n",
       "monthly_rental_price_tenure    float64\n",
       "rental_rate_psm_tenure         float64\n",
       "market_rent_rate_psm           float64\n",
       "monthly_rental_price_market    float64\n",
       "annual_rental_income_market    float64\n",
       "implied_yield_market           float64\n",
       "monthly_rental_price_avg       float64\n",
       "rental_rate_psm_avg            float64\n",
       "Street_Name_Clean               object\n",
       "latitude                       float64\n",
       "longitude                      float64\n",
       "Planning_Area_Clean             object\n",
       "General_Location                object\n",
       "Distance_to_MRT_km             float64\n",
       "Region_Classification           object\n",
       "Distance_to_CBD_km             float64\n",
       "Urban_Classification            object\n",
       "Unit Price ($ PSF)             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_enriched.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c30ad09f-de3b-4454-9461-c29227da4102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      "463\n",
      "Missing values in target:\n",
      "0\n",
      "Missing values after imputation - Train: 0\n",
      "Missing values after imputation - Test: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in features:\")\n",
    "print(X_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in target:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(X, strategy='mean'):\n",
    "    \"\"\"Handle missing values in the feature matrix\"\"\"\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "# Apply to training and test data\n",
    "X_train_imputed = handle_missing_values(X_train)\n",
    "X_test_imputed = handle_missing_values(X_test)\n",
    "\n",
    "print(f\"Missing values after imputation - Train: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after imputation - Test: {X_test_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8991d5-2652-4a4e-8f25-a3f7fd195bfb",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54946741-8057-4c21-836e-07768a952427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Hist Gradient Boosting...\n",
      "Hist Gradient Boosting Results:\n",
      "  MAE: $679,568.02\n",
      "  RMSE: $3,661,116.53\n",
      "  RÂ²: 0.7279\n",
      "  Improvement over baseline: 63.7%\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost Results:\n",
      "  MAE: $577,030.57\n",
      "  RMSE: $3,416,134.31\n",
      "  RÂ²: 0.7631\n",
      "  Improvement over baseline: 69.2%\n",
      "\n",
      " Best Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor  # Make sure xgboost is installed too\n",
    "import numpy as np\n",
    "\n",
    "# Updated models with proper data handling\n",
    "models = {\n",
    "    'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        max_iter=100\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        random_state=42,\n",
    "        enable_categorical=False  # Ensure this is False for one-hot encoded data\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models with proper data splits\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        if name == 'Hist Gradient Boosting':\n",
    "            # Use imputed data for HistGradientBoosting\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "            \n",
    "        else:\n",
    "            # Use imputed data for other models\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test_compare, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_compare, y_pred))\n",
    "        r2 = r2_score(y_test_compare, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'predictions': y_pred,\n",
    "            'y_test': y_test_compare\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  MAE: ${mae:,.2f}\")\n",
    "        print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"  RÂ²: {r2:.4f}\")\n",
    "        print(f\"  Improvement over baseline: {((baseline_mae - mae) / baseline_mae * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Find best model\n",
    "if results:\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mae'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\n Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853f3ba-c4bd-4037-a471-f6de893efb1e",
   "metadata": {},
   "source": [
    "## Hypertune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61a68ab5-2515-49af-8292-1a391bc6408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced training pipeline...\n",
      "Starting Enhanced Model Training Pipeline\n",
      "============================================================\n",
      "Training enhanced models with improved configurations...\n",
      "============================================================\n",
      "\n",
      "--- Training Hist Gradient Boosting ---\n",
      "Test MAE: $717,511.63\n",
      "Test RMSE: $3,742,773.70\n",
      "Test RÂ²: 0.7156\n",
      "CV MAE: $817,851.05 Â± $64,282.14\n",
      "Improvement over baseline: +61.7%\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $585,659.19\n",
      "Test RMSE: $3,368,939.28\n",
      "Test RÂ²: 0.7696\n",
      "CV MAE: $653,212.68 Â± $96,727.86\n",
      "Improvement over baseline: +68.7%\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $547,497.99\n",
      "Test RMSE: $2,843,806.98\n",
      "Test RÂ²: 0.8358\n",
      "CV MAE: $645,888.54 Â± $77,921.55\n",
      "Improvement over baseline: +70.8%\n",
      "\n",
      "--- Training CatBoost ---\n",
      "Test MAE: $696,572.98\n",
      "Test RMSE: $3,739,769.91\n",
      "Test RÂ²: 0.7160\n",
      "CV MAE: $760,776.91 Â± $75,131.50\n",
      "Improvement over baseline: +62.8%\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Model Ranking (by MAE):\n",
      " 1. Gradient Boosting         | MAE: $547,497.99 | RÂ²: 0.8358 | CV: $645,888.54 Â± $77,921.55\n",
      " 2. XGBoost                   | MAE: $585,659.19 | RÂ²: 0.7696 | CV: $653,212.68 Â± $96,727.86\n",
      " 3. CatBoost                  | MAE: $696,572.98 | RÂ²: 0.7160 | CV: $760,776.91 Â± $75,131.50\n",
      " 4. Hist Gradient Boosting    | MAE: $717,511.63 | RÂ²: 0.7156 | CV: $817,851.05 Â± $64,282.14\n",
      "\n",
      "--- Error Analysis for Gradient Boosting ---\n",
      "Mean Absolute Error: $547,497.99\n",
      "Median Absolute Error: $135,300.61\n",
      "Max Error: $56,328,316.38\n",
      "Error Std: $2,791,455.51\n",
      "\n",
      "Error Percentage Stats:\n",
      "  Mean: 68.9%\n",
      "  Median: 14.9%\n",
      "  95th percentile: 66.0%\n",
      "\n",
      "Top 5 Largest Errors:\n",
      "  Actual: $40,100,000 | Predicted: $68,207,302 | Error: 70.1%\n",
      "  Actual: $98,767,600 | Predicted: $67,380,510 | Error: 31.8%\n",
      "  Actual: $74,000,000 | Predicted: $39,868,495 | Error: 46.1%\n",
      "  Actual: $131,000,000 | Predicted: $81,720,583 | Error: 37.6%\n",
      "  Actual: $135,200,000 | Predicted: $78,871,684 | Error: 41.7%\n",
      "\n",
      "Creating ensemble from: ['Gradient Boosting', 'XGBoost', 'CatBoost']\n",
      "Ensemble weights:\n",
      "  Gradient Boosting: 0.368\n",
      "  XGBoost: 0.344\n",
      "  CatBoost: 0.289\n",
      "\n",
      "Ensemble Results:\n",
      "MAE: $579,603.16\n",
      "RMSE: $3,155,038.64\n",
      "RÂ²: 0.7979\n",
      "Ensemble vs Best Single Model: -5.9% improvement\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      " RECOMMENDATION: USE Gradient Boosting\n",
      "   MAE: $547,497.99\n",
      "   RÂ²: 0.8358\n",
      "   CV Consistency: $645,888.54 Â± $77,921.55\n",
      "Final model type: GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Enhanced model training with better validation and ensemble options\"\"\"\n",
    "    \n",
    "    print(\"Training enhanced models with improved configurations...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Enhanced models with optimized hyperparameters\n",
    "    models = {\n",
    "        'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_leaf=20,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add CatBoost if you have categorical features\n",
    "    try:\n",
    "        models['CatBoost'] = CatBoostRegressor(\n",
    "            iterations=200,\n",
    "            depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    except:\n",
    "        print(\"CatBoost not available, skipping...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_imputed, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RMSE: ${rmse:,.2f}\")\n",
    "            print(f\"Test RÂ²: {r2:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} Â± ${cv_std:,.2f}\")\n",
    "            \n",
    "            if baseline_mae:\n",
    "                improvement = ((baseline_mae - mae) / baseline_mae * 100)\n",
    "                print(f\"Improvement over baseline: {improvement:+.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Create intelligent ensemble from best performing models\"\"\"\n",
    "    \n",
    "    # Get top 3 models by MAE\n",
    "    valid_models = {k: v for k, v in results.items() if 'mae' in v}\n",
    "    if len(valid_models) < 2:\n",
    "        print(\"Not enough models for ensemble\")\n",
    "        return None, None\n",
    "    \n",
    "    top_models = sorted(valid_models.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    \n",
    "    print(f\"\\nCreating ensemble from: {[name for name, _ in top_models]}\")\n",
    "    \n",
    "    # Create weighted ensemble based on performance\n",
    "    ensemble_models = []\n",
    "    weights = []\n",
    "    \n",
    "    for name, result in top_models:\n",
    "        ensemble_models.append((name, result['model']))\n",
    "        # Weight inversely proportional to MAE (better models get higher weight)\n",
    "        weight = 1.0 / result['mae']\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w/total_weight for w in weights]\n",
    "    \n",
    "    print(\"Ensemble weights:\")\n",
    "    for (name, _), weight in zip(ensemble_models, normalized_weights):\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "    \n",
    "    # Create voting regressor with weights\n",
    "    ensemble = VotingRegressor(\n",
    "        estimators=ensemble_models,\n",
    "        weights=normalized_weights\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.fit(X_train_imputed, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test_imputed)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    mae_ens = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "    rmse_ens = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "    r2_ens = r2_score(y_test, y_pred_ensemble)\n",
    "    \n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"MAE: ${mae_ens:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse_ens:,.2f}\")\n",
    "    print(f\"RÂ²: {r2_ens:.4f}\")\n",
    "    \n",
    "    # Compare with best single model\n",
    "    best_single_mae = top_models[0][1]['mae']\n",
    "    improvement = ((best_single_mae - mae_ens) / best_single_mae) * 100\n",
    "    \n",
    "    print(f\"Ensemble vs Best Single Model: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    return ensemble, mae_ens\n",
    "\n",
    "def analyze_model_performance(results, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Comprehensive model performance analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort models by MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])\n",
    "    \n",
    "    print(\"\\nModel Ranking (by MAE):\")\n",
    "    for i, (name, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i:2d}. {name:25} | MAE: ${result['mae']:,.2f} | RÂ²: {result['r2']:.4f} | CV: ${result['cv_mae']:,.2f} Â± ${result['cv_std']:,.2f}\")\n",
    "    \n",
    "    best_model_name, best_result = sorted_results[0]\n",
    "    \n",
    "    # Error analysis for best model\n",
    "    print(f\"\\n--- Error Analysis for {best_model_name} ---\")\n",
    "    y_pred_best = best_result['predictions']\n",
    "    errors = np.abs(y_test - y_pred_best)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: ${errors.mean():,.2f}\")\n",
    "    print(f\"Median Absolute Error: ${np.median(errors):,.2f}\")\n",
    "    print(f\"Max Error: ${errors.max():,.2f}\")\n",
    "    print(f\"Error Std: ${errors.std():,.2f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_pct = (errors / y_test) * 100\n",
    "    print(f\"\\nError Percentage Stats:\")\n",
    "    print(f\"  Mean: {error_pct.mean():.1f}%\")\n",
    "    print(f\"  Median: {error_pct.median():.1f}%\")\n",
    "    print(f\"  95th percentile: {np.percentile(error_pct, 95):.1f}%\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    worst_indices = np.argsort(errors)[-5:]\n",
    "    print(f\"\\nTop 5 Largest Errors:\")\n",
    "    for idx in worst_indices:\n",
    "        actual = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]\n",
    "        predicted = y_pred_best[idx]\n",
    "        error = abs(actual - predicted)\n",
    "        error_pct = (error / actual) * 100\n",
    "        print(f\"  Actual: ${actual:,.0f} | Predicted: ${predicted:,.0f} | Error: {error_pct:.1f}%\")\n",
    "    \n",
    "    return best_model_name, best_result\n",
    "\n",
    "# Main execution function\n",
    "def run_enhanced_training(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Complete enhanced training pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting Enhanced Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Train individual models\n",
    "    results = train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No models trained successfully!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. Analyze performance\n",
    "    best_model_name, best_result = analyze_model_performance(results, X_test_imputed, y_test)\n",
    "    \n",
    "    # 3. Create ensemble\n",
    "    ensemble_model, ensemble_mae = create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test)\n",
    "    \n",
    "    # 4. Final recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ensemble_model and ensemble_mae < best_result['mae']:\n",
    "        print(\" RECOMMENDATION: USE ENSEMBLE MODEL\")\n",
    "        print(f\"   Ensemble MAE: ${ensemble_mae:,.2f}\")\n",
    "        print(f\"   Best Single MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Improvement: {((best_result['mae'] - ensemble_mae) / best_result['mae'] * 100):+.1f}%\")\n",
    "        final_model = ensemble_model\n",
    "    else:\n",
    "        print(f\" RECOMMENDATION: USE {best_model_name}\")\n",
    "        print(f\"   MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   RÂ²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV Consistency: ${best_result['cv_mae']:,.2f} Â± ${best_result['cv_std']:,.2f}\")\n",
    "        final_model = best_result['model']\n",
    "    \n",
    "    return final_model, results\n",
    "\n",
    "# Usage - replace your existing training code with this:\n",
    "print(\"Running enhanced training pipeline...\")\n",
    "final_model, all_results = run_enhanced_training(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    ")\n",
    "\n",
    "# You can then use final_model for predictions\n",
    "if final_model:\n",
    "    #print(\"\\nTraining completed successfully!\")\n",
    "    print(f\"Final model type: {type(final_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31f89f58-e339-4fdc-a2cb-926445897f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced training pipeline...\n",
      "Starting Enhanced Model Training Pipeline\n",
      "============================================================\n",
      "Training enhanced models with improved configurations...\n",
      "============================================================\n",
      "\n",
      "--- Training Hist Gradient Boosting ---\n",
      "Test MAE: $717,511.63\n",
      "Test RMSE: $3,742,773.70\n",
      "Test RÂ²: 0.7156\n",
      "CV MAE: $817,851.05 Â± $64,282.14\n",
      "Improvement over baseline: +61.7%\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $585,659.19\n",
      "Test RMSE: $3,368,939.28\n",
      "Test RÂ²: 0.7696\n",
      "CV MAE: $653,212.68 Â± $96,727.86\n",
      "Improvement over baseline: +68.7%\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $547,497.99\n",
      "Test RMSE: $2,843,806.98\n",
      "Test RÂ²: 0.8358\n",
      "CV MAE: $645,888.54 Â± $77,921.55\n",
      "Improvement over baseline: +70.8%\n",
      "\n",
      "--- Training CatBoost ---\n",
      "Test MAE: $696,572.98\n",
      "Test RMSE: $3,739,769.91\n",
      "Test RÂ²: 0.7160\n",
      "CV MAE: $760,776.91 Â± $75,131.50\n",
      "Improvement over baseline: +62.8%\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Model Ranking (by MAE):\n",
      " 1. Gradient Boosting         | MAE: $547,497.99 | RÂ²: 0.8358 | CV: $645,888.54 Â± $77,921.55\n",
      " 2. XGBoost                   | MAE: $585,659.19 | RÂ²: 0.7696 | CV: $653,212.68 Â± $96,727.86\n",
      " 3. CatBoost                  | MAE: $696,572.98 | RÂ²: 0.7160 | CV: $760,776.91 Â± $75,131.50\n",
      " 4. Hist Gradient Boosting    | MAE: $717,511.63 | RÂ²: 0.7156 | CV: $817,851.05 Â± $64,282.14\n",
      "\n",
      "--- Error Analysis for Gradient Boosting ---\n",
      "Mean Absolute Error: $547,497.99\n",
      "Median Absolute Error: $135,300.61\n",
      "Max Error: $56,328,316.38\n",
      "Error Std: $2,791,455.51\n",
      "\n",
      "Error Percentage Stats:\n",
      "  Mean: 68.9%\n",
      "  Median: 14.9%\n",
      "  95th percentile: 66.0%\n",
      "\n",
      "Top 5 Largest Errors:\n",
      "  Actual: $40,100,000 | Predicted: $68,207,302 | Error: 70.1%\n",
      "  Actual: $98,767,600 | Predicted: $67,380,510 | Error: 31.8%\n",
      "  Actual: $74,000,000 | Predicted: $39,868,495 | Error: 46.1%\n",
      "  Actual: $131,000,000 | Predicted: $81,720,583 | Error: 37.6%\n",
      "  Actual: $135,200,000 | Predicted: $78,871,684 | Error: 41.7%\n",
      "\n",
      "Creating ensemble from: ['Gradient Boosting', 'XGBoost', 'CatBoost']\n",
      "Ensemble weights:\n",
      "  Gradient Boosting: 0.368\n",
      "  XGBoost: 0.344\n",
      "  CatBoost: 0.289\n",
      "\n",
      "Ensemble Results:\n",
      "MAE: $579,603.16\n",
      "RMSE: $3,155,038.64\n",
      "RÂ²: 0.7979\n",
      "Ensemble vs Best Single Model: -5.9% improvement\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      " RECOMMENDATION: USE Gradient Boosting\n",
      "   MAE: $547,497.99\n",
      "   RÂ²: 0.8358\n",
      "   CV Consistency: $645,888.54 Â± $77,921.55\n",
      "\n",
      "Final model type: GradientBoostingRegressor\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# Run the full training pipeline\n",
    "print(\"Running enhanced training pipeline...\")\n",
    "final_model, all_results = run_enhanced_training(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    ")\n",
    "\n",
    "# If a model was successfully trained\n",
    "if final_model:\n",
    "    print(f\"\\nFinal model type: {type(final_model).__name__}\")\n",
    "    \n",
    "    # Identify the best-performing model and its metrics\n",
    "    sorted_results = sorted(\n",
    "        all_results.items(), \n",
    "        key=lambda x: x[1]['mae']\n",
    "    )\n",
    "    best_model_name, best_result = sorted_results[0]\n",
    "    \n",
    "    # Prepare feature names\n",
    "    deployment_features = list(X_train_imputed.columns)\n",
    "    \n",
    "    # Define model type (string)\n",
    "    model_type = type(final_model).__name__\n",
    "    \n",
    "    # Create a deployment package for export\n",
    "    deployment_package = {\n",
    "        'model': final_model,\n",
    "        'feature_names': deployment_features,\n",
    "        'performance': best_result,\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'model_type': model_type,\n",
    "        'data_shape': {\n",
    "            'n_features': len(deployment_features),\n",
    "            'n_train_samples': X_train_imputed.shape[0],\n",
    "            'n_test_samples': X_test_imputed.shape[0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save to PKL file\n",
    "    filename = 'industrial_real_estate_model_final.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(deployment_package, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64da68b-d728-412c-b56e-6bd977fa177f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3e0d6-0e44-4b5d-b30a-d872fef51496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

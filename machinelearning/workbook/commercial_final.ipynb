{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a53d81-ec61-48bc-8b37-053fc0807806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import joblib\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e3682e-aeaa-4871-badd-4b20f72e0acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Transacted Price ($)</th>\n",
       "      <th>Area (SQFT)</th>\n",
       "      <th>Unit Price ($ PSF)</th>\n",
       "      <th>Sale Date</th>\n",
       "      <th>Type of Area</th>\n",
       "      <th>Area (SQM)</th>\n",
       "      <th>Unit Price ($ PSM)</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Floor Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N.A.</td>\n",
       "      <td>LORONG 25A GEYLANG</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>1,400,000.00</td>\n",
       "      <td>1,033.34</td>\n",
       "      <td>1,355</td>\n",
       "      <td>Sept-25</td>\n",
       "      <td>Strata</td>\n",
       "      <td>96</td>\n",
       "      <td>14,583</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>14</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KAMPONG GLAM CONSERVATION AREA</td>\n",
       "      <td>BUSSORAH STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>6,350,000.00</td>\n",
       "      <td>1,558.63</td>\n",
       "      <td>4,074</td>\n",
       "      <td>Sept-25</td>\n",
       "      <td>Land</td>\n",
       "      <td>144.8</td>\n",
       "      <td>43,854</td>\n",
       "      <td>99 yrs lease commencing from 2003</td>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N.A.</td>\n",
       "      <td>TOH AVENUE</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>2,000,000.00</td>\n",
       "      <td>1,703.94</td>\n",
       "      <td>1,174</td>\n",
       "      <td>Aug-25</td>\n",
       "      <td>Land</td>\n",
       "      <td>158.3</td>\n",
       "      <td>12,634</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>17</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N.A.</td>\n",
       "      <td>CEYLON ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>3,850,000.00</td>\n",
       "      <td>1,371.33</td>\n",
       "      <td>2,807</td>\n",
       "      <td>Aug-25</td>\n",
       "      <td>Land</td>\n",
       "      <td>127.4</td>\n",
       "      <td>30,220</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>15</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N.A.</td>\n",
       "      <td>CLOVER WAY</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>4,200,000.00</td>\n",
       "      <td>1,767.45</td>\n",
       "      <td>2,376</td>\n",
       "      <td>Aug-25</td>\n",
       "      <td>Land</td>\n",
       "      <td>164.2</td>\n",
       "      <td>25,579</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Project Name         Street Name Property Type  \\\n",
       "0                            N.A.  LORONG 25A GEYLANG    Shop House   \n",
       "1  KAMPONG GLAM CONSERVATION AREA     BUSSORAH STREET    Shop House   \n",
       "2                            N.A.          TOH AVENUE    Shop House   \n",
       "3                            N.A.         CEYLON ROAD    Shop House   \n",
       "4                            N.A.          CLOVER WAY    Shop House   \n",
       "\n",
       "  Transacted Price ($) Area (SQFT) Unit Price ($ PSF) Sale Date Type of Area  \\\n",
       "0         1,400,000.00    1,033.34              1,355   Sept-25       Strata   \n",
       "1         6,350,000.00    1,558.63              4,074   Sept-25         Land   \n",
       "2         2,000,000.00    1,703.94              1,174    Aug-25         Land   \n",
       "3         3,850,000.00    1,371.33              2,807    Aug-25         Land   \n",
       "4         4,200,000.00    1,767.45              2,376    Aug-25         Land   \n",
       "\n",
       "  Area (SQM) Unit Price ($ PSM)                             Tenure  \\\n",
       "0         96             14,583                           Freehold   \n",
       "1      144.8             43,854  99 yrs lease commencing from 2003   \n",
       "2      158.3             12,634                           Freehold   \n",
       "3      127.4             30,220                           Freehold   \n",
       "4      164.2             25,579                           Freehold   \n",
       "\n",
       "   Postal District Floor Level  \n",
       "0               14           -  \n",
       "1                7           -  \n",
       "2               17           -  \n",
       "3               15           -  \n",
       "4               20           -  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"CommercialTransaction20250917124317.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4bfc939-feda-41c7-9cc1-8e16303d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_transaction_data(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Clean 'Transacted Price ($)' - remove commas and convert to float\n",
    "    df_clean['Transacted Price ($)'] = (\n",
    "        df_clean['Transacted Price ($)']\n",
    "        .str.replace(',', '')\n",
    "        .astype(float)\n",
    "    )\n",
    "    \n",
    "    # 2. Clean 'Area (SQFT)' and 'Area (SQM)' - ensure they're numeric\n",
    "    df_clean['Area (SQFT)'] = pd.to_numeric(df_clean['Area (SQFT)'], errors='coerce')\n",
    "    df_clean['Area (SQM)'] = pd.to_numeric(df_clean['Area (SQM)'], errors='coerce')\n",
    "    \n",
    "    # 3. Clean 'Unit Price ($ PSF)' and 'Unit Price ($ PSM)' - remove commas, convert to float\n",
    "    df_clean['Unit Price ($ PSF)'] = (\n",
    "        df_clean['Unit Price ($ PSF)']\n",
    "        .astype(str).str.replace(',', '')\n",
    "        .astype(float)\n",
    "    )\n",
    "    \n",
    "    df_clean['Unit Price ($ PSM)'] = (\n",
    "        df_clean['Unit Price ($ PSM)']\n",
    "        .astype(str).str.replace(',', '')\n",
    "        .astype(float)\n",
    "    )\n",
    "    \n",
    "    # 4. Convert 'Sale Date' to datetime (handling 'Sept-25' format)\n",
    "    df_clean['Sale Date'] = pd.to_datetime(df_clean['Sale Date'], format='%b-%y', errors='coerce')\n",
    "    \n",
    "\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply the cleaning function\n",
    "df = clean_transaction_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f814c-0a52-45fc-8feb-4705cac6ce4c",
   "metadata": {},
   "source": [
    "# Removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018bb4e7-7287-4f72-add4-8eb88408b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Project Name']=df['Project Name'].replace('N.A.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2ce7ff-0120-4446-9d50-b6a076ce3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts per row:\n",
      " 0       3\n",
      "1       2\n",
      "2       2\n",
      "3       2\n",
      "4       2\n",
      "       ..\n",
      "2949    0\n",
      "2950    0\n",
      "2951    0\n",
      "2952    1\n",
      "2953    0\n",
      "Length: 2954, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##remove rows with more than 3 missing values \n",
    "\n",
    "missing_counts = df.isnull().sum(axis=1)\n",
    "print(\"Missing counts per row:\\n\", missing_counts)\n",
    "\n",
    "missing_counts = df.isna().sum(axis=1)  # Count missing values per row\n",
    "df.drop(df[missing_counts >= 3].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8d669-eca1-44ce-90c7-df19d462fc80",
   "metadata": {},
   "source": [
    "## Target column by column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f134bc90-54f2-4cde-895a-da67acb56f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\AppData\\Local\\Temp\\ipykernel_29604\\6328051.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Project Name'].fillna(most_frequent_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "##project name\n",
    "\n",
    "df['Project Name'].unique()\n",
    "most_frequent_value =df['Project Name'].mode()[0]\n",
    "# Fill NaN values in the 'category' column with the most frequent value\n",
    "df['Project Name'].fillna(most_frequent_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49218c3a-7aba-4b2f-b98e-98cad65c7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "##area SQFT\n",
    "\n",
    "df['Area (SQFT)'].unique()\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['Area (SQFT)']] = imputer.fit_transform(df[['Area (SQFT)']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117468a4-6d08-4535-8f3d-561d5b5b433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##area SQM\n",
    "\n",
    "df['Area (SQM)'].unique()\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['Area (SQM)']] = imputer.fit_transform(df[['Area (SQM)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54796f8d-2b70-476c-bbcb-f6e178bb8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sale date\n",
    "\n",
    "df['Sale Date'].unique()\n",
    "\n",
    "df['sale_date_missing'] = df['Sale Date'].isna().astype(int)\n",
    "\n",
    "df['sale_year'] = df['Sale Date'].dt.year\n",
    "df['sale_month'] = df['Sale Date'].dt.month\n",
    "df['sale_quarter'] = df['Sale Date'].dt.quarter\n",
    "df['sale_dayofweek'] = df['Sale Date'].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "\n",
    "# continuous trend feature\n",
    "df['days_since_first_sale'] = (df['Sale Date'] - df['Sale Date'].min()).dt.days\n",
    "\n",
    "df.drop(columns=['Sale Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b097ea-4230-415a-8603-2aa4a0e9e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tenure\n",
    "df = df.dropna(subset=['Tenure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304aa73-056e-41d5-b5ed-adefcc400f59",
   "metadata": {},
   "source": [
    "## Floor level according to property type: shop hse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "290e333a-bb52-449f-8061-963b3e776391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Floor Level'].value_counts().get('-',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7ef88de-455a-467e-8e97-06b9ec567206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Floor Level'] =df['Floor Level'].replace('-', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c13906-d8db-48e6-afb1-6e5f1199368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'col1' is null:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Transacted Price ($)</th>\n",
       "      <th>Area (SQFT)</th>\n",
       "      <th>Unit Price ($ PSF)</th>\n",
       "      <th>Type of Area</th>\n",
       "      <th>Area (SQM)</th>\n",
       "      <th>Unit Price ($ PSM)</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Floor Level</th>\n",
       "      <th>sale_date_missing</th>\n",
       "      <th>sale_year</th>\n",
       "      <th>sale_month</th>\n",
       "      <th>sale_quarter</th>\n",
       "      <th>sale_dayofweek</th>\n",
       "      <th>days_since_first_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KAMPONG GLAM CONSERVATION AREA</td>\n",
       "      <td>BUSSORAH STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>6350000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>144.8</td>\n",
       "      <td>43854.0</td>\n",
       "      <td>99 yrs lease commencing from 2003</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>TOH AVENUE</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>158.3</td>\n",
       "      <td>12634.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>CEYLON ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>3850000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>2807.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>127.4</td>\n",
       "      <td>30220.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>CLOVER WAY</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>4200000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>164.2</td>\n",
       "      <td>25579.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>ROWELL ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>5270000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>5037.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>97.2</td>\n",
       "      <td>54218.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>BOAT QUAY CONSERVATION AREA</td>\n",
       "      <td>CIRCULAR ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>9500000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>9184.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>96.1</td>\n",
       "      <td>98855.0</td>\n",
       "      <td>999 yrs lease commencing from 1826</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>KRETA AYER CONSERVATION AREA</td>\n",
       "      <td>PAGODA STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>13300000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>10782.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>114.6</td>\n",
       "      <td>116056.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>TELOK AYER STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>16800000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>11570.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>134.9</td>\n",
       "      <td>124537.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>TELOK AYER STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>8550000.0</td>\n",
       "      <td>769.630000</td>\n",
       "      <td>11109.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>71.5</td>\n",
       "      <td>119580.0</td>\n",
       "      <td>999 yrs lease commencing from 1827</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>STANLEY STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>15500000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>10397.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>138.5</td>\n",
       "      <td>111913.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Project Name        Street Name Property Type  \\\n",
       "1     KAMPONG GLAM CONSERVATION AREA    BUSSORAH STREET    Shop House   \n",
       "2     LITTLE INDIA CONSERVATION AREA         TOH AVENUE    Shop House   \n",
       "3     LITTLE INDIA CONSERVATION AREA        CEYLON ROAD    Shop House   \n",
       "4     LITTLE INDIA CONSERVATION AREA         CLOVER WAY    Shop House   \n",
       "5     LITTLE INDIA CONSERVATION AREA        ROWELL ROAD    Shop House   \n",
       "...                              ...                ...           ...   \n",
       "2518     BOAT QUAY CONSERVATION AREA      CIRCULAR ROAD    Shop House   \n",
       "2519    KRETA AYER CONSERVATION AREA      PAGODA STREET    Shop House   \n",
       "2520    TELOK AYER CONSERVATION AREA  TELOK AYER STREET    Shop House   \n",
       "2521    TELOK AYER CONSERVATION AREA  TELOK AYER STREET    Shop House   \n",
       "2522    TELOK AYER CONSERVATION AREA     STANLEY STREET    Shop House   \n",
       "\n",
       "      Transacted Price ($)  Area (SQFT)  Unit Price ($ PSF) Type of Area  \\\n",
       "1                6350000.0   496.618271              4074.0         Land   \n",
       "2                2000000.0   496.618271              1174.0         Land   \n",
       "3                3850000.0   496.618271              2807.0         Land   \n",
       "4                4200000.0   496.618271              2376.0         Land   \n",
       "5                5270000.0   496.618271              5037.0         Land   \n",
       "...                    ...          ...                 ...          ...   \n",
       "2518             9500000.0   496.618271              9184.0         Land   \n",
       "2519            13300000.0   496.618271             10782.0         Land   \n",
       "2520            16800000.0   496.618271             11570.0         Land   \n",
       "2521             8550000.0   769.630000             11109.0         Land   \n",
       "2522            15500000.0   496.618271             10397.0         Land   \n",
       "\n",
       "      Area (SQM)  Unit Price ($ PSM)                              Tenure  \\\n",
       "1          144.8             43854.0   99 yrs lease commencing from 2003   \n",
       "2          158.3             12634.0                            Freehold   \n",
       "3          127.4             30220.0                            Freehold   \n",
       "4          164.2             25579.0                            Freehold   \n",
       "5           97.2             54218.0                            Freehold   \n",
       "...          ...                 ...                                 ...   \n",
       "2518        96.1             98855.0  999 yrs lease commencing from 1826   \n",
       "2519       114.6            116056.0                            Freehold   \n",
       "2520       134.9            124537.0                            Freehold   \n",
       "2521        71.5            119580.0  999 yrs lease commencing from 1827   \n",
       "2522       138.5            111913.0                            Freehold   \n",
       "\n",
       "      Postal District Floor Level  sale_date_missing  sale_year  sale_month  \\\n",
       "1                   7         NaN                  1        NaN         NaN   \n",
       "2                  17         NaN                  0     2025.0         8.0   \n",
       "3                  15         NaN                  0     2025.0         8.0   \n",
       "4                  20         NaN                  0     2025.0         8.0   \n",
       "5                   8         NaN                  0     2025.0         8.0   \n",
       "...               ...         ...                ...        ...         ...   \n",
       "2518                1         NaN                  0     2021.0        10.0   \n",
       "2519                1         NaN                  0     2021.0        10.0   \n",
       "2520                1         NaN                  1        NaN         NaN   \n",
       "2521                1         NaN                  1        NaN         NaN   \n",
       "2522                1         NaN                  1        NaN         NaN   \n",
       "\n",
       "      sale_quarter  sale_dayofweek  days_since_first_sale  \n",
       "1              NaN             NaN                    NaN  \n",
       "2              3.0             4.0                 1400.0  \n",
       "3              3.0             4.0                 1400.0  \n",
       "4              3.0             4.0                 1400.0  \n",
       "5              3.0             4.0                 1400.0  \n",
       "...            ...             ...                    ...  \n",
       "2518           4.0             4.0                    0.0  \n",
       "2519           4.0             4.0                    0.0  \n",
       "2520           NaN             NaN                    NaN  \n",
       "2521           NaN             NaN                    NaN  \n",
       "2522           NaN             NaN                    NaN  \n",
       "\n",
       "[523 rows x 18 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_col1_rows = df[df['Floor Level'].isnull() & (df['Property Type']=='Shop House')]\n",
    "print(\"Rows where 'col1' is null:\")\n",
    "null_col1_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f364b4-da35-4712-a5c3-87a0c183256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##shop hse\n",
    "df.loc[\n",
    "    df['Floor Level'].isnull() & (df['Property Type'] == 'Shop House'),\n",
    "    'Floor Level'\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d6fcbfb-1a3d-43ad-a964-30db917b77b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'col1' is null:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Transacted Price ($)</th>\n",
       "      <th>Area (SQFT)</th>\n",
       "      <th>Unit Price ($ PSF)</th>\n",
       "      <th>Type of Area</th>\n",
       "      <th>Area (SQM)</th>\n",
       "      <th>Unit Price ($ PSM)</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Floor Level</th>\n",
       "      <th>sale_date_missing</th>\n",
       "      <th>sale_year</th>\n",
       "      <th>sale_month</th>\n",
       "      <th>sale_quarter</th>\n",
       "      <th>sale_dayofweek</th>\n",
       "      <th>days_since_first_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KAMPONG GLAM CONSERVATION AREA</td>\n",
       "      <td>BUSSORAH STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>6350000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>144.8</td>\n",
       "      <td>43854.0</td>\n",
       "      <td>99 yrs lease commencing from 2003</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>TOH AVENUE</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>158.3</td>\n",
       "      <td>12634.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>CEYLON ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>3850000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>2807.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>127.4</td>\n",
       "      <td>30220.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>CLOVER WAY</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>4200000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>164.2</td>\n",
       "      <td>25579.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LITTLE INDIA CONSERVATION AREA</td>\n",
       "      <td>ROWELL ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>5270000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>5037.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>97.2</td>\n",
       "      <td>54218.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>BOAT QUAY CONSERVATION AREA</td>\n",
       "      <td>CIRCULAR ROAD</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>9500000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>9184.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>96.1</td>\n",
       "      <td>98855.0</td>\n",
       "      <td>999 yrs lease commencing from 1826</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>KRETA AYER CONSERVATION AREA</td>\n",
       "      <td>PAGODA STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>13300000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>10782.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>114.6</td>\n",
       "      <td>116056.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>TELOK AYER STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>16800000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>11570.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>134.9</td>\n",
       "      <td>124537.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>TELOK AYER STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>8550000.0</td>\n",
       "      <td>769.630000</td>\n",
       "      <td>11109.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>71.5</td>\n",
       "      <td>119580.0</td>\n",
       "      <td>999 yrs lease commencing from 1827</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>TELOK AYER CONSERVATION AREA</td>\n",
       "      <td>STANLEY STREET</td>\n",
       "      <td>Shop House</td>\n",
       "      <td>15500000.0</td>\n",
       "      <td>496.618271</td>\n",
       "      <td>10397.0</td>\n",
       "      <td>Land</td>\n",
       "      <td>138.5</td>\n",
       "      <td>111913.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Project Name        Street Name Property Type  \\\n",
       "1     KAMPONG GLAM CONSERVATION AREA    BUSSORAH STREET    Shop House   \n",
       "2     LITTLE INDIA CONSERVATION AREA         TOH AVENUE    Shop House   \n",
       "3     LITTLE INDIA CONSERVATION AREA        CEYLON ROAD    Shop House   \n",
       "4     LITTLE INDIA CONSERVATION AREA         CLOVER WAY    Shop House   \n",
       "5     LITTLE INDIA CONSERVATION AREA        ROWELL ROAD    Shop House   \n",
       "...                              ...                ...           ...   \n",
       "2518     BOAT QUAY CONSERVATION AREA      CIRCULAR ROAD    Shop House   \n",
       "2519    KRETA AYER CONSERVATION AREA      PAGODA STREET    Shop House   \n",
       "2520    TELOK AYER CONSERVATION AREA  TELOK AYER STREET    Shop House   \n",
       "2521    TELOK AYER CONSERVATION AREA  TELOK AYER STREET    Shop House   \n",
       "2522    TELOK AYER CONSERVATION AREA     STANLEY STREET    Shop House   \n",
       "\n",
       "      Transacted Price ($)  Area (SQFT)  Unit Price ($ PSF) Type of Area  \\\n",
       "1                6350000.0   496.618271              4074.0         Land   \n",
       "2                2000000.0   496.618271              1174.0         Land   \n",
       "3                3850000.0   496.618271              2807.0         Land   \n",
       "4                4200000.0   496.618271              2376.0         Land   \n",
       "5                5270000.0   496.618271              5037.0         Land   \n",
       "...                    ...          ...                 ...          ...   \n",
       "2518             9500000.0   496.618271              9184.0         Land   \n",
       "2519            13300000.0   496.618271             10782.0         Land   \n",
       "2520            16800000.0   496.618271             11570.0         Land   \n",
       "2521             8550000.0   769.630000             11109.0         Land   \n",
       "2522            15500000.0   496.618271             10397.0         Land   \n",
       "\n",
       "      Area (SQM)  Unit Price ($ PSM)                              Tenure  \\\n",
       "1          144.8             43854.0   99 yrs lease commencing from 2003   \n",
       "2          158.3             12634.0                            Freehold   \n",
       "3          127.4             30220.0                            Freehold   \n",
       "4          164.2             25579.0                            Freehold   \n",
       "5           97.2             54218.0                            Freehold   \n",
       "...          ...                 ...                                 ...   \n",
       "2518        96.1             98855.0  999 yrs lease commencing from 1826   \n",
       "2519       114.6            116056.0                            Freehold   \n",
       "2520       134.9            124537.0                            Freehold   \n",
       "2521        71.5            119580.0  999 yrs lease commencing from 1827   \n",
       "2522       138.5            111913.0                            Freehold   \n",
       "\n",
       "      Postal District Floor Level  sale_date_missing  sale_year  sale_month  \\\n",
       "1                   7           0                  1        NaN         NaN   \n",
       "2                  17           0                  0     2025.0         8.0   \n",
       "3                  15           0                  0     2025.0         8.0   \n",
       "4                  20           0                  0     2025.0         8.0   \n",
       "5                   8           0                  0     2025.0         8.0   \n",
       "...               ...         ...                ...        ...         ...   \n",
       "2518                1           0                  0     2021.0        10.0   \n",
       "2519                1           0                  0     2021.0        10.0   \n",
       "2520                1           0                  1        NaN         NaN   \n",
       "2521                1           0                  1        NaN         NaN   \n",
       "2522                1           0                  1        NaN         NaN   \n",
       "\n",
       "      sale_quarter  sale_dayofweek  days_since_first_sale  \n",
       "1              NaN             NaN                    NaN  \n",
       "2              3.0             4.0                 1400.0  \n",
       "3              3.0             4.0                 1400.0  \n",
       "4              3.0             4.0                 1400.0  \n",
       "5              3.0             4.0                 1400.0  \n",
       "...            ...             ...                    ...  \n",
       "2518           4.0             4.0                    0.0  \n",
       "2519           4.0             4.0                    0.0  \n",
       "2520           NaN             NaN                    NaN  \n",
       "2521           NaN             NaN                    NaN  \n",
       "2522           NaN             NaN                    NaN  \n",
       "\n",
       "[526 rows x 18 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ull_col1_rows = df[df['Floor Level'].notna() & (df['Property Type']=='Shop House')]\n",
    "print(\"Rows where 'col1' is null:\")\n",
    "ull_col1_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2b4a2-811b-4571-a828-e9fe2d993fbb",
   "metadata": {},
   "source": [
    "## fill in null values for floor level using imputation based off other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ae6c467-422f-4c60-bc19-e7c1fdad1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_floor_imputation(row, df):\n",
    "    if pd.notna(row['Floor Level']):\n",
    "        return row['Floor Level']\n",
    "    \n",
    "    # Try different levels of similarity\n",
    "    similar_criteria = [\n",
    "        # Level 1: Same Property Type + Postal District + Area Bin\n",
    "        {'prop_type': row['Property Type'], 'postal_district': row['Postal District'], 'area_bin': row.get('Area_Bin', 'Unknown')},\n",
    "        # Level 2: Same Property Type + Postal District\n",
    "        {'prop_type': row['Property Type'], 'postal_district': row['Postal District']},\n",
    "        # Level 3: Same Property Type only\n",
    "        {'prop_type': row['Property Type']},\n",
    "        # Level 4: Global most common\n",
    "        {'global': True}\n",
    "    ]\n",
    "    \n",
    "    for criteria in similar_criteria:\n",
    "        if 'global' in criteria:\n",
    "            # Global most common floor level\n",
    "            most_common = df[df['Floor Level'].notna()]['Floor Level'].mode()\n",
    "            if not most_common.empty:\n",
    "                return most_common.iloc[0]\n",
    "        \n",
    "        elif 'area_bin' in criteria:\n",
    "            # Check if Area_Bin column exists, if not skip this level\n",
    "            if 'Area_Bin' not in df.columns:\n",
    "                continue\n",
    "            mask = (\n",
    "                (df['Property Type'] == criteria['prop_type']) &\n",
    "                (df['Postal District'] == criteria['postal_district']) &\n",
    "                (df['Area_Bin'] == criteria['area_bin']) &\n",
    "                (df['Floor Level'].notna())\n",
    "            )\n",
    "        elif 'postal_district' in criteria:\n",
    "            mask = (\n",
    "                (df['Property Type'] == criteria['prop_type']) &\n",
    "                (df['Postal District'] == criteria['postal_district']) &\n",
    "                (df['Floor Level'].notna())\n",
    "            )\n",
    "        else:\n",
    "            # Only property type\n",
    "            mask = (\n",
    "                (df['Property Type'] == criteria['prop_type']) &\n",
    "                (df['Floor Level'].notna())\n",
    "            )\n",
    "        \n",
    "        similar_floors = df[mask]['Floor Level']\n",
    "        if len(similar_floors) > 0:\n",
    "            return similar_floors.mode().iloc[0]\n",
    "    \n",
    "    # Final fallback\n",
    "    return '01 to 05'  # Most common default\n",
    "\n",
    "# Apply smart imputation\n",
    "df['Floor Level'] = df.apply(\n",
    "    lambda row: smart_floor_imputation(row, df), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb8e37-3702-4e28-b060-dc40ea2d3ae3",
   "metadata": {},
   "source": [
    "### Binning for floor levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ced0819d-cca3-4858-a87d-defd0c7f8b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Set for Modeling:\n",
      "   Floor Level  Floor_Low  Floor_High  Floor_Midpoint  Is_Basement  Is_Ground  \\\n",
      "1            0          0           0             0.0            0          1   \n",
      "2            0          0           0             0.0            0          1   \n",
      "3            0          0           0             0.0            0          1   \n",
      "4            0          0           0             0.0            0          1   \n",
      "5            0          0           0             0.0            0          1   \n",
      "6            0          0           0             0.0            0          1   \n",
      "7            0          0           0             0.0            0          1   \n",
      "8            0          0           0             0.0            0          1   \n",
      "9            0          0           0             0.0            0          1   \n",
      "10           0          0           0             0.0            0          1   \n",
      "\n",
      "   Floor_Category_ML  \n",
      "1         shop_house  \n",
      "2         shop_house  \n",
      "3         shop_house  \n",
      "4         shop_house  \n",
      "5         shop_house  \n",
      "6         shop_house  \n",
      "7         shop_house  \n",
      "8         shop_house  \n",
      "9         shop_house  \n",
      "10        shop_house  \n",
      "\n",
      "Floor Category Distribution:\n",
      "Floor_Category_ML\n",
      "basement         100\n",
      "floors_01_05    1393\n",
      "floors_06_10     458\n",
      "floors_11_15     206\n",
      "floors_16_20     118\n",
      "floors_21_25      63\n",
      "floors_26_30      28\n",
      "floors_31_35      20\n",
      "floors_36_40      13\n",
      "floors_41_45       2\n",
      "shop_house       523\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create both numerical and categorical features\n",
    "def extract_floor_features(floor_str):\n",
    "    \"\"\"\n",
    "    Extract multiple numerical features from floor ranges\n",
    "    \"\"\"\n",
    "    if pd.isna(floor_str) or floor_str == '-' or floor_str == '0':\n",
    "        return np.nan, np.nan, np.nan, 0, 0  # Added is_ground flag\n",
    "    \n",
    "    floor_str = str(floor_str).strip()\n",
    "    \n",
    "    # Handle ground floor (0)\n",
    "    if floor_str == '0':\n",
    "        return 0, 0, 0, 0, 1  # low, high, midpoint, is_basement, is_ground\n",
    "    \n",
    "    numbers = re.findall(r'\\d+', floor_str)\n",
    "    \n",
    "    if not numbers:\n",
    "        return np.nan, np.nan, np.nan, 0, 0\n",
    "    \n",
    "    is_basement = 1 if floor_str.startswith('B') else 0\n",
    "    is_ground = 0  # Default not ground floor\n",
    "    \n",
    "    if len(numbers) >= 2:\n",
    "        low = int(numbers[0])\n",
    "        high = int(numbers[1])\n",
    "        return low, high, (low + high) / 2, is_basement, is_ground\n",
    "    else:\n",
    "        num = int(numbers[0])\n",
    "        return num, num, num, is_basement, is_ground\n",
    "\n",
    "# Apply numerical feature extraction\n",
    "floor_features = df['Floor Level'].apply(extract_floor_features)\n",
    "df['Floor_Low'] = [x[0] for x in floor_features]\n",
    "df['Floor_High'] = [x[1] for x in floor_features]\n",
    "df['Floor_Midpoint'] = [x[2] for x in floor_features]\n",
    "df['Is_Basement'] = [x[3] for x in floor_features]\n",
    "df['Is_Ground'] = [x[4] for x in floor_features]  # New column\n",
    "\n",
    "# Create ML-ready categories (updated to handle ground floor)\n",
    "def create_ml_categories(floor_str):\n",
    "    if pd.isna(floor_str) or floor_str == '-':\n",
    "        return 'unknown'\n",
    "    \n",
    "    floor_str = str(floor_str).strip()\n",
    "    \n",
    "    # Handle ground floor\n",
    "    if floor_str == '0':\n",
    "        return 'shop_house'\n",
    "    \n",
    "    # Handle basement\n",
    "    if floor_str.startswith('B'):\n",
    "        return 'basement'\n",
    "    \n",
    "    numbers = re.findall(r'\\d+', floor_str)\n",
    "    if len(numbers) >= 2:\n",
    "        low = int(numbers[0])\n",
    "        if low <= 5: return 'floors_01_05'\n",
    "        elif low <= 10: return 'floors_06_10'\n",
    "        elif low <= 15: return 'floors_11_15'\n",
    "        elif low <= 20: return 'floors_16_20'\n",
    "        elif low <= 25: return 'floors_21_25'\n",
    "        elif low <= 30: return 'floors_26_30'\n",
    "        elif low <= 35: return 'floors_31_35'\n",
    "        elif low <= 40: return 'floors_36_40'\n",
    "        else: return 'floors_41_45'\n",
    "    \n",
    "    # Handle single numbers (if any)\n",
    "    if len(numbers) == 1:\n",
    "        num = int(numbers[0])\n",
    "        if num == 0:\n",
    "            return 'ground_floor'\n",
    "        elif num <= 5: return 'floors_01_05'\n",
    "        elif num <= 10: return 'floors_06_10'\n",
    "        elif num <= 15: return 'floors_11_15'\n",
    "        elif num <= 20: return 'floors_16_20'\n",
    "        elif num <= 25: return 'floors_21_25'\n",
    "        elif num <= 30: return 'floors_26_30'\n",
    "        elif num <= 35: return 'floors_31_35'\n",
    "        elif num <= 40: return 'floors_36_40'\n",
    "        else: return 'floors_41_45'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "df['Floor_Category_ML'] = df['Floor Level'].apply(create_ml_categories)\n",
    "\n",
    "print(\"Final Feature Set for Modeling:\")\n",
    "print(df[['Floor Level', 'Floor_Low', 'Floor_High', 'Floor_Midpoint', 'Is_Basement', 'Is_Ground', 'Floor_Category_ML']].head(10))\n",
    "\n",
    "# Check the distribution of the new categories\n",
    "print(\"\\nFloor Category Distribution:\")\n",
    "print(df['Floor_Category_ML'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3d68a3-cf39-4c8f-864d-5889a4fc11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Floor Level']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb1e0e-599d-48f5-ae22-c9d65ad273fb",
   "metadata": {},
   "source": [
    "## load geo features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c99b251a-004f-43bb-ba89-3f0c3300de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Main transactions: (2924, 23)\n",
      "Postal codes: (121154, 4)\n",
      "City coordinates: (332, 9)\n",
      "Street coordinates: (589, 3)\n",
      "Train stations: (209, 4)\n",
      "Postal district mapping: (28, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load your main transaction dataset\n",
    "transactions_df = df\n",
    "# Load geographic datasets\n",
    "postal_codes = pd.read_csv('SG_postal.csv')\n",
    "city_coordinates = pd.read_csv('singapore_city_coordinates_improved.csv')\n",
    "street_coordinates = pd.read_csv('street_coordinates.csv')\n",
    "train_stations = pd.read_csv('mrt_lrt_data.csv')\n",
    "postal_district_mapping = pd.read_csv('sg_postal_districts.csv')\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Main transactions: {transactions_df.shape}\")\n",
    "print(f\"Postal codes: {postal_codes.shape}\")\n",
    "print(f\"City coordinates: {city_coordinates.shape}\")\n",
    "print(f\"Street coordinates: {street_coordinates.shape}\")\n",
    "print(f\"Train stations: {train_stations.shape}\")\n",
    "print(f\"Postal district mapping: {postal_district_mapping.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90c972e2-e4bc-4bbb-a55a-683b3218a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts per row:\n",
      " 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "327    0\n",
      "328    0\n",
      "329    0\n",
      "330    6\n",
      "331    0\n",
      "Length: 332, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = city_coordinates.isnull().sum(axis=1)\n",
    "print(\"Missing counts per row:\\n\", missing_counts)\n",
    "\n",
    "missing_counts = city_coordinates.isna().sum(axis=1)  # Count missing values per row\n",
    "city_coordinates.drop(city_coordinates[missing_counts >= 4].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4dc97e0-13c0-4b6c-8623-e17e70a32f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding geographic features with enhanced coverage...\n",
      "Starting geographic feature engineering for 2924 properties...\n",
      " Strategy 1A: Street name matching → 1196 properties\n",
      "Strategy 1B: Postal district centroids → 1728 properties\n",
      "FINAL COORDINATE COVERAGE: 1655/2924 properties (56.6%)\n",
      " Added general location for 2924 properties\n",
      "Calculating MRT distances for 1655 properties...\n",
      " Added MRT distances for 1655 properties\n",
      " Added district-average MRT distances for 1269 properties\n",
      " Added region classification for 2924 properties\n",
      "Calculating CBD distances...\n",
      " Added CBD distances for 1655 properties\n",
      " Added urban classification\n",
      "\n",
      "============================================================\n",
      " GEOGRAPHIC FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "FINAL COVERAGE REPORT:\n",
      "   • Coordinates: 1655/2924\n",
      "   • MRT Distances: 2924/2924\n",
      "   • Region Classification: 2924/2924\n",
      "   • General Location: 2924/2924\n",
      "   • CBD Distances: 1655/2924\n"
     ]
    }
   ],
   "source": [
    "def add_geographic_features(main_df, postal_df, street_df, city_df, stations_df, district_df):\n",
    "    \"\"\"Enrich industrial data with geographic and proximity features - FIXED VERSION\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from rapidfuzz import process\n",
    "\n",
    "    df_enriched = main_df.copy()\n",
    "    original_count = len(df_enriched)\n",
    "    \n",
    "    print(f\"Starting geographic feature engineering for {original_count} properties...\")\n",
    "\n",
    "    # --- 1. MULTI-STRATEGY COORDINATE MATCHING ---\n",
    "    \n",
    "    # Strategy 1A: Direct street name matching (your current approach)\n",
    "    if 'Street Name' in df_enriched.columns:\n",
    "        street_df['street_name_clean'] = street_df['street_name'].str.upper().str.strip()\n",
    "        df_enriched['Street_Name_Clean'] = df_enriched['Street Name'].str.upper().str.strip()\n",
    "\n",
    "        street_choices = street_df['street_name_clean'].unique().tolist()\n",
    "\n",
    "        def match_street(name):\n",
    "            if pd.isna(name): return None\n",
    "            match = process.extractOne(name, street_choices, score_cutoff=80)  # Lowered threshold\n",
    "            return match[0] if match else None\n",
    "\n",
    "        df_enriched['Matched_Street'] = df_enriched['Street_Name_Clean'].apply(match_street)\n",
    "\n",
    "        df_enriched = df_enriched.merge(\n",
    "            street_df[['street_name_clean', 'latitude', 'longitude']],\n",
    "            left_on='Matched_Street', right_on='street_name_clean', how='left'\n",
    "        )\n",
    "        street_matches = df_enriched['latitude'].notna().sum()\n",
    "        print(f\" Strategy 1A: Street name matching → {street_matches} properties\")\n",
    "\n",
    "    # Strategy 1B: Postal code matching for missing coordinates\n",
    "    if 'Postal District' in df_enriched.columns and 'postal_code' in postal_df.columns:\n",
    "        # Convert postal codes to district (first 2 digits for Singapore)\n",
    "        postal_df['Postal_District'] = postal_df['postal_code'].astype(str).str[:2]\n",
    "        \n",
    "        # Get centroid coordinates for each postal district\n",
    "        district_coords = postal_df.groupby('Postal_District').agg({\n",
    "            'lat': 'mean',\n",
    "            'lon': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Merge district centroids for properties missing coordinates\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            df_enriched.loc[missing_coords_mask, 'Postal_District_Str'] = df_enriched.loc[missing_coords_mask, 'Postal District'].apply(\n",
    "                lambda x: str(int(x)) if pd.notna(x) else np.nan\n",
    "            )\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                district_coords.rename(columns={'lat': 'lat_district', 'lon': 'lon_district'}),\n",
    "                left_on='Postal_District_Str', right_on='Postal_District', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with district centroids\n",
    "            district_fill_mask = df_enriched['latitude'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            df_enriched.loc[district_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lat_district']\n",
    "            df_enriched.loc[district_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lon_district']\n",
    "            \n",
    "            district_matches = district_fill_mask.sum()\n",
    "            print(f\"Strategy 1B: Postal district centroids → {district_matches} properties\")\n",
    "\n",
    "    # Strategy 1C: Planning Area matching from city coordinates\n",
    "    if 'Planning Area' in df_enriched.columns and 'Place' in city_df.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            city_df['Place_Clean'] = city_df['Place'].str.upper().str.strip()\n",
    "            df_enriched['Planning_Area_Clean'] = df_enriched['Planning Area'].str.upper().str.strip()\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                city_df[['Place_Clean', 'latitude', 'longitude']].rename(\n",
    "                    columns={'latitude': 'lat_city', 'longitude': 'lon_city'}\n",
    "                ),\n",
    "                left_on='Planning_Area_Clean', right_on='Place_Clean', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with city coordinates\n",
    "            city_fill_mask = df_enriched['latitude'].isna() & df_enriched['Planning_Area_Clean'].notna()\n",
    "            df_enriched.loc[city_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lat_city']\n",
    "            df_enriched.loc[city_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lon_city']\n",
    "            \n",
    "            city_matches = city_fill_mask.sum()\n",
    "            print(f\" Strategy 1C: Planning Area matching → {city_matches} properties\")\n",
    "\n",
    "    # Strategy 1D: Region-based fallback coordinates\n",
    "    if 'Region' in df_enriched.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            # Define approximate coordinates for major regions\n",
    "            region_coords = {\n",
    "                'CENTRAL REGION': (1.2923, 103.8536),  # Singapore central\n",
    "                'EAST REGION': (1.3443, 103.9645),     # East area\n",
    "                'WEST REGION': (1.3526, 103.7584),     # West area\n",
    "                'NORTH REGION': (1.4180, 103.8200),    # North area\n",
    "                'NORTH-EAST REGION': (1.3691, 103.8975) # North-East\n",
    "            }\n",
    "            \n",
    "            def get_region_coords(region):\n",
    "                if pd.isna(region): return (np.nan, np.nan)\n",
    "                region_upper = str(region).upper()\n",
    "                for key, coords in region_coords.items():\n",
    "                    if key in region_upper:\n",
    "                        return coords\n",
    "                return (np.nan, np.nan)\n",
    "            \n",
    "            region_coords_df = df_enriched[missing_coords_mask]['Region'].apply(get_region_coords)\n",
    "            region_fill_mask = df_enriched['latitude'].isna() & df_enriched['Region'].notna()\n",
    "            \n",
    "            df_enriched.loc[region_fill_mask, 'latitude'] = region_coords_df.apply(lambda x: x[0])\n",
    "            df_enriched.loc[region_fill_mask, 'longitude'] = region_coords_df.apply(lambda x: x[1])\n",
    "            \n",
    "            region_matches = region_fill_mask.sum()\n",
    "            print(f\" Strategy 1D: Region-based fallback → {region_matches} properties\")\n",
    "\n",
    "    # Final coordinate coverage report\n",
    "    final_coverage = df_enriched['latitude'].notna().sum()\n",
    "    print(f\"FINAL COORDINATE COVERAGE: {final_coverage}/{original_count} properties ({final_coverage/original_count*100:.1f}%)\")\n",
    "\n",
    "    # --- 2. ENHANCED POSTAL DISTRICT FEATURES ---\n",
    "    \n",
    "    if 'Postal District' in df_enriched.columns:\n",
    "        # Create proper postal district string (handle NaNs)\n",
    "        df_enriched['Postal_District_Str'] = df_enriched['Postal District'].apply(\n",
    "            lambda x: str(int(x)) if pd.notna(x) else 'Unknown'\n",
    "        )\n",
    "        \n",
    "        district_df['Postal District'] = district_df['Postal District'].astype(str)\n",
    "        \n",
    "        df_enriched = df_enriched.merge(\n",
    "            district_df[['Postal District', 'General Location']].rename(columns={'General Location': 'General_Location'}),\n",
    "            left_on='Postal_District_Str', right_on='Postal District', how='left'\n",
    "        )\n",
    "        print(f\" Added general location for {df_enriched['General_Location'].notna().sum()} properties\")\n",
    "\n",
    "    # --- 3. FIXED MRT DISTANCE CALCULATION ---\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate Haversine distance between two points in kilometers\"\"\"\n",
    "        R = 6371  # Earth radius in kilometers\n",
    "        \n",
    "        lat1_rad = np.radians(lat1)\n",
    "        lon1_rad = np.radians(lon1)\n",
    "        lat2_rad = np.radians(lat2)\n",
    "        lon2_rad = np.radians(lon2)\n",
    "        \n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        \n",
    "        return R * c\n",
    "\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        station_coords = stations_df[['Latitude', 'Longitude']].dropna()\n",
    "        \n",
    "        if len(valid_coords) > 0 and len(station_coords) > 0:\n",
    "            print(f\"Calculating MRT distances for {len(valid_coords)} properties...\")\n",
    "            \n",
    "            # Vectorized distance calculation\n",
    "            min_dists = []\n",
    "            for idx, prop_row in valid_coords.iterrows():\n",
    "                prop_lat, prop_lon = prop_row['latitude'], prop_row['longitude']\n",
    "                \n",
    "                # Calculate distances to all stations\n",
    "                distances = []\n",
    "                for _, station_row in station_coords.iterrows():\n",
    "                    dist = haversine_distance(\n",
    "                        prop_lat, prop_lon,\n",
    "                        station_row['Latitude'], station_row['Longitude']\n",
    "                    )\n",
    "                    distances.append(dist)\n",
    "                \n",
    "                min_dists.append(min(distances) if distances else np.nan)\n",
    "            \n",
    "            # Assign distances back to dataframe\n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_MRT_km'] = min_dists\n",
    "            print(f\" Added MRT distances for {len([x for x in min_dists if not np.isnan(x)])} properties\")\n",
    "            \n",
    "            # For properties without coordinates, use district average\n",
    "            missing_mrt_mask = df_enriched['Distance_to_MRT_km'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            if missing_mrt_mask.any():\n",
    "                district_mrt_avg = df_enriched.groupby('Postal_District_Str')['Distance_to_MRT_km'].mean()\n",
    "                df_enriched.loc[missing_mrt_mask, 'Distance_to_MRT_km'] = df_enriched.loc[missing_mrt_mask, 'Postal_District_Str'].map(district_mrt_avg)\n",
    "                print(f\" Added district-average MRT distances for {missing_mrt_mask.sum()} properties\")\n",
    "\n",
    "    # --- 4. ENHANCED REGION CLASSIFICATION ---\n",
    "    \n",
    "    def classify_region(d):\n",
    "        if pd.isna(d) or d == 'Unknown': return 'Unknown'\n",
    "        try:\n",
    "            d_int = int(d)\n",
    "            if d_int <= 9: return 'Central Core'\n",
    "            elif d_int <= 16: return 'Rest Central'\n",
    "            elif d_int <= 21: return 'City Fringe'\n",
    "            elif d_int <= 28: return 'Outside Central'\n",
    "            else: return 'Unknown'\n",
    "        except:\n",
    "            return 'Unknown'\n",
    "\n",
    "    df_enriched['Region_Classification'] = df_enriched['Postal_District_Str'].apply(classify_region)\n",
    "    region_coverage = (df_enriched['Region_Classification'] != 'Unknown').sum()\n",
    "    print(f\" Added region classification for {region_coverage} properties\")\n",
    "\n",
    "    # --- 5. ADDITIONAL GEOGRAPHIC FEATURES ---\n",
    "    \n",
    "    # CBD proximity (distance to Raffles Place)\n",
    "    cbd_coords = (1.2833, 103.8515)  # Raffles Place\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        if len(valid_coords) > 0:\n",
    "            print(\"Calculating CBD distances...\")\n",
    "            cbd_distances = []\n",
    "            for idx, row in valid_coords.iterrows():\n",
    "                dist = haversine_distance(\n",
    "                    row['latitude'], row['longitude'],\n",
    "                    cbd_coords[0], cbd_coords[1]\n",
    "                )\n",
    "                cbd_distances.append(dist)\n",
    "            \n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_CBD_km'] = cbd_distances\n",
    "            print(f\" Added CBD distances for {len(cbd_distances)} properties\")\n",
    "\n",
    "    # Urban vs Suburban classification\n",
    "    def classify_urban_rural(distance_to_cbd):\n",
    "        if pd.isna(distance_to_cbd): return 'Unknown'\n",
    "        if distance_to_cbd <= 5: return 'CBD'\n",
    "        elif distance_to_cbd <= 10: return 'Urban'\n",
    "        elif distance_to_cbd <= 20: return 'Suburban'\n",
    "        else: return 'Rural'\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        df_enriched['Urban_Classification'] = df_enriched['Distance_to_CBD_km'].apply(classify_urban_rural)\n",
    "        print(f\" Added urban classification\")\n",
    "\n",
    "    # --- FINAL REPORT ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" GEOGRAPHIC FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"FINAL COVERAGE REPORT:\")\n",
    "    print(f\"   • Coordinates: {df_enriched['latitude'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   • MRT Distances: {df_enriched['Distance_to_MRT_km'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   • Region Classification: {(df_enriched['Region_Classification'] != 'Unknown').sum()}/{original_count}\")\n",
    "    print(f\"   • General Location: {df_enriched['General_Location'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        print(f\"   • CBD Distances: {df_enriched['Distance_to_CBD_km'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = [col for col in df_enriched.columns if col in ['Matched_Street', 'street_name_clean', 'Postal_District_Str', 'Postal District_y']]\n",
    "    df_enriched = df_enriched.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "# Call the improved function\n",
    "print(\"\\nAdding geographic features with enhanced coverage...\")\n",
    "commercial_enriched = add_geographic_features(\n",
    "    transactions_df, \n",
    "    postal_codes, \n",
    "    street_coordinates, \n",
    "    city_coordinates, \n",
    "    train_stations, \n",
    "    postal_district_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84582ee3-e12e-4654-aa3a-3a9c5fbde9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project Name                0\n",
       "Street Name                 0\n",
       "Property Type               0\n",
       "Transacted Price ($)        0\n",
       "Area (SQFT)                 0\n",
       "Unit Price ($ PSF)          0\n",
       "Type of Area                0\n",
       "Area (SQM)                  0\n",
       "Unit Price ($ PSM)          0\n",
       "Tenure                      0\n",
       "Postal District_x           0\n",
       "sale_date_missing           0\n",
       "sale_year                 210\n",
       "sale_month                210\n",
       "sale_quarter              210\n",
       "sale_dayofweek            210\n",
       "days_since_first_sale     210\n",
       "Floor_Low                   0\n",
       "Floor_High                  0\n",
       "Floor_Midpoint              0\n",
       "Is_Basement                 0\n",
       "Is_Ground                   0\n",
       "Floor_Category_ML           0\n",
       "Street_Name_Clean           0\n",
       "latitude                 1269\n",
       "longitude                1269\n",
       "General_Location            0\n",
       "Distance_to_MRT_km          0\n",
       "Region_Classification       0\n",
       "Distance_to_CBD_km       1269\n",
       "Urban_Classification        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commercial_enriched.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62219fcf-5c13-4dbd-be4c-6a29cd54b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATING PROPERTY-TYPE SPECIFIC MODELS\n",
      "==================================================\n",
      "\n",
      " PROCESSING SHOP HOUSE...\n",
      "   Samples: 526\n",
      "   Features: 28 total, 10 categorical, 18 numerical\n",
      "\n",
      " PROCESSING RETAIL...\n",
      "   Samples: 1050\n",
      "   Features: 28 total, 10 categorical, 18 numerical\n",
      "\n",
      " PROCESSING OFFICE...\n",
      "   Samples: 1348\n",
      "   Features: 28 total, 10 categorical, 18 numerical\n"
     ]
    }
   ],
   "source": [
    "target_column = 'Unit Price ($ PSF)'\n",
    "\n",
    "exclude_columns = ['Transacted Price ($)', 'Unit Price ($ PSM)',\n",
    "    'Unit Price ($ PSF)',\n",
    "    'monthly_rental_price_yield',\n",
    "    'rental_rate_psm_yield',\n",
    "    'monthly_rental_price_tenure',\n",
    "    'rental_rate_psm_tenure',\n",
    "    'monthly_rental_price_market',\n",
    "    'rental_rate_psm_avg',\n",
    "    'monthly_rental_price_avg',\n",
    "    'market_rent_rate_psm',\n",
    "    'annual_rental_income_yield','implied_yield_market']\n",
    "\n",
    "print(\" CREATING PROPERTY-TYPE SPECIFIC MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "property_type_models = {}\n",
    "\n",
    "for prop_type in commercial_enriched['Property Type'].unique():\n",
    "    print(f\"\\n PROCESSING {prop_type.upper()}...\")\n",
    "    \n",
    "    # Filter data for this property type\n",
    "    type_mask = commercial_enriched['Property Type'] == prop_type\n",
    "    type_data = commercial_enriched[type_mask]\n",
    "    \n",
    "    # Skip if not enough samples\n",
    "    if len(type_data) < 30:\n",
    "        print(f\"   ⚠ Skipped - only {len(type_data)} samples (need at least 30)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   Samples: {len(type_data)}\")\n",
    "    \n",
    "    # Use YOUR EXACT feature selection logic\n",
    "    feature_columns = [col for col in type_data.columns \n",
    "                      if col not in exclude_columns and col != target_column]\n",
    "    \n",
    "    # Use YOUR EXACT preprocessing\n",
    "    categorical_columns = type_data[feature_columns].select_dtypes(include=['object', 'category']).columns\n",
    "    numerical_columns = type_data[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    print(f\"   Features: {len(feature_columns)} total, {len(categorical_columns)} categorical, {len(numerical_columns)} numerical\")\n",
    "    \n",
    "    # One-hot encode using YOUR method\n",
    "    X_encoded = pd.get_dummies(type_data[feature_columns], columns=categorical_columns, drop_first=True)\n",
    "    y = type_data[target_column]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_type, X_test_type, y_train_type, y_test_type = train_test_split(\n",
    "        X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e5c6e89-b73a-4155-99f5-0855073dd18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1078, 447)\n",
      "Testing set: (270, 447)\n",
      "\n",
      "Baseline Model (Mean Prediction):\n",
      "MAE: $884.26\n",
      "RMSE: $1,495.46\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "# Create a simple baseline (predict mean)\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "\n",
    "print(f\"\\nBaseline Model (Mean Prediction):\")\n",
    "print(f\"MAE: ${baseline_mae:,.2f}\")\n",
    "print(f\"RMSE: ${baseline_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e102bf3-93ab-4c16-95cc-2c53f9853478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      "2393\n",
      "Missing values in target:\n",
      "0\n",
      "Missing values after imputation - Train: 0\n",
      "Missing values after imputation - Test: 0\n",
      "Training set: (1078, 447)\n",
      "Test set: (270, 447)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in features:\")\n",
    "print(X_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in target:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Handle missing values - FIXED VERSION\n",
    "def handle_missing_values(X, strategy='mean'):\n",
    "    \"\"\"Handle missing values in the feature matrix\"\"\"\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns, index=X.index), imputer\n",
    "\n",
    "# Apply to training and test data\n",
    "X_train_imputed, imputer = handle_missing_values(X_train)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"Missing values after imputation - Train: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after imputation - Test: {X_test_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Training set: {X_train_imputed.shape}\")\n",
    "print(f\"Test set: {X_test_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c274895-e19c-47e5-baa7-a4a6d82a64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n",
      "XGBoost Results:\n",
      "  MAE: $324.66\n",
      "  RMSE: $1,075.44\n",
      "  R²: 0.4816\n",
      "  Improvement over baseline: 63.3%\n",
      "\n",
      " Best Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "\n",
    "# Updated models with proper data handling\n",
    "models = {\n",
    "\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        random_state=42,\n",
    "        enable_categorical=False  # Ensure this is False for one-hot encoded data\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models with proper data splits\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        if name == 'Hist Gradient Boosting':\n",
    "            # Use imputed data for HistGradientBoosting\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "            \n",
    "        else:\n",
    "            # Use imputed data for other models\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test_compare, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_compare, y_pred))\n",
    "        r2 = r2_score(y_test_compare, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'predictions': y_pred,\n",
    "            'y_test': y_test_compare\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  MAE: ${mae:,.2f}\")\n",
    "        print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  Improvement over baseline: {((baseline_mae - mae) / baseline_mae * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Find best model\n",
    "if results:\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mae'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\n Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38e94f70-ea86-43ac-86f9-13e6f0d61477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced training pipeline...\n",
      "Starting Enhanced Model Training Pipeline\n",
      "============================================================\n",
      "Training enhanced models with improved configurations...\n",
      "============================================================\n",
      "\n",
      "--- Training Hist Gradient Boosting ---\n",
      "Test MAE: $621.84\n",
      "Test RMSE: $1,294.54\n",
      "Test R²: 0.2488\n",
      "CV MAE: $754.46 ± $146.40\n",
      "Improvement over baseline: +29.7%\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $304.62\n",
      "Test RMSE: $882.43\n",
      "Test R²: 0.6510\n",
      "CV MAE: $625.45 ± $176.00\n",
      "Improvement over baseline: +65.6%\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $338.74\n",
      "Test RMSE: $985.08\n",
      "Test R²: 0.5650\n",
      "CV MAE: $574.69 ± $201.13\n",
      "Improvement over baseline: +61.7%\n",
      "\n",
      "--- Training CatBoost ---\n",
      "Test MAE: $353.12\n",
      "Test RMSE: $854.73\n",
      "Test R²: 0.6725\n",
      "CV MAE: $565.67 ± $158.91\n",
      "Improvement over baseline: +60.1%\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Model Ranking (by MAE):\n",
      " 1. XGBoost                   | MAE: $304.62 | R²: 0.6510 | CV: $625.45 ± $176.00\n",
      " 2. Gradient Boosting         | MAE: $338.74 | R²: 0.5650 | CV: $574.69 ± $201.13\n",
      " 3. CatBoost                  | MAE: $353.12 | R²: 0.6725 | CV: $565.67 ± $158.91\n",
      " 4. Hist Gradient Boosting    | MAE: $621.84 | R²: 0.2488 | CV: $754.46 ± $146.40\n",
      "\n",
      "--- Error Analysis for XGBoost ---\n",
      "Mean Absolute Error: $304.62\n",
      "Median Absolute Error: $117.01\n",
      "Max Error: $11,012.55\n",
      "Error Std: $829.72\n",
      "\n",
      "Error Percentage Stats:\n",
      "  Mean: 12.0%\n",
      "  Median: 5.4%\n",
      "  95th percentile: 47.0%\n",
      "\n",
      "Top 5 Largest Errors:\n",
      "  Actual: $5,156 | Predicted: $2,819 | Error: 45.3%\n",
      "  Actual: $12,842 | Predicted: $10,288 | Error: 19.9%\n",
      "  Actual: $2,049 | Predicted: $4,677 | Error: 128.2%\n",
      "  Actual: $8,040 | Predicted: $13,828 | Error: 72.0%\n",
      "  Actual: $16,575 | Predicted: $5,562 | Error: 66.4%\n",
      "\n",
      "Creating ensemble from: ['XGBoost', 'Gradient Boosting', 'CatBoost']\n",
      "Ensemble weights:\n",
      "  XGBoost: 0.362\n",
      "  Gradient Boosting: 0.326\n",
      "  CatBoost: 0.312\n",
      "\n",
      "Ensemble Results:\n",
      "MAE: $314.95\n",
      "RMSE: $859.39\n",
      "R²: 0.6689\n",
      "Ensemble vs Best Single Model: -3.4% improvement\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      " RECOMMENDATION: USE XGBoost\n",
      "   MAE: $304.62\n",
      "   R²: 0.6510\n",
      "   CV Consistency: $625.45 ± $176.00\n",
      "Final model type: XGBRegressor\n"
     ]
    }
   ],
   "source": [
    "def train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Enhanced model training with better validation and ensemble options\"\"\"\n",
    "    \n",
    "    print(\"Training enhanced models with improved configurations...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Enhanced models with optimized hyperparameters\n",
    "    models = {\n",
    "        'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_leaf=20,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add CatBoost if you have categorical features\n",
    "    try:\n",
    "        models['CatBoost'] = CatBoostRegressor(\n",
    "            iterations=200,\n",
    "            depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    except:\n",
    "        print(\"CatBoost not available, skipping...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_imputed, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RMSE: ${rmse:,.2f}\")\n",
    "            print(f\"Test R²: {r2:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} ± ${cv_std:,.2f}\")\n",
    "            \n",
    "            if baseline_mae:\n",
    "                improvement = ((baseline_mae - mae) / baseline_mae * 100)\n",
    "                print(f\"Improvement over baseline: {improvement:+.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Create intelligent ensemble from best performing models\"\"\"\n",
    "    \n",
    "    # Get top 3 models by MAE\n",
    "    valid_models = {k: v for k, v in results.items() if 'mae' in v}\n",
    "    if len(valid_models) < 2:\n",
    "        print(\"Not enough models for ensemble\")\n",
    "        return None, None\n",
    "    \n",
    "    top_models = sorted(valid_models.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    \n",
    "    print(f\"\\nCreating ensemble from: {[name for name, _ in top_models]}\")\n",
    "    \n",
    "    # Create weighted ensemble based on performance\n",
    "    ensemble_models = []\n",
    "    weights = []\n",
    "    \n",
    "    for name, result in top_models:\n",
    "        ensemble_models.append((name, result['model']))\n",
    "        # Weight inversely proportional to MAE (better models get higher weight)\n",
    "        weight = 1.0 / result['mae']\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w/total_weight for w in weights]\n",
    "    \n",
    "    print(\"Ensemble weights:\")\n",
    "    for (name, _), weight in zip(ensemble_models, normalized_weights):\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "    \n",
    "    # Create voting regressor with weights\n",
    "    ensemble = VotingRegressor(\n",
    "        estimators=ensemble_models,\n",
    "        weights=normalized_weights\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.fit(X_train_imputed, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test_imputed)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    mae_ens = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "    rmse_ens = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "    r2_ens = r2_score(y_test, y_pred_ensemble)\n",
    "    \n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"MAE: ${mae_ens:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse_ens:,.2f}\")\n",
    "    print(f\"R²: {r2_ens:.4f}\")\n",
    "    \n",
    "    # Compare with best single model\n",
    "    best_single_mae = top_models[0][1]['mae']\n",
    "    improvement = ((best_single_mae - mae_ens) / best_single_mae) * 100\n",
    "    \n",
    "    print(f\"Ensemble vs Best Single Model: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    return ensemble, mae_ens\n",
    "\n",
    "def analyze_model_performance(results, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Comprehensive model performance analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort models by MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])\n",
    "    \n",
    "    print(\"\\nModel Ranking (by MAE):\")\n",
    "    for i, (name, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i:2d}. {name:25} | MAE: ${result['mae']:,.2f} | R²: {result['r2']:.4f} | CV: ${result['cv_mae']:,.2f} ± ${result['cv_std']:,.2f}\")\n",
    "    \n",
    "    best_model_name, best_result = sorted_results[0]\n",
    "    \n",
    "    # Error analysis for best model\n",
    "    print(f\"\\n--- Error Analysis for {best_model_name} ---\")\n",
    "    y_pred_best = best_result['predictions']\n",
    "    errors = np.abs(y_test - y_pred_best)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: ${errors.mean():,.2f}\")\n",
    "    print(f\"Median Absolute Error: ${np.median(errors):,.2f}\")\n",
    "    print(f\"Max Error: ${errors.max():,.2f}\")\n",
    "    print(f\"Error Std: ${errors.std():,.2f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_pct = (errors / y_test) * 100\n",
    "    print(f\"\\nError Percentage Stats:\")\n",
    "    print(f\"  Mean: {error_pct.mean():.1f}%\")\n",
    "    print(f\"  Median: {error_pct.median():.1f}%\")\n",
    "    print(f\"  95th percentile: {np.percentile(error_pct, 95):.1f}%\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    worst_indices = np.argsort(errors)[-5:]\n",
    "    print(f\"\\nTop 5 Largest Errors:\")\n",
    "    for idx in worst_indices:\n",
    "        actual = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]\n",
    "        predicted = y_pred_best[idx]\n",
    "        error = abs(actual - predicted)\n",
    "        error_pct = (error / actual) * 100\n",
    "        print(f\"  Actual: ${actual:,.0f} | Predicted: ${predicted:,.0f} | Error: {error_pct:.1f}%\")\n",
    "    \n",
    "    return best_model_name, best_result\n",
    "\n",
    "# Main execution function\n",
    "def run_enhanced_training(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Complete enhanced training pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting Enhanced Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Train individual models\n",
    "    results = train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No models trained successfully!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. Analyze performance\n",
    "    best_model_name, best_result = analyze_model_performance(results, X_test_imputed, y_test)\n",
    "    \n",
    "    # 3. Create ensemble\n",
    "    ensemble_model, ensemble_mae = create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test)\n",
    "    \n",
    "    # 4. Final recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ensemble_model and ensemble_mae < best_result['mae']:\n",
    "        print(\" RECOMMENDATION: USE ENSEMBLE MODEL\")\n",
    "        print(f\"   Ensemble MAE: ${ensemble_mae:,.2f}\")\n",
    "        print(f\"   Best Single MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Improvement: {((best_result['mae'] - ensemble_mae) / best_result['mae'] * 100):+.1f}%\")\n",
    "        final_model = ensemble_model\n",
    "    else:\n",
    "        print(f\" RECOMMENDATION: USE {best_model_name}\")\n",
    "        print(f\"   MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   R²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV Consistency: ${best_result['cv_mae']:,.2f} ± ${best_result['cv_std']:,.2f}\")\n",
    "        final_model = best_result['model']\n",
    "    \n",
    "    return final_model, results\n",
    "\n",
    "# Usage - replace your existing training code with this:\n",
    "print(\"Running enhanced training pipeline...\")\n",
    "final_model, all_results = run_enhanced_training(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    ")\n",
    "\n",
    "# You can then use final_model for predictions\n",
    "if final_model:\n",
    "    #print(\"\\nTraining completed successfully!\")\n",
    "    print(f\"Final model type: {type(final_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9288c110-ba47-4153-9bcf-7c5840c4b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added targeted features for problem areas\n",
      "   Conservation areas: 522 properties\n",
      "   Large properties: 432 properties\n",
      "   Premium buildings: 165 properties\n"
     ]
    }
   ],
   "source": [
    "def enhance_commercial_features_targeted(commercial_enriched):\n",
    "    \"\"\"\n",
    "    Add features specifically for problematic property types\n",
    "    \"\"\"\n",
    "    df = commercial_enriched.copy()\n",
    "    \n",
    "    # 1. Conservation area indicator\n",
    "    conservation_keywords = ['conservation', 'heritage', 'shophouse', 'shop house']\n",
    "    df['is_conservation'] = df['Project Name'].str.lower().str.contains(\n",
    "        '|'.join(conservation_keywords), na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 2. Size-based features for large properties\n",
    "    if 'Area (SQM)' in df.columns:\n",
    "        df['is_large_property'] = (df['Area (SQM)'] > 150).astype(int)\n",
    "        df['is_very_large'] = (df['Area (SQM)'] > 300).astype(int)\n",
    "        \n",
    "        # Size-price interaction\n",
    "        df['area_price_interaction'] = df['Area (SQM)'] * df.get('Unit Price ($ PSM)', 1)\n",
    "    \n",
    "    # 3. Premium building indicator\n",
    "    premium_buildings = ['peninsula', 'marina', 'orchard', 'raffles', 'capital', 'suntec']\n",
    "    df['is_premium_building'] = df['Project Name'].str.lower().str.contains(\n",
    "        '|'.join(premium_buildings), na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 4. Property type interactions\n",
    "    property_dummies = pd.get_dummies(df['Property Type'], prefix='type')\n",
    "    df = pd.concat([df, property_dummies], axis=1)\n",
    "    \n",
    "    # 5. Location clusters based on performance\n",
    "    high_error_locations = ['peninsula', 'little india', 'telok ayer', 'oxley']\n",
    "    df['is_high_variance_location'] = df['Project Name'].str.lower().str.contains(\n",
    "        '|'.join(high_error_locations), na=False\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"Added targeted features for problem areas\")\n",
    "    print(f\"   Conservation areas: {df['is_conservation'].sum()} properties\")\n",
    "    print(f\"   Large properties: {df['is_large_property'].sum()} properties\")\n",
    "    print(f\"   Premium buildings: {df['is_premium_building'].sum()} properties\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply targeted feature engineering\n",
    "commercial_enriched = enhance_commercial_features_targeted(commercial_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe89e002-66d3-4cb0-8ea5-137bb7a3738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING PROPERTY-TYPE SPECIFIC MODELS\n",
      "==================================================\n",
      "\n",
      " PROCESSING SHOP HOUSE...\n",
      "   Samples: 526\n",
      "   Features: 37 total, 10 categorical, 24 numerical\n",
      "   Training set: (420, 454)\n",
      "   Test set: (106, 454)\n",
      "   Performance - MAE: $498.70, RMSE: $985.59, R²: 0.9189\n",
      "\n",
      " PROCESSING RETAIL...\n",
      "   Samples: 1050\n",
      "   Features: 37 total, 10 categorical, 24 numerical\n",
      "   Training set: (840, 626)\n",
      "   Test set: (210, 626)\n",
      "   Performance - MAE: $239.25, RMSE: $433.08, R²: 0.8980\n",
      "\n",
      " PROCESSING OFFICE...\n",
      "   Samples: 1348\n",
      "   Features: 37 total, 10 categorical, 24 numerical\n",
      "   Training set: (1078, 456)\n",
      "   Test set: (270, 456)\n",
      "   Performance - MAE: $251.75, RMSE: $688.73, R²: 0.7874\n",
      "\n",
      "🎉 CREATED 3 PROPERTY-TYPE MODELS:\n",
      "   Shop House      | MAE: $498.70 | R²: 0.9189 | Samples: 420\n",
      "   Retail          | MAE: $239.25 | R²: 0.8980 | Samples: 840\n",
      "   Office          | MAE: $251.75 | R²: 0.7874 | Samples: 1078\n"
     ]
    }
   ],
   "source": [
    "def create_property_type_models(commercial_enriched):\n",
    "    \"\"\"\n",
    "    Create and train separate models for each property type\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    target_column = 'Unit Price ($ PSF)'\n",
    "    \n",
    "    exclude_columns = ['Transacted Price ($)', 'Unit Price ($ PSM)',\n",
    "        'Unit Price ($ PSF)',\n",
    "        'monthly_rental_price_yield',\n",
    "        'rental_rate_psm_yield',\n",
    "        'monthly_rental_price_tenure',\n",
    "        'rental_rate_psm_tenure',\n",
    "        'monthly_rental_price_market',\n",
    "        'rental_rate_psm_avg',\n",
    "        'monthly_rental_price_avg',\n",
    "        'market_rent_rate_psm',\n",
    "        'annual_rental_income_yield','implied_yield_market']\n",
    "    \n",
    "    print(\"CREATING PROPERTY-TYPE SPECIFIC MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    property_type_models = {}\n",
    "    \n",
    "    def handle_missing_values(X, strategy='mean'):\n",
    "        \"\"\"Handle missing values in the feature matrix\"\"\"\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "        return pd.DataFrame(X_imputed, columns=X.columns, index=X.index), imputer\n",
    "    \n",
    "    for prop_type in commercial_enriched['Property Type'].unique():\n",
    "        print(f\"\\n PROCESSING {prop_type.upper()}...\")\n",
    "        \n",
    "        # Filter data for this property type\n",
    "        type_mask = commercial_enriched['Property Type'] == prop_type\n",
    "        type_data = commercial_enriched[type_mask]\n",
    "        \n",
    "        # Skip if not enough samples\n",
    "        if len(type_data) < 30:\n",
    "            print(f\"   ⚠  Skipped - only {len(type_data)} samples (need at least 30)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"   Samples: {len(type_data)}\")\n",
    "        \n",
    "        # Use YOUR EXACT feature selection logic\n",
    "        feature_columns = [col for col in type_data.columns \n",
    "                          if col not in exclude_columns and col != target_column]\n",
    "        \n",
    "        # Use YOUR EXACT preprocessing\n",
    "        categorical_columns = type_data[feature_columns].select_dtypes(include=['object', 'category']).columns\n",
    "        numerical_columns = type_data[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        print(f\"   Features: {len(feature_columns)} total, {len(categorical_columns)} categorical, {len(numerical_columns)} numerical\")\n",
    "        \n",
    "        # One-hot encode using YOUR method\n",
    "        X_encoded = pd.get_dummies(type_data[feature_columns], columns=categorical_columns, drop_first=True)\n",
    "        y = type_data[target_column]\n",
    "        \n",
    "        # Split the data\n",
    "        X_train_type, X_test_type, y_train_type, y_test_type = train_test_split(\n",
    "            X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Handle missing values using FIXED function\n",
    "        X_train_imputed_type, imputer = handle_missing_values(X_train_type)\n",
    "        X_test_imputed_type = pd.DataFrame(imputer.transform(X_test_type), \n",
    "                                         columns=X_test_type.columns, index=X_test_type.index)\n",
    "        \n",
    "        print(f\"   Training set: {X_train_imputed_type.shape}\")\n",
    "        print(f\"   Test set: {X_test_imputed_type.shape}\")\n",
    "        \n",
    "        # Train model (using your preferred algorithm)\n",
    "        model_type = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model_type.fit(X_train_imputed_type, y_train_type)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred_type = model_type.predict(X_test_imputed_type)\n",
    "        mae_type = mean_absolute_error(y_test_type, y_pred_type)\n",
    "        rmse_type = np.sqrt(mean_squared_error(y_test_type, y_pred_type))\n",
    "        r2_type = r2_score(y_test_type, y_pred_type)\n",
    "        \n",
    "        print(f\"   Performance - MAE: ${mae_type:,.2f}, RMSE: ${rmse_type:,.2f}, R²: {r2_type:.4f}\")\n",
    "        \n",
    "        # Store the model and its preprocessing information\n",
    "        property_type_models[prop_type] = {\n",
    "            'model': model_type,\n",
    "            'feature_columns': feature_columns,\n",
    "            'categorical_columns': categorical_columns.tolist(),\n",
    "            'imputer': imputer,\n",
    "            'performance': {\n",
    "                'mae': mae_type,\n",
    "                'rmse': rmse_type, \n",
    "                'r2': r2_type\n",
    "            },\n",
    "            'training_samples': len(X_train_imputed_type),\n",
    "            'feature_names_after_encoding': X_train_imputed_type.columns.tolist()\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n CREATED {len(property_type_models)} PROPERTY-TYPE MODELS:\")\n",
    "    for prop_type, info in property_type_models.items():\n",
    "        perf = info['performance']\n",
    "        print(f\"   {prop_type:15} | MAE: ${perf['mae']:,.2f} | R²: {perf['r2']:.4f} | Samples: {info['training_samples']}\")\n",
    "    \n",
    "    return property_type_models\n",
    "\n",
    "# Usage:\n",
    "property_type_models = create_property_type_models(commercial_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4df351ab-f022-4fec-af00-843ff5c66d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving property-type-specific models...\n",
      "   Models for: ['Shop House', 'Retail', 'Office']\n",
      "   Shop House: 454 features, R²=0.9189\n",
      "   Retail: 626 features, R²=0.8980\n",
      "   Office: 456 features, R²=0.7874\n",
      "\n",
      " Model saved to: commercial_real_estate_model_final.pkl\n",
      "   Model type: property_type_specific\n",
      "   Property-type-specific: True\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# IMPORTANT: Save the property-type-specific models from Cell 34\n",
    "# These models have much better performance (R²: 0.79-0.92) than the combined model\n",
    "# Each property type has its own model, feature set, and imputer\n",
    "\n",
    "# Check if property_type_models exists (from Cell 34)\n",
    "if 'property_type_models' not in globals() or not property_type_models:\n",
    "    print(\" WARNING: property_type_models not found. Please run Cell 34 first to create property-type-specific models.\")\n",
    "    print(\"   Falling back to combined model (less accurate)...\")\n",
    "    \n",
    "    # Fallback: Use combined model if property-type models don't exist\n",
    "    print(\"Running enhanced training pipeline...\")\n",
    "    final_model, all_results = run_enhanced_training(\n",
    "        X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    "    )\n",
    "    \n",
    "    if final_model:\n",
    "        sorted_results = sorted(all_results.items(), key=lambda x: x[1]['mae'])\n",
    "        best_model_name, best_result = sorted_results[0]\n",
    "        deployment_package = {\n",
    "            'model': final_model,  # Single combined model\n",
    "            'feature_names': list(X_train_imputed.columns),\n",
    "            'performance': best_result,\n",
    "            'timestamp': datetime.datetime.now(),\n",
    "            'model_type': 'single_model',\n",
    "            'is_property_type_specific': False\n",
    "        }\n",
    "    else:\n",
    "        print(\" Failed to create model\")\n",
    "        deployment_package = None\n",
    "else:\n",
    "    # Use property-type-specific models (preferred - better accuracy)\n",
    "    print(\" Saving property-type-specific models...\")\n",
    "    print(f\"   Models for: {list(property_type_models.keys())}\")\n",
    "    \n",
    "    # Create deployment package with property-type-specific models\n",
    "    deployment_package = {\n",
    "        'model': property_type_models,  # Dictionary of property-type-specific models\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'model_type': 'property_type_specific',\n",
    "        'is_property_type_specific': True,\n",
    "        'property_types': list(property_type_models.keys()),\n",
    "        'model_info': {}\n",
    "    }\n",
    "    \n",
    "    # Add metadata for each property type model\n",
    "    for prop_type, model_info in property_type_models.items():\n",
    "        deployment_package['model_info'][prop_type] = {\n",
    "            'feature_names_after_encoding': model_info['feature_names_after_encoding'],\n",
    "            'feature_columns': model_info['feature_columns'],\n",
    "            'categorical_columns': model_info['categorical_columns'],\n",
    "            'performance': model_info['performance'],\n",
    "            'training_samples': model_info['training_samples'],\n",
    "            'n_features': len(model_info['feature_names_after_encoding'])\n",
    "        }\n",
    "        print(f\"   {prop_type}: {len(model_info['feature_names_after_encoding'])} features, R²={model_info['performance']['r2']:.4f}\")\n",
    "\n",
    "# Save to PKL file\n",
    "if deployment_package:\n",
    "    filename = 'commercial_real_estate_model_final.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(deployment_package, f)\n",
    "    print(f\"\\n Model saved to: {filename}\")\n",
    "    print(f\"   Model type: {deployment_package.get('model_type', 'unknown')}\")\n",
    "    print(f\"   Property-type-specific: {deployment_package.get('is_property_type_specific', False)}\")\n",
    "else:\n",
    "    print(\" Failed to create deployment package\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

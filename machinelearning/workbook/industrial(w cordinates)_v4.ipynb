{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b514c1b-9d7e-4f51-8cf9-a7a60c1fdf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import joblib\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5410682-02bc-4996-9c63-78468e6bc3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Month Year</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Planning Area</th>\n",
       "      <th>Type of Sale</th>\n",
       "      <th>Price</th>\n",
       "      <th>$psm</th>\n",
       "      <th>Area</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Contract Date</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Type Of Area</th>\n",
       "      <th>Floor Level</th>\n",
       "      <th>Region</th>\n",
       "      <th>Postal Sector</th>\n",
       "      <th>Postal District</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41711.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>WAVE9</td>\n",
       "      <td>WOODLANDS INDUSTRIAL PARK E9</td>\n",
       "      <td>Woodlands</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$1,108,888</td>\n",
       "      <td>$3,772</td>\n",
       "      <td>294.0</td>\n",
       "      <td>30 yrs from 05/06/2014</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>First Floor</td>\n",
       "      <td>North Region</td>\n",
       "      <td>75.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41712.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>CATTEL BUILDING</td>\n",
       "      <td>ALEXANDRA TERRACE</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$3,800,000</td>\n",
       "      <td>$3,007</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Land</td>\n",
       "      <td>-</td>\n",
       "      <td>Central Region</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41713.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>LAM SOON INDUSTRIAL BUILDING</td>\n",
       "      <td>HILLVIEW AVENUE</td>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$1,510,900</td>\n",
       "      <td>$6,243</td>\n",
       "      <td>242.0</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>West Region</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41714.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>SYNERGY @ KB</td>\n",
       "      <td>KAKI BUKIT ROAD 4</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$458,000</td>\n",
       "      <td>$3,368</td>\n",
       "      <td>136.0</td>\n",
       "      <td>30 yrs from 20/01/2012</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>East Region</td>\n",
       "      <td>41.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41715.0</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>WCEGA TOWER</td>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>Resale</td>\n",
       "      <td>$800,000</td>\n",
       "      <td>$4,372</td>\n",
       "      <td>183.0</td>\n",
       "      <td>60 yrs from 13/03/1997</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>Multiple-User Factory</td>\n",
       "      <td>Strata</td>\n",
       "      <td>Non-First Floor</td>\n",
       "      <td>West Region</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID Month Year                  Project Name  \\\n",
       "0  41711.0    2021-01                         WAVE9   \n",
       "1  41712.0    2021-01               CATTEL BUILDING   \n",
       "2  41713.0    2021-01  LAM SOON INDUSTRIAL BUILDING   \n",
       "3  41714.0    2021-01                  SYNERGY @ KB   \n",
       "4  41715.0    2021-01                   WCEGA TOWER   \n",
       "\n",
       "                    Street Name Planning Area Type of Sale       Price  \\\n",
       "0  WOODLANDS INDUSTRIAL PARK E9     Woodlands       Resale  $1,108,888   \n",
       "1             ALEXANDRA TERRACE    Queenstown       Resale  $3,800,000   \n",
       "2               HILLVIEW AVENUE   Bukit Batok       Resale  $1,510,900   \n",
       "3             KAKI BUKIT ROAD 4         Bedok       Resale    $458,000   \n",
       "4          BUKIT BATOK CRESCENT   Bukit Batok       Resale    $800,000   \n",
       "\n",
       "     $psm    Area                  Tenure Contract Date  \\\n",
       "0  $3,772   294.0  30 yrs from 05/06/2014    2021-01-04   \n",
       "1  $3,007  1264.0                Freehold    2021-01-04   \n",
       "2  $6,243   242.0                Freehold    2021-01-04   \n",
       "3  $3,368   136.0  30 yrs from 20/01/2012    2021-01-04   \n",
       "4  $4,372   183.0  60 yrs from 13/03/1997    2021-01-04   \n",
       "\n",
       "           Property Type Type Of Area      Floor Level          Region  \\\n",
       "0  Multiple-User Factory       Strata      First Floor    North Region   \n",
       "1  Multiple-User Factory         Land                -  Central Region   \n",
       "2  Multiple-User Factory       Strata  Non-First Floor     West Region   \n",
       "3  Multiple-User Factory       Strata  Non-First Floor     East Region   \n",
       "4  Multiple-User Factory       Strata  Non-First Floor     West Region   \n",
       "\n",
       "   Postal Sector  Postal District  Unnamed: 17  Unnamed: 18  \n",
       "0           75.0             27.0          NaN          NaN  \n",
       "1           11.0              5.0          NaN          NaN  \n",
       "2           66.0             23.0          NaN          NaN  \n",
       "3           41.0             14.0          NaN          NaN  \n",
       "4           65.0             23.0          NaN          NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_df=pd.read_csv(\"industrial_data.csv\")\n",
    "industrial_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daefe2f-4a58-4b9d-bea3-c2b69d3ea5e2",
   "metadata": {},
   "source": [
    "## cleaning dataset & removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da712e4-b1d3-4611-84ce-d6419589f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\AppData\\Local\\Temp\\ipykernel_28840\\104824214.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  industrial_df = industrial_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "industrial_df = industrial_df.drop(['ID', 'Unnamed: 17','Unnamed: 18'], axis=1)\n",
    "industrial_df = industrial_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a2c685-d64a-4668-9d88-f74630a97219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing empty rows: (8221, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\AppData\\Local\\Temp\\ipykernel_28840\\3471636226.py:105: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  return pd.to_datetime(date_str)\n"
     ]
    }
   ],
   "source": [
    "def clean_industrial_data(df):\n",
    "    \"\"\"Clean and prepare industrial property data with proper error handling\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove completely empty rows\n",
    "    df_clean = df_clean.dropna(how='all')\n",
    "    \n",
    "    print(f\"Data after removing empty rows: {df_clean.shape}\")\n",
    "    \n",
    "    # Clean price columns with better error handling\n",
    "    def clean_price_column(price_series):\n",
    "        \"\"\"Clean price column with comprehensive handling\"\"\"\n",
    "        cleaned_prices = []\n",
    "        for price in price_series:\n",
    "            if pd.isna(price) or price in ['', ' ', 'NaN', 'nan']:\n",
    "                cleaned_prices.append(np.nan)\n",
    "            else:\n",
    "                # Convert to string and clean\n",
    "                price_str = str(price).strip()\n",
    "                # Remove $ and commas\n",
    "                price_str = price_str.replace('$', '').replace(',', '')\n",
    "                # Remove any extra spaces\n",
    "                price_str = price_str.replace(' ', '')\n",
    "                try:\n",
    "                    cleaned_prices.append(float(price_str))\n",
    "                except (ValueError, TypeError):\n",
    "                    cleaned_prices.append(np.nan)\n",
    "        return cleaned_prices\n",
    "    \n",
    "    df_clean['Price'] = clean_price_column(df_clean['Price'])\n",
    "    df_clean['$psm'] = clean_price_column(df_clean['$psm'])\n",
    "    \n",
    "    # Remove rows with invalid prices\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Price', '$psm'])\n",
    "    \n",
    "    # Rename columns to match commercial model format\n",
    "    df_clean = df_clean.rename(columns={\n",
    "        '$psm': 'Unit Price ($ PSM)',\n",
    "        'Area': 'Area (SQM)',\n",
    "        'Price': 'Transacted Price ($)',\n",
    "        'Property Type': 'Property Type',\n",
    "        'Type Of Area': 'Type of Area',\n",
    "        'Floor Level': 'Floor Level',\n",
    "        'Postal District': 'Postal District',\n",
    "        'Planning Area': 'Planning Area',\n",
    "        'Region': 'Region'\n",
    "    })\n",
    "    \n",
    "    # Handle Floor Level\n",
    "    def parse_industrial_floor_level(floor_str):\n",
    "        if pd.isna(floor_str) or floor_str in ['-', ' ', '']:\n",
    "            return 0, 0, 0, 0, 1  # ground/first floor\n",
    "        elif str(floor_str).strip() == 'First Floor':\n",
    "            return 0, 0, 0, 0, 1  # first floor\n",
    "        elif str(floor_str).strip() == 'Non-First Floor':\n",
    "            return 1, 1, 1, 0, 0  # non-first floor\n",
    "        else:\n",
    "            # Try to extract numbers if available\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', str(floor_str))\n",
    "            if numbers:\n",
    "                floor_num = int(numbers[0])\n",
    "                return floor_num, floor_num, floor_num, 0, 0\n",
    "            else:\n",
    "                return 0, 0, 0, 0, 1  # default to ground\n",
    "    \n",
    "    floor_data = df_clean['Floor Level'].apply(parse_industrial_floor_level)\n",
    "    df_clean['Floor_Low'] = [x[0] for x in floor_data]\n",
    "    df_clean['Floor_High'] = [x[1] for x in floor_data]\n",
    "    df_clean['Floor_Midpoint'] = [x[2] for x in floor_data]\n",
    "    df_clean['Is_Basement'] = [x[3] for x in floor_data]\n",
    "    df_clean['Is_Ground'] = [x[4] for x in floor_data]\n",
    "    \n",
    "    # Create floor categories for industrial\n",
    "    def create_industrial_floor_category(midpoint):\n",
    "        if midpoint == 0: \n",
    "            return 'ground_floor'\n",
    "        elif midpoint == 1:\n",
    "            return 'upper_floor'\n",
    "        else:\n",
    "            return f'level_{int(midpoint)}'\n",
    "    \n",
    "    df_clean['Floor_Category'] = df_clean['Floor_Midpoint'].apply(create_industrial_floor_category)\n",
    "    \n",
    "    # Handle Tenure\n",
    "    def categorize_tenure(tenure_str):\n",
    "        if pd.isna(tenure_str) or tenure_str in ['', ' ']:\n",
    "            return 'Unknown'\n",
    "        tenure_str = str(tenure_str).lower()\n",
    "        if 'freehold' in tenure_str:\n",
    "            return 'Freehold'\n",
    "        elif any(x in tenure_str for x in ['30 yrs', '60 yrs', '99 yrs']):\n",
    "            return 'Leasehold'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    df_clean['Tenure_Type'] = df_clean['Tenure'].apply(categorize_tenure)\n",
    "    \n",
    "    # Convert date with error handling\n",
    "    def parse_contract_date(date_str):\n",
    "        if pd.isna(date_str) or date_str in ['', ' ']:\n",
    "            return pd.NaT\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    df_clean['Contract Date'] = df_clean['Contract Date'].apply(parse_contract_date)\n",
    "    df_clean['Contract Date'].unique()\n",
    "    \n",
    "    df_clean['Contract_date_missing'] = df_clean['Contract Date'].isna().astype(int)\n",
    "    \n",
    "    df_clean['Contract_year'] = df_clean['Contract Date'].dt.year\n",
    "    df_clean['Contract_month'] = df_clean['Contract Date'].dt.month\n",
    "    df_clean['Contract_quarter'] = df_clean['Contract Date'].dt.quarter\n",
    "    df_clean['Contract_dayofweek'] = df_clean['Contract Date'].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "    \n",
    "    # continuous trend feature\n",
    "    #df['days_since_first_Contract'] = (df['Contract Date'] - df['Contract Date'].min()).dt.days\n",
    "    \n",
    "    df_clean.drop(columns=['Contract Date'], inplace=True)\n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean industrial data with proper error handling\n",
    "industrial_df = clean_industrial_data(industrial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2828d0-92d3-4806-82f4-293162e37390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts per row:\n",
      " 0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "8216    0\n",
      "8217    0\n",
      "8218    0\n",
      "8219    0\n",
      "8220    0\n",
      "Length: 8220, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = industrial_df.isnull().sum(axis=1)\n",
    "print(\"Missing counts per row:\\n\", missing_counts)\n",
    "\n",
    "missing_counts = industrial_df.isna().sum(axis=1)  # Count missing values per row\n",
    "industrial_df.drop(industrial_df[missing_counts >= 4].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757cc2e-e4ec-4033-b38b-9dbad456f60e",
   "metadata": {},
   "source": [
    "### fill in project name null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c9a7f4-0b63-45a9-a663-ee9bf4a995d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING PROJECT NAME FILLING PIPELINE\n",
      "==================================================\n",
      "ðŸ— COMPREHENSIVE PROJECT NAME FILLING\n",
      "==================================================\n",
      "\n",
      "1.  Pattern matching...\n",
      "Found 187 known street-project mappings\n",
      "Filled 174 missing project names using pattern matching\n",
      "\n",
      "2.  Pattern extraction...\n",
      "Filled 127 missing project names using pattern extraction\n",
      "\n",
      "3.  Geographic clustering...\n",
      "Filled 0 missing project names using geographic clustering\n",
      "\n",
      "4.  Fuzzy matching...\n",
      "Known properties: 8077\n",
      "Properties with missing project names: 142\n",
      "Filled 46 missing project names using fuzzy matching\n",
      "\n",
      " FILLING SUMMARY:\n",
      "Initial missing: 443\n",
      "Pattern matched: 174\n",
      "Pattern extracted: 127\n",
      "Cluster matched: 0\n",
      "Fuzzy matched: 46\n",
      "Final missing: 96\n",
      "Success rate: 78.3%\n",
      "Applied 1654 custom project name mappings\n",
      " VERIFYING PROJECT NAME FILLING\n",
      "========================================\n",
      "Missing project names:\n",
      "  Before filling: 443\n",
      "  After filling: 95\n",
      "  Filled: 348\n",
      "  Success rate: 78.6%\n",
      "\n",
      " SAMPLE FILLED PROJECT NAMES (first 10):\n",
      "  Street: kian teck way -> Project: JURONG INDUSTRIAL ESTATE\n",
      "  Street: tuas avenue 8 -> Project: Tuas\n",
      "  Street: woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "  Street:  playfair road -> Project: KAPO FACTORY BUILDING\n",
      "  Street:  woodlands loop -> Project: ADVANCED HQ\n",
      "  Street:  macpherson road -> Project: BETIME BUILDING\n",
      "  Street: woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "  Street: woodlands industrial park e1 -> Project: WOODLANDS INDUSTRIAL PARK\n",
      "\n",
      " REMAINING MISSING PROJECT NAMES (sample):\n",
      "  Street:  neythal road\n",
      "  Street:  gul lane\n",
      "  Street: woodlands terrace\n",
      "  Street: chang charn road\n",
      "  Street: neythal road\n",
      "\n",
      " FINAL DATA QUALITY:\n",
      "Total properties: 8219\n",
      "Properties with project names: 8124\n",
      "Properties without project names: 95\n",
      "\n",
      " SAMPLE RESULTS:\n",
      "  woodlands industrial park e9 -> WOODLANDS INDUSTRIAL PARK (woodlands)\n",
      "  alexandra terrace -> CATTEL BUILDING (queenstown)\n",
      "  hillview avenue -> LAM SOON INDUSTRIAL BUILDING (bukit batok)\n",
      "  kaki bukit road 4 -> KAKI BUKIT INDUSTRIAL ESTATE (bedok)\n",
      "  bukit batok crescent -> BUKIT BATOK INDUSTRIAL ESTATE (bukit batok)\n",
      "  woodlands link -> woodlands east industrial estate (sembawang)\n",
      "  jalan pemimpin -> mapex (bishan)\n",
      "  jalan pemimpin -> m38 (bishan)\n",
      "  boon lay way -> tradehub 21 (clementi)\n",
      "  boon lay way -> 8 @ tradehub 21 (clementi)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# First, define all the individual functions\n",
    "\n",
    "def fill_project_names_pattern_based(df):\n",
    "    \"\"\"Fill missing project names based on street name patterns\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Create a mapping from known project names to street names\n",
    "    known_mappings = {}\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.notna(row['Project Name']) and pd.notna(row['Street Name']):\n",
    "            project = str(row['Project Name']).strip().upper()\n",
    "            street = str(row['Street Name']).strip().upper()\n",
    "            if project and street:\n",
    "                known_mappings[street] = project\n",
    "    \n",
    "    print(f\"Found {len(known_mappings)} known street-project mappings\")\n",
    "    \n",
    "    # Fill missing project names using the mapping\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            street_name = str(row['Street Name']).strip().upper() if pd.notna(row['Street Name']) else ''\n",
    "            if street_name in known_mappings:\n",
    "                df_filled.at[idx, 'Project Name'] = known_mappings[street_name]\n",
    "                filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using pattern matching\")\n",
    "    return df_filled, known_mappings\n",
    "\n",
    "def extract_project_from_street_patterns(df):\n",
    "    \"\"\"Extract project names from street names using common patterns\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Common industrial estate patterns in Singapore\n",
    "    industrial_patterns = [\n",
    "        (r'(.*)\\s(INDUSTRIAL|INDUSTRIAL PARK|BIZHUB|TECHNOPARK|TECH PARK)', 1),  # \"WOODLANDS INDUSTRIAL PARK\" -> \"WOODLANDS\"\n",
    "        (r'(.*)\\s(ROAD|STREET|AVENUE|DRIVE|LANE|TERRACE)\\s*(\\d+[A-Z]?)', 1),    # \"KAKI BUKIT ROAD 4\" -> \"KAKI BUKIT\"\n",
    "        (r'(.*)\\s(CRESCENT|CIRCLE|PLACE|GARDENS|VIEW)', 1),                     # \"BUKIT BATOK CRESCENT\" -> \"BUKIT BATOK\"\n",
    "        (r'(.*)\\s(COMPLEX|CENTRE|HUB|BUILDING|TOWER)', 1),                      # \"CATTEL BUILDING\" -> \"CATTEL\"\n",
    "    ]\n",
    "    \n",
    "    filled_count = 0\n",
    "    \n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            street_name = str(row['Street Name']) if pd.notna(row['Street Name']) else ''\n",
    "            \n",
    "            if street_name:\n",
    "                # Try each pattern\n",
    "                for pattern, group_num in industrial_patterns:\n",
    "                    match = re.search(pattern, street_name, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        extracted_name = match.group(group_num).strip()\n",
    "                        # Clean up the extracted name\n",
    "                        extracted_name = extracted_name.title()\n",
    "                        df_filled.at[idx, 'Project Name'] = extracted_name\n",
    "                        filled_count += 1\n",
    "                        break\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using pattern extraction\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_geographic_clusters(df):\n",
    "    \"\"\"Fill project names based on geographic clustering\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Group by Planning Area and Street Name patterns\n",
    "    geographic_clusters = {}\n",
    "    \n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.notna(row['Project Name']) and str(row['Project Name']).strip() not in ['', 'NaN', 'nan']:\n",
    "            planning_area = str(row['Planning Area']).upper() if pd.notna(row['Planning Area']) else 'UNKNOWN'\n",
    "            street_name = str(row['Street Name']).upper() if pd.notna(row['Street Name']) else 'UNKNOWN'\n",
    "            \n",
    "            cluster_key = f\"{planning_area}_{street_name}\"\n",
    "            if cluster_key not in geographic_clusters:\n",
    "                geographic_clusters[cluster_key] = []\n",
    "            \n",
    "            geographic_clusters[cluster_key].append(row['Project Name'])\n",
    "    \n",
    "    # For each cluster, find the most common project name\n",
    "    cluster_project_names = {}\n",
    "    for cluster_key, projects in geographic_clusters.items():\n",
    "        if projects:\n",
    "            # Get the most frequent project name in this cluster\n",
    "            most_common = Counter(projects).most_common(1)\n",
    "            if most_common:\n",
    "                cluster_project_names[cluster_key] = most_common[0][0]\n",
    "    \n",
    "    # Fill missing project names using clusters\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled.iterrows():\n",
    "        if pd.isna(row['Project Name']) or str(row['Project Name']).strip() in ['', 'NaN', 'nan']:\n",
    "            planning_area = str(row['Planning Area']).upper() if pd.notna(row['Planning Area']) else 'UNKNOWN'\n",
    "            street_name = str(row['Street Name']).upper() if pd.notna(row['Street Name']) else 'UNKNOWN'\n",
    "            \n",
    "            cluster_key = f\"{planning_area}_{street_name}\"\n",
    "            if cluster_key in cluster_project_names:\n",
    "                df_filled.at[idx, 'Project Name'] = cluster_project_names[cluster_key]\n",
    "                filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using geographic clustering\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_fuzzy(df, similarity_threshold=80):\n",
    "    \"\"\"Fill missing project names using fuzzy matching on similar properties\"\"\"\n",
    "    try:\n",
    "        from fuzzywuzzy import fuzz\n",
    "    except ImportError:\n",
    "        print(\"Fuzzywuzzy not installed, skipping fuzzy matching\")\n",
    "        return df\n",
    "        \n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    # Get properties with known project names\n",
    "    known_properties = df_filled[df_filled['Project Name'].notna() & \n",
    "                                (df_filled['Project Name'].str.strip() != '')].copy()\n",
    "    \n",
    "    # Get properties with missing project names\n",
    "    missing_properties = df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')].copy()\n",
    "    \n",
    "    print(f\"Known properties: {len(known_properties)}\")\n",
    "    print(f\"Properties with missing project names: {len(missing_properties)}\")\n",
    "    \n",
    "    filled_count = 0\n",
    "    \n",
    "    for idx, missing_row in missing_properties.iterrows():\n",
    "        missing_street = str(missing_row['Street Name']).upper() if pd.notna(missing_row['Street Name']) else ''\n",
    "        missing_planning_area = str(missing_row['Planning Area']).upper() if pd.notna(missing_row['Planning Area']) else ''\n",
    "        \n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for _, known_row in known_properties.iterrows():\n",
    "            known_street = str(known_row['Street Name']).upper() if pd.notna(known_row['Street Name']) else ''\n",
    "            known_planning_area = str(known_row['Planning Area']).upper() if pd.notna(known_row['Planning Area']) else ''\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            street_similarity = fuzz.ratio(missing_street, known_street)\n",
    "            area_similarity = fuzz.ratio(missing_planning_area, known_planning_area)\n",
    "            \n",
    "            # Combined score (weight street name more heavily)\n",
    "            total_score = (street_similarity * 0.7) + (area_similarity * 0.3)\n",
    "            \n",
    "            if total_score > best_score and total_score >= similarity_threshold:\n",
    "                best_score = total_score\n",
    "                best_match = known_row['Project Name']\n",
    "        \n",
    "        if best_match:\n",
    "            df_filled.at[idx, 'Project Name'] = best_match\n",
    "            filled_count += 1\n",
    "    \n",
    "    print(f\"Filled {filled_count} missing project names using fuzzy matching\")\n",
    "    return df_filled\n",
    "\n",
    "def fill_project_names_comprehensive(df):\n",
    "    \"\"\"Comprehensive approach combining multiple methods\"\"\"\n",
    "    df_filled = df.copy()\n",
    "    \n",
    "    print(\"ðŸ— COMPREHENSIVE PROJECT NAME FILLING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Track filling statistics\n",
    "    initial_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                   (df_filled['Project Name'].str.strip() == '')])\n",
    "    \n",
    "    filling_stats = {\n",
    "        'initial_missing': initial_missing,\n",
    "        'pattern_matched': 0,\n",
    "        'extracted': 0,\n",
    "        'cluster_matched': 0,\n",
    "        'fuzzy_matched': 0\n",
    "    }\n",
    "    \n",
    "    # Method 1: Direct pattern matching\n",
    "    print(\"\\n1.  Pattern matching...\")\n",
    "    df_filled, mappings = fill_project_names_pattern_based(df_filled)\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['pattern_matched'] = filling_stats['initial_missing'] - current_missing\n",
    "    \n",
    "    # Method 2: Pattern extraction\n",
    "    print(\"\\n2.  Pattern extraction...\")\n",
    "    df_filled = extract_project_from_street_patterns(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['extracted'] = prev_missing - current_missing\n",
    "    \n",
    "    # Method 3: Geographic clustering\n",
    "    print(\"\\n3.  Geographic clustering...\")\n",
    "    df_filled = fill_project_names_geographic_clusters(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    current_missing = len(df_filled[df_filled['Project Name'].isna() | \n",
    "                                  (df_filled['Project Name'].str.strip() == '')])\n",
    "    filling_stats['cluster_matched'] = prev_missing - current_missing\n",
    "    \n",
    "    # Method 4: Try fuzzy matching if available\n",
    "    print(\"\\n4.  Fuzzy matching...\")\n",
    "    df_filled_fuzzy = fill_project_names_fuzzy(df_filled)\n",
    "    prev_missing = current_missing\n",
    "    final_missing = len(df_filled_fuzzy[df_filled_fuzzy['Project Name'].isna() | \n",
    "                                      (df_filled_fuzzy['Project Name'].str.strip() == '')])\n",
    "    filling_stats['fuzzy_matched'] = prev_missing - final_missing\n",
    "    df_filled = df_filled_fuzzy\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n FILLING SUMMARY:\")\n",
    "    print(f\"Initial missing: {filling_stats['initial_missing']}\")\n",
    "    print(f\"Pattern matched: {filling_stats['pattern_matched']}\")\n",
    "    print(f\"Pattern extracted: {filling_stats['extracted']}\")\n",
    "    print(f\"Cluster matched: {filling_stats['cluster_matched']}\")\n",
    "    print(f\"Fuzzy matched: {filling_stats['fuzzy_matched']}\")\n",
    "    print(f\"Final missing: {final_missing}\")\n",
    "    \n",
    "    if filling_stats['initial_missing'] > 0:\n",
    "        success_rate = (filling_stats['initial_missing'] - final_missing) / filling_stats['initial_missing'] * 100\n",
    "        print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "def add_custom_project_rules(df):\n",
    "    \"\"\"Add custom rules for specific known industrial estates\"\"\"\n",
    "    df_custom = df.copy()\n",
    "    \n",
    "    # Singapore industrial estate mappings\n",
    "    custom_mappings = {\n",
    "        # Woodlands area\n",
    "        'WOODLANDS INDUSTRIAL PARK E9': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS INDUSTRIAL PARK E5': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS INDUSTRIAL PARK E1': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        'WOODLANDS CLOSE': 'WOODLANDS INDUSTRIAL PARK',\n",
    "        \n",
    "        # Kaki Bukit area\n",
    "        'KAKI BUKIT ROAD 1': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        'KAKI BUKIT ROAD 2': 'KAKI BUKIT INDUSTRIAL ESTATE', \n",
    "        'KAKI BUKIT ROAD 3': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        'KAKI BUKIT ROAD 4': 'KAKI BUKIT INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Bukit Batok area\n",
    "        'BUKIT BATOK CRESCENT': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        'BUKIT BATOK STREET 23': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        'BUKIT BATOK EAST AVENUE 6': 'BUKIT BATOK INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Tuas area\n",
    "        'TUAS SOUTH AVENUE 3': 'TUAS INDUSTRIAL ESTATE',\n",
    "        'TUAS AVENUE 11': 'TUAS INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Pioneer area\n",
    "        'PIONEER ROAD': 'PIONEER INDUSTRIAL ESTATE',\n",
    "        'PIONEER SECTOR 1': 'PIONEER INDUSTRIAL ESTATE',\n",
    "        \n",
    "        # Specific known buildings\n",
    "        'ALEXANDRA TERRACE': 'CATTEL BUILDING',\n",
    "        'HILLVIEW AVENUE': 'LAM SOON INDUSTRIAL BUILDING',\n",
    "    }\n",
    "    \n",
    "    filled_count = 0\n",
    "    for idx, row in df_custom.iterrows():\n",
    "        street_name = str(row['Street Name']).upper().strip() if pd.notna(row['Street Name']) else ''\n",
    "        \n",
    "        # Check if this street matches any custom mapping\n",
    "        for street_pattern, project_name in custom_mappings.items():\n",
    "            if street_pattern in street_name:\n",
    "                df_custom.at[idx, 'Project Name'] = project_name\n",
    "                filled_count += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Applied {filled_count} custom project name mappings\")\n",
    "    return df_custom\n",
    "\n",
    "def verify_project_name_filling(df_before, df_after):\n",
    "    \"\"\"Verify the quality of project name filling\"\"\"\n",
    "    \n",
    "    print(\" VERIFYING PROJECT NAME FILLING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Count missing project names\n",
    "    missing_before = len(df_before[df_before['Project Name'].isna() | \n",
    "                                 (df_before['Project Name'].str.strip() == '')])\n",
    "    missing_after = len(df_after[df_after['Project Name'].isna() | \n",
    "                               (df_after['Project Name'].str.strip() == '')])\n",
    "    \n",
    "    print(f\"Missing project names:\")\n",
    "    print(f\"  Before filling: {missing_before}\")\n",
    "    print(f\"  After filling: {missing_after}\")\n",
    "    print(f\"  Filled: {missing_before - missing_after}\")\n",
    "    \n",
    "    if missing_before > 0:\n",
    "        success_rate = (missing_before - missing_after) / missing_before * 100\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    # Show sample of filled project names\n",
    "    filled_indices = []\n",
    "    for idx in df_after.index:\n",
    "        if idx in df_before.index:\n",
    "            before_name = df_before.loc[idx, 'Project Name']\n",
    "            after_name = df_after.loc[idx, 'Project Name']\n",
    "            if (pd.isna(before_name) or str(before_name).strip() in ['', 'NaN', 'nan']) and pd.notna(after_name):\n",
    "                filled_indices.append(idx)\n",
    "    \n",
    "    if filled_indices:\n",
    "        print(f\"\\n SAMPLE FILLED PROJECT NAMES (first 10):\")\n",
    "        for idx in filled_indices[:10]:\n",
    "            street = df_after.loc[idx, 'Street Name']\n",
    "            project = df_after.loc[idx, 'Project Name']\n",
    "            print(f\"  Street: {street} -> Project: {project}\")\n",
    "    \n",
    "    # Show remaining missing ones\n",
    "    if missing_after > 0:\n",
    "        missing_samples = df_after[df_after['Project Name'].isna() | \n",
    "                                 (df_after['Project Name'].str.strip() == '')].head(5)\n",
    "        \n",
    "        print(f\"\\n REMAINING MISSING PROJECT NAMES (sample):\")\n",
    "        for idx, row in missing_samples.iterrows():\n",
    "            print(f\"  Street: {row['Street Name']}\")\n",
    "\n",
    "# Now run the complete pipeline\n",
    "print(\"STARTING PROJECT NAME FILLING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make a copy of your original data to preserve it\n",
    "industrial_clean_filled = industrial_df.copy()\n",
    "\n",
    "# Step 1: Comprehensive filling\n",
    "industrial_complete = fill_project_names_comprehensive(industrial_clean_filled)\n",
    "\n",
    "# Step 2: Apply custom rules\n",
    "industrial_final = add_custom_project_rules(industrial_complete)\n",
    "\n",
    "# Step 3: Verify results\n",
    "verify_project_name_filling(industrial_df, industrial_final)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n FINAL DATA QUALITY:\")\n",
    "print(f\"Total properties: {len(industrial_final)}\")\n",
    "print(f\"Properties with project names: {len(industrial_final[industrial_final['Project Name'].notna() & (industrial_final['Project Name'].str.strip() != '')])}\")\n",
    "print(f\"Properties without project names: {len(industrial_final[industrial_final['Project Name'].isna() | (industrial_final['Project Name'].str.strip() == '')])}\")\n",
    "\n",
    "# Show some examples of the results\n",
    "print(f\"\\n SAMPLE RESULTS:\")\n",
    "sample_data = industrial_final[['Street Name', 'Project Name', 'Planning Area']].head(10)\n",
    "for idx, row in sample_data.iterrows():\n",
    "    print(f\"  {row['Street Name']} -> {row['Project Name']} ({row['Planning Area']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2924d5-90a9-441a-9b6e-89a55b8a445e",
   "metadata": {},
   "source": [
    "## fill in remaining values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8c283b-af94-477b-8c37-ba184fb33b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining null Project Names: 95\n",
      "Direct mapping filled: 1\n",
      "Pattern extraction filled: -261\n",
      "Initial missing: 95\n",
      "Final missing: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "Final null counts:\n",
      "Street Name        1\n",
      "Floor Level        1\n",
      "Postal Sector      2\n",
      "Postal District    1\n",
      "dtype: int64\n",
      "\n",
      "Sample filled project names:\n",
      "  woodlands industrial park e9 -> WOODLANDS INDUSTRIAL PARK\n",
      "  alexandra terrace -> CATTEL BUILDING\n",
      "  hillview avenue -> LAM SOON INDUSTRIAL BUILDING\n",
      "  kaki bukit road 4 -> KAKI BUKIT INDUSTRIAL ESTATE\n",
      "  bukit batok crescent -> BUKIT BATOK INDUSTRIAL ESTATE\n",
      "  woodlands link -> woodlands east industrial estate\n",
      "  jalan pemimpin -> mapex\n",
      "  jalan pemimpin -> m38\n"
     ]
    }
   ],
   "source": [
    "# Quick cleaning for remaining nulls\n",
    "def clean_remaining_nulls(df):\n",
    "    df_clean = industrial_final.copy()\n",
    "    \n",
    "    # Fill simple nulls\n",
    "    df_clean['Street Name'] = df_clean['Street Name'].fillna('Boon Lay')\n",
    "    #df_clean['Contract Date'] = df_clean['Contract Date'].fillna(df_clean['Contract Date'].mode()[0] if len(df_clean['Contract Date'].mode()) > 0 else '2021-01-01')\n",
    "    df_clean['Floor Level'] = df_clean['Floor Level'].fillna('Unknown')\n",
    "    df_clean['Postal District'] = df_clean['Postal District'].fillna(df_clean['Postal District'].median())\n",
    "    df_clean['Postal Sector'] = df_clean['Postal Sector'].fillna(df_clean['Postal Sector'].median())\n",
    "    \n",
    "    # Fill date-derived columns\n",
    "    latest_year = df_clean['Contract_year'].max()\n",
    "    df_clean['Contract_year'] = df_clean['Contract_year'].fillna(latest_year)\n",
    "    df_clean['Contract_month'] = df_clean['Contract_month'].fillna(1)\n",
    "    \n",
    "    print(f\"Remaining null Project Names: {df_clean['Project Name'].isna().sum()}\")\n",
    "    return df_clean\n",
    "\n",
    "industrial_clean = clean_remaining_nulls(industrial_final)\n",
    "\n",
    "# Efficient project name filling\n",
    "def fill_project_names_efficient(df):\n",
    "    df_filled = industrial_final.copy()\n",
    "    initial_missing = df_filled['Project Name'].isna().sum()\n",
    "    \n",
    "    # Method 1: Direct mapping from existing data\n",
    "    street_to_project = {}\n",
    "    for _, row in df_filled[df_filled['Project Name'].notna()].iterrows():\n",
    "        street = str(row['Street Name']).upper().strip()\n",
    "        project = str(row['Project Name']).strip()\n",
    "        if street and project:\n",
    "            street_to_project[street] = project\n",
    "    \n",
    "    # Apply direct mappings\n",
    "    filled_count = 0\n",
    "    for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "        street = str(row['Street Name']).upper().strip()\n",
    "        if street in street_to_project:\n",
    "            df_filled.at[idx, 'Project Name'] = street_to_project[street]\n",
    "            filled_count += 1\n",
    "    \n",
    "    print(f\"Direct mapping filled: {filled_count}\")\n",
    "    \n",
    "    # Method 2: Extract from street patterns\n",
    "    patterns = [\n",
    "        (r'(.*)\\s(INDUSTRIAL|INDUSTRIAL PARK|BIZHUB)', 1),\n",
    "        (r'(.*)\\s(ROAD|STREET|AVENUE|DRIVE)\\s*(\\d+)', 1),\n",
    "        (r'(.*)\\s(CRESCENT|CIRCLE|PLACE)', 1),\n",
    "    ]\n",
    "    \n",
    "    for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "        street = str(row['Street Name'])\n",
    "        for pattern, group_num in patterns:\n",
    "            match = re.search(pattern, street, re.IGNORECASE)\n",
    "            if match:\n",
    "                df_filled.at[idx, 'Project Name'] = match.group(group_num).strip().title()\n",
    "                filled_count += 1\n",
    "                break\n",
    "    \n",
    "    print(f\"Pattern extraction filled: {filled_count - len(street_to_project)}\")\n",
    "    \n",
    "    # Method 3: Planning area + property type as fallback\n",
    "    remaining_missing = df_filled['Project Name'].isna().sum()\n",
    "    if remaining_missing > 0:\n",
    "        for idx, row in df_filled[df_filled['Project Name'].isna()].iterrows():\n",
    "            area = str(row['Planning Area']) if pd.notna(row['Planning Area']) else 'Industrial'\n",
    "            prop_type = str(row['Property Type']) if pd.notna(row['Property Type']) else 'Estate'\n",
    "            df_filled.at[idx, 'Project Name'] = f\"{area} {prop_type}\"\n",
    "    \n",
    "    final_missing = df_filled['Project Name'].isna().sum()\n",
    "    print(f\"Initial missing: {initial_missing}\")\n",
    "    print(f\"Final missing: {final_missing}\")\n",
    "    print(f\"Success rate: {(initial_missing - final_missing) / initial_missing * 100:.1f}%\")\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "# Apply efficient filling\n",
    "industrial_final_v2 = fill_project_names_efficient(industrial_final)\n",
    "\n",
    "# Verify all nulls are handled\n",
    "print(\"\\nFinal null counts:\")\n",
    "null_summary = industrial_final_v2.isnull().sum()\n",
    "print(null_summary[null_summary > 0])\n",
    "\n",
    "# Sample of filled project names\n",
    "print(\"\\nSample filled project names:\")\n",
    "filled_samples = industrial_final_v2[industrial_final_v2['Project Name'].notna()].head(8)[['Street Name', 'Project Name']]\n",
    "for _, row in filled_samples.iterrows():\n",
    "    print(f\"  {row['Street Name']} -> {row['Project Name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337eeec2-cc81-4fbf-8da8-e8bf2f8b7b05",
   "metadata": {},
   "source": [
    "## predict rental price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6e2541-b80d-4ed0-981b-7a11dba98c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_rental_methods(df, annual_yield=0.055, market_rental_rates=None):\n",
    "    \"\"\"\n",
    "    Calculate rental prices using all three methods for comparison\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Method 1: Standard Yield Method\n",
    "    df['annual_rental_income_yield'] = df['Transacted Price ($)'] * annual_yield\n",
    "    df['monthly_rental_price_yield'] = df['annual_rental_income_yield'] / 12\n",
    "    df['rental_rate_psm_yield'] = df['monthly_rental_price_yield'] / df['Area (SQM)']\n",
    "    \n",
    "    # Method 2: Tenure-Based Yield Method\n",
    "    freehold_yield = 0.05    # 5% for freehold\n",
    "    leasehold_yield = 0.06   # 6% for leasehold\n",
    "    \n",
    "    df['tenure_based_yield'] = df['Tenure_Type'].map({\n",
    "        'Freehold': freehold_yield,\n",
    "        'Leasehold': leasehold_yield\n",
    "    })\n",
    "    df['annual_rental_income_tenure'] = df['Transacted Price ($)'] * df['tenure_based_yield']\n",
    "    df['monthly_rental_price_tenure'] = df['annual_rental_income_tenure'] / 12\n",
    "    df['rental_rate_psm_tenure'] = df['monthly_rental_price_tenure'] / df['Area (SQM)']\n",
    "    \n",
    "    # Method 3: Market Rates Method\n",
    "    if market_rental_rates is None:\n",
    "        # Default market rates if not provided\n",
    "        market_rental_rates = {\n",
    "            'woodlands': 18.0,\n",
    "            'queenstown': 25.0,\n",
    "            'bukit batok': 20.0,\n",
    "            'bedok': 19.0,\n",
    "            'toa payoh': 22.0,\n",
    "            'jurong east': 19.0,\n",
    "            'jurong west': 18.0,\n",
    "            'kallang': 23.0,\n",
    "            'clementi': 21.0,\n",
    "            'serangoon': 20.0\n",
    "        }\n",
    "    \n",
    "    df['planning_area_lower'] = df['Planning Area'].str.lower()\n",
    "    df['market_rent_rate_psm'] = df['planning_area_lower'].map(market_rental_rates)\n",
    "    \n",
    "    # Fill missing rates with overall median\n",
    "    if df['market_rent_rate_psm'].isna().any():\n",
    "        median_rate = df['market_rent_rate_psm'].median()\n",
    "        df['market_rent_rate_psm'] = df['market_rent_rate_psm'].fillna(median_rate)\n",
    "    \n",
    "    df['monthly_rental_price_market'] = df['market_rent_rate_psm'] * df['Area (SQM)']\n",
    "    df['annual_rental_income_market'] = df['monthly_rental_price_market'] * 12\n",
    "    df['implied_yield_market'] = df['annual_rental_income_market'] / df['Transacted Price ($)']\n",
    "    \n",
    "    # Calculate averages across all methods\n",
    "    df['monthly_rental_price_avg'] = df[[\n",
    "        'monthly_rental_price_yield', \n",
    "        'monthly_rental_price_tenure', \n",
    "        'monthly_rental_price_market'\n",
    "    ]].mean(axis=1)\n",
    "    \n",
    "    df['rental_rate_psm_avg'] = df[[\n",
    "        'rental_rate_psm_yield', \n",
    "        'rental_rate_psm_tenure', \n",
    "        'market_rent_rate_psm'\n",
    "    ]].mean(axis=1)\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop('planning_area_lower', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply all three methods\n",
    "industrial_final_v2 = calculate_all_rental_methods(industrial_df, annual_yield=0.055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aedca4-8082-42fd-8c81-2f65b7c6b754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27291ed8-6313-41e6-bbca-454e673b3fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e37c6a5b-1476-4370-9217-d4760958fb9c",
   "metadata": {},
   "source": [
    "## Cordinates loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaeecda9-5ef7-4c05-9de7-3917bdc8a06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Industrial data: (8219, 27)\n",
      "Postal codes: (121154, 4)\n",
      "City coordinates: (332, 9)\n",
      "Street coordinates: (589, 3)\n",
      "Train stations: (209, 4)\n",
      "Postal district mapping: (28, 3)\n"
     ]
    }
   ],
   "source": [
    "postal_codes = pd.read_csv('SG_postal.csv')\n",
    "city_coordinates = pd.read_csv('singapore_city_coordinates_improved.csv')\n",
    "street_coordinates = pd.read_csv('street_coordinates.csv')\n",
    "train_stations = pd.read_csv('mrt_lrt_data.csv')\n",
    "postal_district_mapping = pd.read_csv('sg_postal_districts.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Industrial data: {industrial_df.shape}\")\n",
    "print(f\"Postal codes: {postal_codes.shape}\")\n",
    "print(f\"City coordinates: {city_coordinates.shape}\")\n",
    "print(f\"Street coordinates: {street_coordinates.shape}\")\n",
    "print(f\"Train stations: {train_stations.shape}\")\n",
    "print(f\"Postal district mapping: {postal_district_mapping.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff4f8c2-bd58-4fba-ab70-0df089fdf2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Month Year                  Project Name                   Street Name  \\\n",
      "0    2021-01                         wave9  woodlands industrial park e9   \n",
      "1    2021-01               cattel building             alexandra terrace   \n",
      "2    2021-01  lam soon industrial building               hillview avenue   \n",
      "3    2021-01                  synergy @ kb             kaki bukit road 4   \n",
      "4    2021-01                   wcega tower          bukit batok crescent   \n",
      "\n",
      "  Planning Area Type of Sale  Transacted Price ($)  Unit Price ($ PSM)  \\\n",
      "0     woodlands       resale             1108888.0              3772.0   \n",
      "1    queenstown       resale             3800000.0              3007.0   \n",
      "2   bukit batok       resale             1510900.0              6243.0   \n",
      "3         bedok       resale              458000.0              3368.0   \n",
      "4   bukit batok       resale              800000.0              4372.0   \n",
      "\n",
      "   Area (SQM)                  Tenure          Property Type  ...  \\\n",
      "0       294.0  30 yrs from 05/06/2014  multiple-user factory  ...   \n",
      "1      1264.0                freehold  multiple-user factory  ...   \n",
      "2       242.0                freehold  multiple-user factory  ...   \n",
      "3       136.0  30 yrs from 20/01/2012  multiple-user factory  ...   \n",
      "4       183.0  60 yrs from 13/03/1997  multiple-user factory  ...   \n",
      "\n",
      "  tenure_based_yield annual_rental_income_tenure monthly_rental_price_tenure  \\\n",
      "0               0.06                    66533.28                 5544.440000   \n",
      "1               0.05                   190000.00                15833.333333   \n",
      "2               0.05                    75545.00                 6295.416667   \n",
      "3               0.06                    27480.00                 2290.000000   \n",
      "4               0.06                    48000.00                 4000.000000   \n",
      "\n",
      "   rental_rate_psm_tenure  market_rent_rate_psm  monthly_rental_price_market  \\\n",
      "0               18.858639                  18.0                       5292.0   \n",
      "1               12.526371                  25.0                      31600.0   \n",
      "2               26.014118                  20.0                       4840.0   \n",
      "3               16.838235                  19.0                       2584.0   \n",
      "4               21.857923                  20.0                       3660.0   \n",
      "\n",
      "   annual_rental_income_market  implied_yield_market  \\\n",
      "0                      63504.0              0.057268   \n",
      "1                     379200.0              0.099789   \n",
      "2                      58080.0              0.038441   \n",
      "3                      31008.0              0.067703   \n",
      "4                      43920.0              0.054900   \n",
      "\n",
      "   monthly_rental_price_avg  rental_rate_psm_avg  \n",
      "0               5306.281111            18.048575  \n",
      "1              21616.666667            17.101793  \n",
      "2               6020.125000            24.876550  \n",
      "3               2324.388889            17.091095  \n",
      "4               3775.555556            20.631451  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "print(industrial_final_v2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa2c3f84-eda1-4044-8519-60fddac1a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding geographic features with enhanced coverage...\n",
      "Starting geographic feature engineering for 8219 properties...\n",
      " Strategy 1A: Street name matching â†’ 5549 properties\n",
      "Strategy 1B: Postal district centroids â†’ 2670 properties\n",
      " Strategy 1C: Planning Area matching â†’ 108 properties\n",
      " Strategy 1D: Region-based fallback â†’ 68 properties\n",
      "FINAL COORDINATE COVERAGE: 8219/8219 properties (100.0%)\n",
      " Added general location for 8218 properties\n",
      "Calculating MRT distances for 8219 properties...\n",
      " Added MRT distances for 8219 properties\n",
      " Added region classification for 8218 properties\n",
      "Calculating CBD distances...\n",
      " Added CBD distances for 8219 properties\n",
      " Added urban classification\n",
      "\n",
      "============================================================\n",
      " GEOGRAPHIC FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "FINAL COVERAGE REPORT:\n",
      "   â€¢ Coordinates: 8219/8219\n",
      "   â€¢ MRT Distances: 8219/8219\n",
      "   â€¢ Region Classification: 8218/8219\n",
      "   â€¢ General Location: 8218/8219\n",
      "   â€¢ CBD Distances: 8219/8219\n"
     ]
    }
   ],
   "source": [
    "def add_geographic_features(main_df, postal_df, street_df, city_df, stations_df, district_df):\n",
    "    \"\"\"Enrich industrial data with geographic and proximity features - FIXED VERSION\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from rapidfuzz import process\n",
    "\n",
    "    df_enriched = main_df.copy()\n",
    "    original_count = len(df_enriched)\n",
    "    industrial_final_v2.rename(columns={'Postal_District_Str  ': 'Postal_District'}, inplace=True)\n",
    "    print(f\"Starting geographic feature engineering for {original_count} properties...\")\n",
    "\n",
    "    # --- 1. MULTI-STRATEGY COORDINATE MATCHING ---\n",
    "    \n",
    "    # Strategy 1A: Direct street name matching (your current approach)\n",
    "    if 'Street Name' in df_enriched.columns:\n",
    "        street_df['street_name_clean'] = street_df['street_name'].str.upper().str.strip()\n",
    "        df_enriched['Street_Name_Clean'] = df_enriched['Street Name'].str.upper().str.strip()\n",
    "\n",
    "        street_choices = street_df['street_name_clean'].unique().tolist()\n",
    "\n",
    "        def match_street(name):\n",
    "            if pd.isna(name): return None\n",
    "            match = process.extractOne(name, street_choices, score_cutoff=80)  # Lowered threshold\n",
    "            return match[0] if match else None\n",
    "\n",
    "        df_enriched['Matched_Street'] = df_enriched['Street_Name_Clean'].apply(match_street)\n",
    "\n",
    "        df_enriched = df_enriched.merge(\n",
    "            street_df[['street_name_clean', 'latitude', 'longitude']],\n",
    "            left_on='Matched_Street', right_on='street_name_clean', how='left'\n",
    "        )\n",
    "        street_matches = df_enriched['latitude'].notna().sum()\n",
    "        print(f\" Strategy 1A: Street name matching â†’ {street_matches} properties\")\n",
    "\n",
    "    # Strategy 1B: Postal code matching for missing coordinates\n",
    "    if 'Postal District' in df_enriched.columns and 'postal_code' in postal_df.columns:\n",
    "        # Convert postal codes to district (first 2 digits for Singapore)\n",
    "        postal_df['Postal_District'] = postal_df['postal_code'].astype(str).str[:2]\n",
    "        \n",
    "        # Get centroid coordinates for each postal district\n",
    "        district_coords = postal_df.groupby('Postal_District').agg({\n",
    "            'lat': 'mean',\n",
    "            'lon': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Merge district centroids for properties missing coordinates\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            df_enriched.loc[missing_coords_mask, 'Postal_District_Str'] = df_enriched.loc[missing_coords_mask, 'Postal District'].apply(\n",
    "                lambda x: str(int(x)) if pd.notna(x) else np.nan\n",
    "            )\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                district_coords.rename(columns={'lat': 'lat_district', 'lon': 'lon_district'}),\n",
    "                left_on='Postal_District_Str', right_on='Postal_District', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with district centroids\n",
    "            district_fill_mask = df_enriched['latitude'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            df_enriched.loc[district_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lat_district']\n",
    "            df_enriched.loc[district_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[district_fill_mask].index)['lon_district']\n",
    "            \n",
    "            district_matches = district_fill_mask.sum()\n",
    "            print(f\"Strategy 1B: Postal district centroids â†’ {district_matches} properties\")\n",
    "\n",
    "    # Strategy 1C: Planning Area matching from city coordinates\n",
    "    if 'Planning Area' in df_enriched.columns and 'Place' in city_df.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            city_df['Place_Clean'] = city_df['Place'].str.upper().str.strip()\n",
    "            df_enriched['Planning_Area_Clean'] = df_enriched['Planning Area'].str.upper().str.strip()\n",
    "            \n",
    "            temp_merge = df_enriched[missing_coords_mask].merge(\n",
    "                city_df[['Place_Clean', 'latitude', 'longitude']].rename(\n",
    "                    columns={'latitude': 'lat_city', 'longitude': 'lon_city'}\n",
    "                ),\n",
    "                left_on='Planning_Area_Clean', right_on='Place_Clean', how='left'\n",
    "            )\n",
    "            \n",
    "            # Fill missing coordinates with city coordinates\n",
    "            city_fill_mask = df_enriched['latitude'].isna() & df_enriched['Planning_Area_Clean'].notna()\n",
    "            df_enriched.loc[city_fill_mask, 'latitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lat_city']\n",
    "            df_enriched.loc[city_fill_mask, 'longitude'] = temp_merge.set_index(df_enriched[city_fill_mask].index)['lon_city']\n",
    "            \n",
    "            city_matches = city_fill_mask.sum()\n",
    "            print(f\" Strategy 1C: Planning Area matching â†’ {city_matches} properties\")\n",
    "\n",
    "    # Strategy 1D: Region-based fallback coordinates\n",
    "    if 'Region' in df_enriched.columns:\n",
    "        missing_coords_mask = df_enriched['latitude'].isna()\n",
    "        if missing_coords_mask.any():\n",
    "            # Define approximate coordinates for major regions\n",
    "            region_coords = {\n",
    "                'CENTRAL REGION': (1.2923, 103.8536),  # Singapore central\n",
    "                'EAST REGION': (1.3443, 103.9645),     # East area\n",
    "                'WEST REGION': (1.3526, 103.7584),     # West area\n",
    "                'NORTH REGION': (1.4180, 103.8200),    # North area\n",
    "                'NORTH-EAST REGION': (1.3691, 103.8975) # North-East\n",
    "            }\n",
    "            \n",
    "            def get_region_coords(region):\n",
    "                if pd.isna(region): return (np.nan, np.nan)\n",
    "                region_upper = str(region).upper()\n",
    "                for key, coords in region_coords.items():\n",
    "                    if key in region_upper:\n",
    "                        return coords\n",
    "                return (np.nan, np.nan)\n",
    "            \n",
    "            region_coords_df = df_enriched[missing_coords_mask]['Region'].apply(get_region_coords)\n",
    "            region_fill_mask = df_enriched['latitude'].isna() & df_enriched['Region'].notna()\n",
    "            \n",
    "            df_enriched.loc[region_fill_mask, 'latitude'] = region_coords_df.apply(lambda x: x[0])\n",
    "            df_enriched.loc[region_fill_mask, 'longitude'] = region_coords_df.apply(lambda x: x[1])\n",
    "            \n",
    "            region_matches = region_fill_mask.sum()\n",
    "            print(f\" Strategy 1D: Region-based fallback â†’ {region_matches} properties\")\n",
    "\n",
    "    # Final coordinate coverage report\n",
    "    final_coverage = df_enriched['latitude'].notna().sum()\n",
    "    print(f\"FINAL COORDINATE COVERAGE: {final_coverage}/{original_count} properties ({final_coverage/original_count*100:.1f}%)\")\n",
    "\n",
    "    # --- 2. ENHANCED POSTAL DISTRICT FEATURES ---\n",
    "    \n",
    "    if 'Postal District' in df_enriched.columns:\n",
    "        # Create proper postal district string (handle NaNs)\n",
    "        df_enriched['Postal_District_Str'] = df_enriched['Postal District'].apply(\n",
    "            lambda x: str(int(x)) if pd.notna(x) else 'Unknown'\n",
    "        )\n",
    "        \n",
    "        district_df['Postal District'] = district_df['Postal District'].astype(str)\n",
    "        \n",
    "        df_enriched = df_enriched.merge(\n",
    "            district_df[['Postal District', 'General Location']].rename(columns={'General Location': 'General_Location'}),\n",
    "            left_on='Postal_District_Str', right_on='Postal District', how='left'\n",
    "        )\n",
    "        print(f\" Added general location for {df_enriched['General_Location'].notna().sum()} properties\")\n",
    "\n",
    "    # --- 3. FIXED MRT DISTANCE CALCULATION ---\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate Haversine distance between two points in kilometers\"\"\"\n",
    "        R = 6371  # Earth radius in kilometers\n",
    "        \n",
    "        lat1_rad = np.radians(lat1)\n",
    "        lon1_rad = np.radians(lon1)\n",
    "        lat2_rad = np.radians(lat2)\n",
    "        lon2_rad = np.radians(lon2)\n",
    "        \n",
    "        dlat = lat2_rad - lat1_rad\n",
    "        dlon = lon2_rad - lon1_rad\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        \n",
    "        return R * c\n",
    "\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        station_coords = stations_df[['Latitude', 'Longitude']].dropna()\n",
    "        \n",
    "        if len(valid_coords) > 0 and len(station_coords) > 0:\n",
    "            print(f\"Calculating MRT distances for {len(valid_coords)} properties...\")\n",
    "            \n",
    "            # Vectorized distance calculation\n",
    "            min_dists = []\n",
    "            for idx, prop_row in valid_coords.iterrows():\n",
    "                prop_lat, prop_lon = prop_row['latitude'], prop_row['longitude']\n",
    "                \n",
    "                # Calculate distances to all stations\n",
    "                distances = []\n",
    "                for _, station_row in station_coords.iterrows():\n",
    "                    dist = haversine_distance(\n",
    "                        prop_lat, prop_lon,\n",
    "                        station_row['Latitude'], station_row['Longitude']\n",
    "                    )\n",
    "                    distances.append(dist)\n",
    "                \n",
    "                min_dists.append(min(distances) if distances else np.nan)\n",
    "            \n",
    "            # Assign distances back to dataframe\n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_MRT_km'] = min_dists\n",
    "            print(f\" Added MRT distances for {len([x for x in min_dists if not np.isnan(x)])} properties\")\n",
    "            \n",
    "            # For properties without coordinates, use district average\n",
    "            missing_mrt_mask = df_enriched['Distance_to_MRT_km'].isna() & df_enriched['Postal_District_Str'].notna()\n",
    "            if missing_mrt_mask.any():\n",
    "                district_mrt_avg = df_enriched.groupby('Postal_District_Str')['Distance_to_MRT_km'].mean()\n",
    "                df_enriched.loc[missing_mrt_mask, 'Distance_to_MRT_km'] = df_enriched.loc[missing_mrt_mask, 'Postal_District_Str'].map(district_mrt_avg)\n",
    "                print(f\" Added district-average MRT distances for {missing_mrt_mask.sum()} properties\")\n",
    "\n",
    "    # --- 4. ENHANCED REGION CLASSIFICATION ---\n",
    "    \n",
    "    def classify_region(d):\n",
    "        if pd.isna(d) or d == 'Unknown': return 'Unknown'\n",
    "        try:\n",
    "            d_int = int(d)\n",
    "            if d_int <= 9: return 'Central Core'\n",
    "            elif d_int <= 16: return 'Rest Central'\n",
    "            elif d_int <= 21: return 'City Fringe'\n",
    "            elif d_int <= 28: return 'Outside Central'\n",
    "            else: return 'Unknown'\n",
    "        except:\n",
    "            return 'Unknown'\n",
    "\n",
    "    df_enriched['Region_Classification'] = df_enriched['Postal_District_Str'].apply(classify_region)\n",
    "    region_coverage = (df_enriched['Region_Classification'] != 'Unknown').sum()\n",
    "    print(f\" Added region classification for {region_coverage} properties\")\n",
    "\n",
    "    # --- 5. ADDITIONAL GEOGRAPHIC FEATURES ---\n",
    "    \n",
    "    # CBD proximity (distance to Raffles Place)\n",
    "    cbd_coords = (1.2833, 103.8515)  # Raffles Place\n",
    "    if all(col in df_enriched.columns for col in ['latitude', 'longitude']):\n",
    "        valid_coords = df_enriched[['latitude', 'longitude']].dropna()\n",
    "        if len(valid_coords) > 0:\n",
    "            print(\"Calculating CBD distances...\")\n",
    "            cbd_distances = []\n",
    "            for idx, row in valid_coords.iterrows():\n",
    "                dist = haversine_distance(\n",
    "                    row['latitude'], row['longitude'],\n",
    "                    cbd_coords[0], cbd_coords[1]\n",
    "                )\n",
    "                cbd_distances.append(dist)\n",
    "            \n",
    "            df_enriched.loc[valid_coords.index, 'Distance_to_CBD_km'] = cbd_distances\n",
    "            print(f\" Added CBD distances for {len(cbd_distances)} properties\")\n",
    "\n",
    "    # Urban vs Suburban classification\n",
    "    def classify_urban_rural(distance_to_cbd):\n",
    "        if pd.isna(distance_to_cbd): return 'Unknown'\n",
    "        if distance_to_cbd <= 5: return 'CBD'\n",
    "        elif distance_to_cbd <= 10: return 'Urban'\n",
    "        elif distance_to_cbd <= 20: return 'Suburban'\n",
    "        else: return 'Rural'\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        df_enriched['Urban_Classification'] = df_enriched['Distance_to_CBD_km'].apply(classify_urban_rural)\n",
    "        print(f\" Added urban classification\")\n",
    "\n",
    "    # --- FINAL REPORT ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" GEOGRAPHIC FEATURE ENGINEERING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"FINAL COVERAGE REPORT:\")\n",
    "    print(f\"   â€¢ Coordinates: {df_enriched['latitude'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ MRT Distances: {df_enriched['Distance_to_MRT_km'].notna().sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ Region Classification: {(df_enriched['Region_Classification'] != 'Unknown').sum()}/{original_count}\")\n",
    "    print(f\"   â€¢ General Location: {df_enriched['General_Location'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    if 'Distance_to_CBD_km' in df_enriched.columns:\n",
    "        print(f\"   â€¢ CBD Distances: {df_enriched['Distance_to_CBD_km'].notna().sum()}/{original_count}\")\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    cols_to_drop = [col for col in df_enriched.columns if col in ['Matched_Street', 'street_name_clean', 'Postal_District_Str', 'Postal District_y']]\n",
    "    df_enriched = df_enriched.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df_enriched\n",
    "\n",
    "# Call the improved function\n",
    "print(\"\\nAdding geographic features with enhanced coverage...\")\n",
    "industrial_enriched = add_geographic_features(\n",
    "    industrial_final_v2, \n",
    "    postal_codes, \n",
    "    street_coordinates, \n",
    "    city_coordinates, \n",
    "    train_stations, \n",
    "    postal_district_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e6b3fad-35cf-4293-bda0-d592fc3f635d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month Year</th>\n",
       "      <th>Project Name</th>\n",
       "      <th>Street Name</th>\n",
       "      <th>Planning Area</th>\n",
       "      <th>Type of Sale</th>\n",
       "      <th>Transacted Price ($)</th>\n",
       "      <th>Unit Price ($ PSM)</th>\n",
       "      <th>Area (SQM)</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>...</th>\n",
       "      <th>rental_rate_psm_avg</th>\n",
       "      <th>Street_Name_Clean</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Planning_Area_Clean</th>\n",
       "      <th>General_Location</th>\n",
       "      <th>Distance_to_MRT_km</th>\n",
       "      <th>Region_Classification</th>\n",
       "      <th>Distance_to_CBD_km</th>\n",
       "      <th>Urban_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>wave9</td>\n",
       "      <td>woodlands industrial park e9</td>\n",
       "      <td>woodlands</td>\n",
       "      <td>resale</td>\n",
       "      <td>1108888.0</td>\n",
       "      <td>3772.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>30 yrs from 05/06/2014</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>18.048575</td>\n",
       "      <td>WOODLANDS INDUSTRIAL PARK E9</td>\n",
       "      <td>1.440807</td>\n",
       "      <td>103.771032</td>\n",
       "      <td>WOODLANDS</td>\n",
       "      <td>Yishun, Sembawang</td>\n",
       "      <td>0.982056</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>19.666080</td>\n",
       "      <td>Suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>cattel building</td>\n",
       "      <td>alexandra terrace</td>\n",
       "      <td>queenstown</td>\n",
       "      <td>resale</td>\n",
       "      <td>3800000.0</td>\n",
       "      <td>3007.0</td>\n",
       "      <td>1264.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>17.101793</td>\n",
       "      <td>ALEXANDRA TERRACE</td>\n",
       "      <td>1.291067</td>\n",
       "      <td>103.819738</td>\n",
       "      <td>QUEENSTOWN</td>\n",
       "      <td>Pasir Panjang, Hong Leong Garden, Clementi New...</td>\n",
       "      <td>0.365268</td>\n",
       "      <td>Central Core</td>\n",
       "      <td>3.634989</td>\n",
       "      <td>CBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>lam soon industrial building</td>\n",
       "      <td>hillview avenue</td>\n",
       "      <td>bukit batok</td>\n",
       "      <td>resale</td>\n",
       "      <td>1510900.0</td>\n",
       "      <td>6243.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>24.876550</td>\n",
       "      <td>HILLVIEW AVENUE</td>\n",
       "      <td>1.355502</td>\n",
       "      <td>103.761731</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...</td>\n",
       "      <td>0.826301</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>12.807806</td>\n",
       "      <td>Suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>synergy @ kb</td>\n",
       "      <td>kaki bukit road 4</td>\n",
       "      <td>bedok</td>\n",
       "      <td>resale</td>\n",
       "      <td>458000.0</td>\n",
       "      <td>3368.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>30 yrs from 20/01/2012</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>17.091095</td>\n",
       "      <td>KAKI BUKIT ROAD 4</td>\n",
       "      <td>1.341884</td>\n",
       "      <td>103.956072</td>\n",
       "      <td>BEDOK</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.334138</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>13.325616</td>\n",
       "      <td>Suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01</td>\n",
       "      <td>wcega tower</td>\n",
       "      <td>bukit batok crescent</td>\n",
       "      <td>bukit batok</td>\n",
       "      <td>resale</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>4372.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>60 yrs from 13/03/1997</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>20.631451</td>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>1.348442</td>\n",
       "      <td>103.746379</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...</td>\n",
       "      <td>0.360454</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>13.748672</td>\n",
       "      <td>Suburban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8214</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>henderson industrial park</td>\n",
       "      <td>henderson road</td>\n",
       "      <td>bukit merah</td>\n",
       "      <td>resale</td>\n",
       "      <td>1788000.0</td>\n",
       "      <td>9824.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>freehold</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>34.987179</td>\n",
       "      <td>HENDERSON ROAD</td>\n",
       "      <td>1.282146</td>\n",
       "      <td>103.819361</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>Queenstown, Tiong Bahru</td>\n",
       "      <td>0.871885</td>\n",
       "      <td>Central Core</td>\n",
       "      <td>3.575114</td>\n",
       "      <td>CBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8215</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>trivex</td>\n",
       "      <td>burn road</td>\n",
       "      <td>toa payoh</td>\n",
       "      <td>resale</td>\n",
       "      <td>726000.0</td>\n",
       "      <td>8067.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60 yrs from 20/05/2008</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>33.101852</td>\n",
       "      <td>BURN ROAD</td>\n",
       "      <td>1.301155</td>\n",
       "      <td>103.786307</td>\n",
       "      <td>TOA PAYOH</td>\n",
       "      <td>Macpherson, Braddell</td>\n",
       "      <td>0.201015</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>7.514296</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>excalibur centre</td>\n",
       "      <td>ubi crescent</td>\n",
       "      <td>geylang</td>\n",
       "      <td>resale</td>\n",
       "      <td>708000.0</td>\n",
       "      <td>5168.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>60 yrs from 05/07/1997</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>22.841849</td>\n",
       "      <td>UBI CRESCENT</td>\n",
       "      <td>1.296707</td>\n",
       "      <td>103.803661</td>\n",
       "      <td>GEYLANG</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.321296</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>5.523142</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>vertex</td>\n",
       "      <td>ubi avenue 3</td>\n",
       "      <td>geylang</td>\n",
       "      <td>resale</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>6061.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>60 yrs from 01/01/2007</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>...</td>\n",
       "      <td>25.693603</td>\n",
       "      <td>UBI AVENUE 3</td>\n",
       "      <td>1.331528</td>\n",
       "      <td>103.866315</td>\n",
       "      <td>GEYLANG</td>\n",
       "      <td>Geylang, Eunos</td>\n",
       "      <td>0.305073</td>\n",
       "      <td>Rest Central</td>\n",
       "      <td>5.609937</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>2025-01</td>\n",
       "      <td>primz bizhub</td>\n",
       "      <td>woodlands close</td>\n",
       "      <td>woodlands</td>\n",
       "      <td>resale</td>\n",
       "      <td>775000.0</td>\n",
       "      <td>6458.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60 yrs from 27/09/2011</td>\n",
       "      <td>multiple-user factory</td>\n",
       "      <td>...</td>\n",
       "      <td>26.630787</td>\n",
       "      <td>WOODLANDS CLOSE</td>\n",
       "      <td>1.442340</td>\n",
       "      <td>103.796633</td>\n",
       "      <td>WOODLANDS</td>\n",
       "      <td>Kranji, Woodgrove</td>\n",
       "      <td>0.521949</td>\n",
       "      <td>Outside Central</td>\n",
       "      <td>18.706665</td>\n",
       "      <td>Suburban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8219 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Month Year                  Project Name                   Street Name  \\\n",
       "0       2021-01                         wave9  woodlands industrial park e9   \n",
       "1       2021-01               cattel building             alexandra terrace   \n",
       "2       2021-01  lam soon industrial building               hillview avenue   \n",
       "3       2021-01                  synergy @ kb             kaki bukit road 4   \n",
       "4       2021-01                   wcega tower          bukit batok crescent   \n",
       "...         ...                           ...                           ...   \n",
       "8214    2025-01     henderson industrial park                henderson road   \n",
       "8215    2025-01                        trivex                     burn road   \n",
       "8216    2025-01              excalibur centre                  ubi crescent   \n",
       "8217    2025-01                        vertex                  ubi avenue 3   \n",
       "8218    2025-01                  primz bizhub               woodlands close   \n",
       "\n",
       "     Planning Area Type of Sale  Transacted Price ($)  Unit Price ($ PSM)  \\\n",
       "0        woodlands       resale             1108888.0              3772.0   \n",
       "1       queenstown       resale             3800000.0              3007.0   \n",
       "2      bukit batok       resale             1510900.0              6243.0   \n",
       "3            bedok       resale              458000.0              3368.0   \n",
       "4      bukit batok       resale              800000.0              4372.0   \n",
       "...            ...          ...                   ...                 ...   \n",
       "8214   bukit merah       resale             1788000.0              9824.0   \n",
       "8215     toa payoh       resale              726000.0              8067.0   \n",
       "8216       geylang       resale              708000.0              5168.0   \n",
       "8217       geylang       resale              800000.0              6061.0   \n",
       "8218     woodlands       resale              775000.0              6458.0   \n",
       "\n",
       "      Area (SQM)                  Tenure          Property Type  ...  \\\n",
       "0          294.0  30 yrs from 05/06/2014  multiple-user factory  ...   \n",
       "1         1264.0                freehold  multiple-user factory  ...   \n",
       "2          242.0                freehold  multiple-user factory  ...   \n",
       "3          136.0  30 yrs from 20/01/2012  multiple-user factory  ...   \n",
       "4          183.0  60 yrs from 13/03/1997  multiple-user factory  ...   \n",
       "...          ...                     ...                    ...  ...   \n",
       "8214       182.0                freehold  multiple-user factory  ...   \n",
       "8215        90.0  60 yrs from 20/05/2008  multiple-user factory  ...   \n",
       "8216       137.0  60 yrs from 05/07/1997  multiple-user factory  ...   \n",
       "8217       132.0  60 yrs from 01/01/2007              warehouse  ...   \n",
       "8218       120.0  60 yrs from 27/09/2011  multiple-user factory  ...   \n",
       "\n",
       "     rental_rate_psm_avg             Street_Name_Clean  latitude   longitude  \\\n",
       "0              18.048575  WOODLANDS INDUSTRIAL PARK E9  1.440807  103.771032   \n",
       "1              17.101793             ALEXANDRA TERRACE  1.291067  103.819738   \n",
       "2              24.876550               HILLVIEW AVENUE  1.355502  103.761731   \n",
       "3              17.091095             KAKI BUKIT ROAD 4  1.341884  103.956072   \n",
       "4              20.631451          BUKIT BATOK CRESCENT  1.348442  103.746379   \n",
       "...                  ...                           ...       ...         ...   \n",
       "8214           34.987179                HENDERSON ROAD  1.282146  103.819361   \n",
       "8215           33.101852                     BURN ROAD  1.301155  103.786307   \n",
       "8216           22.841849                  UBI CRESCENT  1.296707  103.803661   \n",
       "8217           25.693603                  UBI AVENUE 3  1.331528  103.866315   \n",
       "8218           26.630787               WOODLANDS CLOSE  1.442340  103.796633   \n",
       "\n",
       "      Planning_Area_Clean                                   General_Location  \\\n",
       "0               WOODLANDS                                  Yishun, Sembawang   \n",
       "1              QUEENSTOWN  Pasir Panjang, Hong Leong Garden, Clementi New...   \n",
       "2             BUKIT BATOK  Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...   \n",
       "3                   BEDOK                                     Geylang, Eunos   \n",
       "4             BUKIT BATOK  Hillview, Dairy Farm, Bukit Panjang, Choa Chu ...   \n",
       "...                   ...                                                ...   \n",
       "8214          BUKIT MERAH                            Queenstown, Tiong Bahru   \n",
       "8215            TOA PAYOH                               Macpherson, Braddell   \n",
       "8216              GEYLANG                                     Geylang, Eunos   \n",
       "8217              GEYLANG                                     Geylang, Eunos   \n",
       "8218            WOODLANDS                                  Kranji, Woodgrove   \n",
       "\n",
       "      Distance_to_MRT_km  Region_Classification  Distance_to_CBD_km  \\\n",
       "0               0.982056        Outside Central           19.666080   \n",
       "1               0.365268           Central Core            3.634989   \n",
       "2               0.826301        Outside Central           12.807806   \n",
       "3               0.334138           Rest Central           13.325616   \n",
       "4               0.360454        Outside Central           13.748672   \n",
       "...                  ...                    ...                 ...   \n",
       "8214            0.871885           Central Core            3.575114   \n",
       "8215            0.201015           Rest Central            7.514296   \n",
       "8216            0.321296           Rest Central            5.523142   \n",
       "8217            0.305073           Rest Central            5.609937   \n",
       "8218            0.521949        Outside Central           18.706665   \n",
       "\n",
       "      Urban_Classification  \n",
       "0                 Suburban  \n",
       "1                      CBD  \n",
       "2                 Suburban  \n",
       "3                 Suburban  \n",
       "4                 Suburban  \n",
       "...                    ...  \n",
       "8214                   CBD  \n",
       "8215                 Urban  \n",
       "8216                 Urban  \n",
       "8217                 Urban  \n",
       "8218              Suburban  \n",
       "\n",
       "[8219 rows x 49 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e7c166-e20d-4ebb-88dd-be20018d2faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Month Year', 'Project Name', 'Street Name', 'Planning Area', 'Type of Sale', 'Transacted Price ($)', 'Unit Price ($ PSM)', 'Area (SQM)', 'Tenure', 'Property Type', 'Type of Area', 'Floor Level', 'Region', 'Postal Sector', 'Postal District_x', 'Floor_Low', 'Floor_High', 'Floor_Midpoint', 'Is_Basement', 'Is_Ground', 'Floor_Category', 'Tenure_Type', 'Contract_date_missing', 'Contract_year', 'Contract_month', 'Contract_quarter', 'Contract_dayofweek', 'annual_rental_income_yield', 'monthly_rental_price_yield', 'rental_rate_psm_yield', 'tenure_based_yield', 'annual_rental_income_tenure', 'monthly_rental_price_tenure', 'rental_rate_psm_tenure', 'market_rent_rate_psm', 'monthly_rental_price_market', 'annual_rental_income_market', 'implied_yield_market', 'monthly_rental_price_avg', 'rental_rate_psm_avg', 'Street_Name_Clean', 'latitude', 'longitude', 'Planning_Area_Clean', 'General_Location', 'Distance_to_MRT_km', 'Region_Classification', 'Distance_to_CBD_km', 'Urban_Classification']\n"
     ]
    }
   ],
   "source": [
    "print(industrial_enriched.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff89be21-1745-46ab-af11-a687f1e3f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: ['Month Year', 'Project Name', 'Street Name', 'Planning Area', 'Type of Sale', 'Tenure', 'Property Type', 'Type of Area', 'Floor Level', 'Region', 'Floor_Category', 'Tenure_Type', 'Street_Name_Clean', 'Planning_Area_Clean', 'General_Location', 'Region_Classification', 'Urban_Classification']\n",
      "Numerical columns: ['Transacted Price ($)', 'Unit Price ($ PSM)', 'Area (SQM)', 'Postal Sector', 'Postal District_x', 'Floor_Low', 'Floor_High', 'Floor_Midpoint', 'Is_Basement', 'Is_Ground', 'Contract_date_missing', 'Contract_year', 'Contract_month', 'Contract_quarter', 'Contract_dayofweek', 'annual_rental_income_yield', 'monthly_rental_price_yield', 'rental_rate_psm_yield', 'tenure_based_yield', 'annual_rental_income_tenure', 'monthly_rental_price_tenure', 'rental_rate_psm_tenure', 'market_rent_rate_psm', 'monthly_rental_price_market', 'annual_rental_income_market', 'implied_yield_market', 'monthly_rental_price_avg', 'rental_rate_psm_avg', 'latitude', 'longitude', 'Distance_to_MRT_km', 'Distance_to_CBD_km']\n",
      "Training set: (6575, 1790)\n",
      "Testing set: (1644, 1790)\n",
      "\n",
      "Baseline Model (Mean Prediction):\n",
      "MAE: $1,872,074.27\n",
      "RMSE: $7,018,482.58\n"
     ]
    }
   ],
   "source": [
    "target_column='Transacted Price ($)'\n",
    "\n",
    "feature_columns = [col for col in industrial_enriched.columns]\n",
    "categorical_columns = industrial_enriched[feature_columns].select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = industrial_enriched[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "\n",
    "print(f\"\\nCategorical columns: {list(categorical_columns)}\")\n",
    "print(f\"Numerical columns: {list(numerical_columns)}\")\n",
    "\n",
    "X_encoded = pd.get_dummies(industrial_enriched[feature_columns], columns=categorical_columns, drop_first=True)\n",
    "y = industrial_enriched[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")\n",
    "\n",
    "# Create a simple baseline (predict mean)\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "\n",
    "print(f\"\\nBaseline Model (Mean Prediction):\")\n",
    "print(f\"MAE: ${baseline_mae:,.2f}\")\n",
    "print(f\"RMSE: ${baseline_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c2c5319-b267-4954-abc9-93a6db03189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month Year                      object\n",
       "Project Name                    object\n",
       "Street Name                     object\n",
       "Planning Area                   object\n",
       "Type of Sale                    object\n",
       "Transacted Price ($)           float64\n",
       "Unit Price ($ PSM)             float64\n",
       "Area (SQM)                     float64\n",
       "Tenure                          object\n",
       "Property Type                   object\n",
       "Type of Area                    object\n",
       "Floor Level                     object\n",
       "Region                          object\n",
       "Postal Sector                  float64\n",
       "Postal District_x              float64\n",
       "Floor_Low                        int64\n",
       "Floor_High                       int64\n",
       "Floor_Midpoint                   int64\n",
       "Is_Basement                      int64\n",
       "Is_Ground                        int64\n",
       "Floor_Category                  object\n",
       "Tenure_Type                     object\n",
       "Contract_date_missing            int32\n",
       "Contract_year                  float64\n",
       "Contract_month                 float64\n",
       "Contract_quarter               float64\n",
       "Contract_dayofweek             float64\n",
       "annual_rental_income_yield     float64\n",
       "monthly_rental_price_yield     float64\n",
       "rental_rate_psm_yield          float64\n",
       "tenure_based_yield             float64\n",
       "annual_rental_income_tenure    float64\n",
       "monthly_rental_price_tenure    float64\n",
       "rental_rate_psm_tenure         float64\n",
       "market_rent_rate_psm           float64\n",
       "monthly_rental_price_market    float64\n",
       "annual_rental_income_market    float64\n",
       "implied_yield_market           float64\n",
       "monthly_rental_price_avg       float64\n",
       "rental_rate_psm_avg            float64\n",
       "Street_Name_Clean               object\n",
       "latitude                       float64\n",
       "longitude                      float64\n",
       "Planning_Area_Clean             object\n",
       "General_Location                object\n",
       "Distance_to_MRT_km             float64\n",
       "Region_Classification           object\n",
       "Distance_to_CBD_km             float64\n",
       "Urban_Classification            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industrial_enriched.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30ad09f-de3b-4454-9461-c29227da4102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      "1843\n",
      "Missing values in target:\n",
      "0\n",
      "Missing values after imputation - Train: 0\n",
      "Missing values after imputation - Test: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in features:\")\n",
    "print(X_encoded.isnull().sum().sum())\n",
    "print(\"Missing values in target:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(X, strategy='mean'):\n",
    "    \"\"\"Handle missing values in the feature matrix\"\"\"\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    return pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "\n",
    "# Apply to training and test data\n",
    "X_train_imputed = handle_missing_values(X_train)\n",
    "X_test_imputed = handle_missing_values(X_test)\n",
    "\n",
    "print(f\"Missing values after imputation - Train: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after imputation - Test: {X_test_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8991d5-2652-4a4e-8f25-a3f7fd195bfb",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54946741-8057-4c21-836e-07768a952427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zafee\\anaconda3\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:18: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "Random Forest Results:\n",
      "  MAE: $15,307.96\n",
      "  RMSE: $232,879.63\n",
      "  RÂ²: 0.9989\n",
      "  Improvement over baseline: 99.2%\n",
      "\n",
      "Training Hist Gradient Boosting...\n",
      "Hist Gradient Boosting Results:\n",
      "  MAE: $157,588.02\n",
      "  RMSE: $1,781,788.92\n",
      "  RÂ²: 0.9355\n",
      "  Improvement over baseline: 91.6%\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost Results:\n",
      "  MAE: $116,894.38\n",
      "  RMSE: $1,765,213.05\n",
      "  RÂ²: 0.9367\n",
      "  Improvement over baseline: 93.8%\n",
      "\n",
      " Best Model: Random Forest\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor  # Make sure xgboost is installed too\n",
    "import numpy as np\n",
    "\n",
    "# Updated models with proper data handling\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        max_iter=100\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        random_state=42,\n",
    "        enable_categorical=False  # Ensure this is False for one-hot encoded data\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models with proper data splits\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    try:\n",
    "            \n",
    "        if name == 'Hist Gradient Boosting':\n",
    "            # Use imputed data for HistGradientBoosting\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "            \n",
    "        else:\n",
    "            # Use imputed data for other models\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            y_test_compare = y_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test_compare, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_compare, y_pred))\n",
    "        r2 = r2_score(y_test_compare, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'predictions': y_pred,\n",
    "            'y_test': y_test_compare\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  MAE: ${mae:,.2f}\")\n",
    "        print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "        print(f\"  RÂ²: {r2:.4f}\")\n",
    "        print(f\"  Improvement over baseline: {((baseline_mae - mae) / baseline_mae * 100):.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Find best model\n",
    "if results:\n",
    "    best_model_name = min(results.keys(), key=lambda x: results[x]['mae'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\n Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853f3ba-c4bd-4037-a471-f6de893efb1e",
   "metadata": {},
   "source": [
    "## Hypertune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61a68ab5-2515-49af-8292-1a391bc6408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced training pipeline...\n",
      "Starting Enhanced Model Training Pipeline\n",
      "============================================================\n",
      "Training enhanced models with improved configurations...\n",
      "============================================================\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Test MAE: $15,307.96\n",
      "Test RMSE: $232,879.63\n",
      "Test RÂ²: 0.9989\n",
      "CV MAE: $24,634.07 Â± $10,994.80\n",
      "Improvement over baseline: +99.2%\n",
      "\n",
      "--- Training Hist Gradient Boosting ---\n",
      "Test MAE: $152,104.53\n",
      "Test RMSE: $1,563,124.16\n",
      "Test RÂ²: 0.9504\n",
      "CV MAE: $214,496.84 Â± $57,549.58\n",
      "Improvement over baseline: +91.9%\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $100,050.47\n",
      "Test RMSE: $1,609,125.15\n",
      "Test RÂ²: 0.9474\n",
      "CV MAE: $147,347.56 Â± $66,857.82\n",
      "Improvement over baseline: +94.7%\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $21,981.55\n",
      "Test RMSE: $306,766.35\n",
      "Test RÂ²: 0.9981\n",
      "CV MAE: $24,498.84 Â± $10,083.98\n",
      "Improvement over baseline: +98.8%\n",
      "\n",
      "--- Training CatBoost ---\n",
      "Test MAE: $131,640.70\n",
      "Test RMSE: $1,165,682.62\n",
      "Test RÂ²: 0.9724\n",
      "CV MAE: $184,756.31 Â± $45,004.02\n",
      "Improvement over baseline: +93.0%\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Model Ranking (by MAE):\n",
      " 1. Random Forest             | MAE: $15,307.96 | RÂ²: 0.9989 | CV: $24,634.07 Â± $10,994.80\n",
      " 2. Gradient Boosting         | MAE: $21,981.55 | RÂ²: 0.9981 | CV: $24,498.84 Â± $10,083.98\n",
      " 3. XGBoost                   | MAE: $100,050.47 | RÂ²: 0.9474 | CV: $147,347.56 Â± $66,857.82\n",
      " 4. CatBoost                  | MAE: $131,640.70 | RÂ²: 0.9724 | CV: $184,756.31 Â± $45,004.02\n",
      " 5. Hist Gradient Boosting    | MAE: $152,104.53 | RÂ²: 0.9504 | CV: $214,496.84 Â± $57,549.58\n",
      "\n",
      "--- Error Analysis for Random Forest ---\n",
      "Mean Absolute Error: $15,307.96\n",
      "Median Absolute Error: $67.72\n",
      "Max Error: $7,265,100.00\n",
      "Error Std: $232,446.67\n",
      "\n",
      "Error Percentage Stats:\n",
      "  Mean: 3.8%\n",
      "  Median: 0.0%\n",
      "  95th percentile: 0.3%\n",
      "\n",
      "Top 5 Largest Errors:\n",
      "  Actual: $57,760,000 | Predicted: $59,226,370 | Error: 2.5%\n",
      "  Actual: $58,000,000 | Predicted: $59,671,390 | Error: 2.9%\n",
      "  Actual: $135,200,000 | Predicted: $137,743,000 | Error: 1.9%\n",
      "  Actual: $74,000,000 | Predicted: $69,397,268 | Error: 6.2%\n",
      "  Actual: $131,000,000 | Predicted: $138,265,100 | Error: 5.5%\n",
      "\n",
      "Creating ensemble from: ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
      "Ensemble weights:\n",
      "  Random Forest: 0.541\n",
      "  Gradient Boosting: 0.377\n",
      "  XGBoost: 0.083\n",
      "\n",
      "Ensemble Results:\n",
      "MAE: $18,159.92\n",
      "RMSE: $249,348.16\n",
      "RÂ²: 0.9987\n",
      "Ensemble vs Best Single Model: -18.6% improvement\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      " RECOMMENDATION: USE Random Forest\n",
      "   MAE: $15,307.96\n",
      "   RÂ²: 0.9989\n",
      "   CV Consistency: $24,634.07 Â± $10,994.80\n",
      "Final model type: RandomForestRegressor\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Enhanced model training with better validation and ensemble options\"\"\"\n",
    "    \n",
    "    print(\"Training enhanced models with improved configurations...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Enhanced models with optimized hyperparameters\n",
    "    models = {\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            # max_depth=20,\n",
    "            # min_samples_split=10,\n",
    "            # min_samples_leaf=4,\n",
    "            # max_features='sqrt',\n",
    "            # bootstrap=True,\n",
    "            random_state=42,\n",
    "            # n_jobs=-1\n",
    "        ),\n",
    "        'Hist Gradient Boosting': HistGradientBoostingRegressor(\n",
    "            max_iter=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_leaf=20,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add CatBoost if you have categorical features\n",
    "    try:\n",
    "        models['CatBoost'] = CatBoostRegressor(\n",
    "            iterations=200,\n",
    "            depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    except:\n",
    "        print(\"CatBoost not available, skipping...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_imputed, y_train)\n",
    "            y_pred = model.predict(X_test_imputed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation for robustness\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_imputed, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RMSE: ${rmse:,.2f}\")\n",
    "            print(f\"Test RÂ²: {r2:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} Â± ${cv_std:,.2f}\")\n",
    "            \n",
    "            if baseline_mae:\n",
    "                improvement = ((baseline_mae - mae) / baseline_mae * 100)\n",
    "                print(f\"Improvement over baseline: {improvement:+.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Create intelligent ensemble from best performing models\"\"\"\n",
    "    \n",
    "    # Get top 3 models by MAE\n",
    "    valid_models = {k: v for k, v in results.items() if 'mae' in v}\n",
    "    if len(valid_models) < 2:\n",
    "        print(\"Not enough models for ensemble\")\n",
    "        return None, None\n",
    "    \n",
    "    top_models = sorted(valid_models.items(), key=lambda x: x[1]['mae'])[:3]\n",
    "    \n",
    "    print(f\"\\nCreating ensemble from: {[name for name, _ in top_models]}\")\n",
    "    \n",
    "    # Create weighted ensemble based on performance\n",
    "    ensemble_models = []\n",
    "    weights = []\n",
    "    \n",
    "    for name, result in top_models:\n",
    "        ensemble_models.append((name, result['model']))\n",
    "        # Weight inversely proportional to MAE (better models get higher weight)\n",
    "        weight = 1.0 / result['mae']\n",
    "        weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w/total_weight for w in weights]\n",
    "    \n",
    "    print(\"Ensemble weights:\")\n",
    "    for (name, _), weight in zip(ensemble_models, normalized_weights):\n",
    "        print(f\"  {name}: {weight:.3f}\")\n",
    "    \n",
    "    # Create voting regressor with weights\n",
    "    ensemble = VotingRegressor(\n",
    "        estimators=ensemble_models,\n",
    "        weights=normalized_weights\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.fit(X_train_imputed, y_train)\n",
    "    y_pred_ensemble = ensemble.predict(X_test_imputed)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    mae_ens = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "    rmse_ens = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "    r2_ens = r2_score(y_test, y_pred_ensemble)\n",
    "    \n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"MAE: ${mae_ens:,.2f}\")\n",
    "    print(f\"RMSE: ${rmse_ens:,.2f}\")\n",
    "    print(f\"RÂ²: {r2_ens:.4f}\")\n",
    "    \n",
    "    # Compare with best single model\n",
    "    best_single_mae = top_models[0][1]['mae']\n",
    "    improvement = ((best_single_mae - mae_ens) / best_single_mae) * 100\n",
    "    \n",
    "    print(f\"Ensemble vs Best Single Model: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    return ensemble, mae_ens\n",
    "\n",
    "def analyze_model_performance(results, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Comprehensive model performance analysis\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort models by MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mae'])\n",
    "    \n",
    "    print(\"\\nModel Ranking (by MAE):\")\n",
    "    for i, (name, result) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i:2d}. {name:25} | MAE: ${result['mae']:,.2f} | RÂ²: {result['r2']:.4f} | CV: ${result['cv_mae']:,.2f} Â± ${result['cv_std']:,.2f}\")\n",
    "    \n",
    "    best_model_name, best_result = sorted_results[0]\n",
    "    \n",
    "    # Error analysis for best model\n",
    "    print(f\"\\n--- Error Analysis for {best_model_name} ---\")\n",
    "    y_pred_best = best_result['predictions']\n",
    "    errors = np.abs(y_test - y_pred_best)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: ${errors.mean():,.2f}\")\n",
    "    print(f\"Median Absolute Error: ${np.median(errors):,.2f}\")\n",
    "    print(f\"Max Error: ${errors.max():,.2f}\")\n",
    "    print(f\"Error Std: ${errors.std():,.2f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_pct = (errors / y_test) * 100\n",
    "    print(f\"\\nError Percentage Stats:\")\n",
    "    print(f\"  Mean: {error_pct.mean():.1f}%\")\n",
    "    print(f\"  Median: {error_pct.median():.1f}%\")\n",
    "    print(f\"  95th percentile: {np.percentile(error_pct, 95):.1f}%\")\n",
    "    \n",
    "    # Worst predictions\n",
    "    worst_indices = np.argsort(errors)[-5:]\n",
    "    print(f\"\\nTop 5 Largest Errors:\")\n",
    "    for idx in worst_indices:\n",
    "        actual = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]\n",
    "        predicted = y_pred_best[idx]\n",
    "        error = abs(actual - predicted)\n",
    "        error_pct = (error / actual) * 100\n",
    "        print(f\"  Actual: ${actual:,.0f} | Predicted: ${predicted:,.0f} | Error: {error_pct:.1f}%\")\n",
    "    \n",
    "    return best_model_name, best_result\n",
    "\n",
    "# Main execution function\n",
    "def run_enhanced_training(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae=None):\n",
    "    \"\"\"Complete enhanced training pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting Enhanced Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Train individual models\n",
    "    results = train_enhanced_models(X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No models trained successfully!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. Analyze performance\n",
    "    best_model_name, best_result = analyze_model_performance(results, X_test_imputed, y_test)\n",
    "    \n",
    "    # 3. Create ensemble\n",
    "    ensemble_model, ensemble_mae = create_smart_ensemble(results, X_train_imputed, y_train, X_test_imputed, y_test)\n",
    "    \n",
    "    # 4. Final recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if ensemble_model and ensemble_mae < best_result['mae']:\n",
    "        print(\" RECOMMENDATION: USE ENSEMBLE MODEL\")\n",
    "        print(f\"   Ensemble MAE: ${ensemble_mae:,.2f}\")\n",
    "        print(f\"   Best Single MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Improvement: {((best_result['mae'] - ensemble_mae) / best_result['mae'] * 100):+.1f}%\")\n",
    "        final_model = ensemble_model\n",
    "    else:\n",
    "        print(f\" RECOMMENDATION: USE {best_model_name}\")\n",
    "        print(f\"   MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   RÂ²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV Consistency: ${best_result['cv_mae']:,.2f} Â± ${best_result['cv_std']:,.2f}\")\n",
    "        final_model = best_result['model']\n",
    "    \n",
    "    return final_model, results\n",
    "\n",
    "# Usage - replace your existing training code with this:\n",
    "print(\"Running enhanced training pipeline...\")\n",
    "final_model, all_results = run_enhanced_training(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, baseline_mae\n",
    ")\n",
    "\n",
    "# You can then use final_model for predictions\n",
    "if final_model:\n",
    "    #print(\"\\nTraining completed successfully!\")\n",
    "    print(f\"Final model type: {type(final_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbedd97-f8e9-4444-87cb-640e649f9f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking available variables...\n",
      "No feature_names found, using generic names\n",
      "\n",
      "Running comprehensive analysis...\n",
      "COMPREHENSIVE MODEL ANALYSIS PIPELINE\n",
      "============================================================\n",
      "INVESTIGATING DATA ISSUES\n",
      "============================================================\n",
      "1. Checking for data leakage...\n",
      "   Using generic feature names: 1790 features\n",
      "Top feature correlations with target:\n",
      "target       1.0\n",
      "feature_0    NaN\n",
      "feature_1    NaN\n",
      "feature_2    NaN\n",
      "feature_3    NaN\n",
      "feature_4    NaN\n",
      "feature_5    NaN\n",
      "feature_6    NaN\n",
      "feature_7    NaN\n",
      "feature_8    NaN\n",
      "Name: target, dtype: float64\n",
      "\n",
      "2. Train-Test overlap check:\n",
      "   Train size: (6575, 1790)\n",
      "   Test size: (1644, 1790)\n",
      "\n",
      "3. Target variable analysis:\n",
      "   Train - Min: $20,535, Max: $142,000,000, Mean: $2,024,662\n",
      "   Test  - Min: $797, Max: $135,200,000, Mean: $1,947,991\n",
      "\n",
      "4. Constant features: 1541 out of 1790\n",
      "\n",
      "TRAINING ROBUST MODELS WITH SAFEGUARDS\n",
      "============================================================\n",
      "\n",
      "--- Training Gradient Boosting (Conservative) ---\n",
      "Test MAE: $65,644.29\n",
      "Test RÂ²: 0.9891\n",
      "Train RÂ²: 0.9914\n",
      "Overfit Gap: 0.0024\n",
      "CV MAE: $82,896.01 Â± $26,951.32\n",
      "\n",
      "--- Training XGBoost (Conservative) ---\n",
      "Error training XGBoost (Conservative): XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'\n",
      "\n",
      "--- Training Random Forest (Conservative) ---\n",
      "Test MAE: $15,307.96\n",
      "Test RÂ²: 0.9989\n",
      "Train RÂ²: 0.9997\n",
      "Overfit Gap: 0.0008\n",
      "CV MAE: $24,634.07 Â± $10,994.80\n",
      "\n",
      "============================================================\n",
      "ROBUST MODEL COMPARISON\n",
      "============================================================\n",
      "Gradient Boosting (Conservative)    | MAE: $65,644.29 | RÂ²: 0.9891 | Overfit: 0.0024\n",
      "Random Forest (Conservative)        | MAE: $15,307.96 | RÂ²: 0.9989 | Overfit: 0.0008\n",
      "\n",
      "Best Robust Model: Random Forest (Conservative)\n",
      "\n",
      "REALITY CHECKS\n",
      "============================================================\n",
      "1. Prediction Range Check:\n",
      "   Actual Min: $797\n",
      "   Actual Max: $135,200,000\n",
      "   Predicted Min: $49,410\n",
      "   Predicted Max: $138,265,100\n",
      "2. Unique predictions: 1283\n",
      "3. Standard Deviation Check:\n",
      "   Actual Std: $7,020,199\n",
      "   Predicted Std: $7,130,443\n",
      "   Ratio: 1.016\n",
      "4. Business Sense Check:\n",
      "   Negative predictions: 0\n",
      "\n",
      "============================================================\n",
      "COMPARISON: ORIGINAL vs ROBUST\n",
      "============================================================\n",
      "Original Gradient Boosting:\n",
      "  MAE: $62,707.10, RÂ²: 0.9997\n",
      "Robust Model (Random Forest (Conservative)):\n",
      "  MAE: $15,307.96, RÂ²: 0.9989\n",
      "MAE Change: -75.6%\n",
      "RÂ² Change: -0.0008\n",
      "Results are consistent\n",
      "\n",
      "Overfitting Analysis:\n",
      "Original CV MAE: $668,723 vs Test MAE: $62,707\n",
      "Robust CV MAE: $24,634 vs Test MAE: $15,308\n"
     ]
    }
   ],
   "source": [
    "def investigate_data_issues(X_train_imputed, y_train, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Investigate potential data leakage and overfitting\"\"\"\n",
    "    \n",
    "    print(\"INVESTIGATING DATA ISSUES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Check for duplicate features or target leakage\n",
    "    print(\"1. Checking for data leakage...\")\n",
    "    \n",
    "    # Check if we have feature names, if not create generic ones\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'feature_{i}' for i in range(X_train_imputed.shape[1])]\n",
    "        print(f\"   Using generic feature names: {len(feature_names)} features\")\n",
    "    \n",
    "    # Check correlation between features and target\n",
    "    try:\n",
    "        train_df = pd.DataFrame(X_train_imputed, columns=feature_names)\n",
    "        train_df['target'] = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "        \n",
    "        # Find highly correlated features\n",
    "        correlations = train_df.corr()['target'].abs().sort_values(ascending=False)\n",
    "        print(\"Top feature correlations with target:\")\n",
    "        print(correlations.head(10))\n",
    "        \n",
    "        # Flag suspicious correlations (>0.95)\n",
    "        suspicious = correlations[correlations > 0.95]\n",
    "        if len(suspicious) > 1:  # More than just target itself\n",
    "            print(f\"SUSPICIOUS: {len(suspicious)} features with >0.95 correlation\")\n",
    "            print(suspicious)\n",
    "    except Exception as e:\n",
    "        print(f\"   Correlation analysis failed: {e}\")\n",
    "    \n",
    "    # 2. Check for train-test contamination\n",
    "    print(f\"\\n2. Train-Test overlap check:\")\n",
    "    print(f\"   Train size: {X_train_imputed.shape}\")\n",
    "    print(f\"   Test size: {X_test_imputed.shape}\")\n",
    "    \n",
    "    # 3. Check target variable distribution\n",
    "    print(f\"\\n3. Target variable analysis:\")\n",
    "    print(f\"   Train - Min: ${y_train.min():,.0f}, Max: ${y_train.max():,.0f}, Mean: ${y_train.mean():,.0f}\")\n",
    "    print(f\"   Test  - Min: ${y_test.min():,.0f}, Max: ${y_test.max():,.0f}, Mean: ${y_test.mean():,.0f}\")\n",
    "    \n",
    "    # 4. Check for constant or near-constant features\n",
    "    try:\n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "        selector = VarianceThreshold(threshold=0.01)\n",
    "        selector.fit(X_train_imputed)\n",
    "        n_features = X_train_imputed.shape[1]\n",
    "        n_constant = n_features - selector.get_support().sum()\n",
    "        print(f\"\\n4. Constant features: {n_constant} out of {n_features}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Constant feature check failed: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def train_robust_models(X_train_imputed, y_train, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Train models with safeguards against overfitting\"\"\"\n",
    "    \n",
    "    print(\"\\nTRAINING ROBUST MODELS WITH SAFEGUARDS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Remove potential leakage features\n",
    "    X_train_clean = X_train_imputed.copy()\n",
    "    X_test_clean = X_test_imputed.copy()\n",
    "    \n",
    "    # More conservative models to prevent overfitting\n",
    "    models = {\n",
    "        'Gradient Boosting (Conservative)': GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,  # Reduced depth\n",
    "            learning_rate=0.05,  # Lower learning rate\n",
    "            subsample=0.7,  # More aggressive subsampling\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost (Conservative)': XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "            reg_alpha=1.0,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Random Forest (Conservative)': RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            # max_depth=10,\n",
    "            # min_samples_split=20,\n",
    "            # min_samples_leaf=10,\n",
    "            # max_features=0.5,  # More feature sampling\n",
    "            # bootstrap=True,\n",
    "            random_state=42,\n",
    "            # n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            if 'XGB' in name:\n",
    "                model.fit(\n",
    "                    X_train_clean, y_train,\n",
    "                    eval_set=[(X_test_clean, y_test)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_train_clean, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_clean)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # More rigorous cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_clean, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Calculate train score for overfitting check\n",
    "            y_pred_train = model.predict(X_train_clean)\n",
    "            train_r2 = r2_score(y_train, y_pred_train)\n",
    "            \n",
    "            # Overfitting indicator\n",
    "            overfit_gap = train_r2 - r2\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'train_r2': train_r2,\n",
    "                'overfit_gap': overfit_gap,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RÂ²: {r2:.4f}\")\n",
    "            print(f\"Train RÂ²: {train_r2:.4f}\")\n",
    "            print(f\"Overfit Gap: {overfit_gap:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} Â± ${cv_std:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def validate_model_reality_check(model, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Perform reality checks on model predictions\"\"\"\n",
    "    \n",
    "    print(\"\\nREALITY CHECKS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y_pred = model.predict(X_test_imputed)\n",
    "    \n",
    "    # 1. Check prediction ranges\n",
    "    print(\"1. Prediction Range Check:\")\n",
    "    print(f\"   Actual Min: ${y_test.min():,.0f}\")\n",
    "    print(f\"   Actual Max: ${y_test.max():,.0f}\")\n",
    "    print(f\"   Predicted Min: ${y_pred.min():,.0f}\")\n",
    "    print(f\"   Predicted Max: ${y_pred.max():,.0f}\")\n",
    "    \n",
    "    # 2. Check for constant predictions\n",
    "    unique_predictions = len(np.unique(y_pred))\n",
    "    print(f\"2. Unique predictions: {unique_predictions}\")\n",
    "    if unique_predictions < 10:\n",
    "        print(\"   WARNING: Very few unique predictions - model might be degenerate\")\n",
    "    \n",
    "    # 3. Check prediction distribution\n",
    "    pred_std = y_pred.std()\n",
    "    actual_std = y_test.std() if hasattr(y_test, 'std') else np.std(y_test)\n",
    "    print(f\"3. Standard Deviation Check:\")\n",
    "    print(f\"   Actual Std: ${actual_std:,.0f}\")\n",
    "    print(f\"   Predicted Std: ${pred_std:,.0f}\")\n",
    "    print(f\"   Ratio: {pred_std/actual_std:.3f}\")\n",
    "    \n",
    "    # 4. Check if predictions make business sense\n",
    "    negative_predictions = (y_pred < 0).sum()\n",
    "    print(f\"4. Business Sense Check:\")\n",
    "    print(f\"   Negative predictions: {negative_predictions}\")\n",
    "    if negative_predictions > 0:\n",
    "        print(f\"   WARNING: {negative_predictions} negative price predictions\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# Main investigation pipeline\n",
    "def run_comprehensive_analysis(X_train_imputed, y_train, X_test_imputed, y_test, feature_names=None):\n",
    "    \"\"\"Run comprehensive analysis to identify and fix issues\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE MODEL ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Investigate data issues\n",
    "    investigate_data_issues(X_train_imputed, y_train, X_test_imputed, y_test, feature_names)\n",
    "    \n",
    "    # Step 2: Train robust models\n",
    "    robust_results = train_robust_models(X_train_imputed, y_train, X_test_imputed, y_test, feature_names)\n",
    "    \n",
    "    if not robust_results:\n",
    "        print(\"No robust models trained successfully\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Analyze robust results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ROBUST MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, result in robust_results.items():\n",
    "        print(f\"{name:35} | MAE: ${result['mae']:,.2f} | RÂ²: {result['r2']:.4f} | Overfit: {result['overfit_gap']:.4f}\")\n",
    "    \n",
    "    # Step 4: Reality check best model\n",
    "    best_robust_name = min(robust_results.keys(), key=lambda x: robust_results[x]['mae'])\n",
    "    best_robust_model = robust_results[best_robust_name]['model']\n",
    "    \n",
    "    print(f\"\\nBest Robust Model: {best_robust_name}\")\n",
    "    validate_model_reality_check(best_robust_model, X_test_imputed, y_test, feature_names)\n",
    "    \n",
    "    return best_robust_model, robust_results\n",
    "\n",
    "# First, let's check what feature names we have available\n",
    "print(\"Checking available variables...\")\n",
    "\n",
    "# Try to get feature_names from your environment\n",
    "try:\n",
    "    # If you have feature_names defined from previous steps\n",
    "    if 'feature_names' in locals():\n",
    "        print(f\"Found feature_names with {len(feature_names)} features\")\n",
    "        feature_names_to_use = feature_names\n",
    "    else:\n",
    "        print(\"No feature_names found, using generic names\")\n",
    "        feature_names_to_use = None\n",
    "except:\n",
    "    feature_names_to_use = None\n",
    "\n",
    "# Run the investigation\n",
    "print(\"\\nRunning comprehensive analysis...\")\n",
    "robust_model, robust_results = run_comprehensive_analysis(\n",
    "    X_train_imputed, y_train, X_test_imputed, y_test, feature_names_to_use\n",
    ")\n",
    "\n",
    "# Compare with original results\n",
    "if robust_model:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON: ORIGINAL vs ROBUST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    original_mae = 62707.10  # From your Gradient Boosting results\n",
    "    original_r2 = 0.9997\n",
    "    \n",
    "    # Get the best robust model results\n",
    "    best_robust_name = min(robust_results.keys(), key=lambda x: robust_results[x]['mae'])\n",
    "    robust_mae = robust_results[best_robust_name]['mae']\n",
    "    robust_r2 = robust_results[best_robust_name]['r2']\n",
    "    \n",
    "    print(f\"Original Gradient Boosting:\")\n",
    "    print(f\"  MAE: ${original_mae:,.2f}, RÂ²: {original_r2:.4f}\")\n",
    "    print(f\"Robust Model ({best_robust_name}):\")\n",
    "    print(f\"  MAE: ${robust_mae:,.2f}, RÂ²: {robust_r2:.4f}\")\n",
    "    \n",
    "    mae_change = ((robust_mae - original_mae) / original_mae) * 100\n",
    "    r2_change = robust_r2 - original_r2\n",
    "    \n",
    "    print(f\"MAE Change: {mae_change:+.1f}%\")\n",
    "    print(f\"RÂ² Change: {r2_change:+.4f}\")\n",
    "    \n",
    "    if abs(r2_change) > 0.1:\n",
    "        print(\"SIGNIFICANT DIFFERENCE: Original model likely overfitted\")\n",
    "    else:\n",
    "        print(\"Results are consistent\")\n",
    "        \n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Original CV MAE: $668,723 vs Test MAE: $62,707\")\n",
    "    print(f\"Robust CV MAE: ${robust_results[best_robust_name]['cv_mae']:,.0f} vs Test MAE: ${robust_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3854fa93-7dee-486d-8ea9-982db12a6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete data cleaning and model rebuilding...\n",
      "STARTING DATA CLEANING AND MODEL REBUILDING\n",
      "============================================================\n",
      "CLEANING DATASET AND REBUILDING MODELS\n",
      "============================================================\n",
      "1. Removing constant features...\n",
      "   Removed 1541 constant features\n",
      "   Remaining features: 249\n",
      "2. Removing highly correlated features...\n",
      "   Removed 81 highly correlated features\n",
      "   Final features: 168\n",
      "\n",
      "3. TRAINING MODELS ON CLEANED DATA\n",
      "============================================================\n",
      "\n",
      "--- Training Gradient Boosting ---\n",
      "Test MAE: $36,246.89\n",
      "Test RÂ²: 0.9928\n",
      "Train RÂ²: 0.9963\n",
      "Overfit Gap: 0.0035\n",
      "CV MAE: $58,011.48 Â± $22,996.08\n",
      "\n",
      "--- Training XGBoost ---\n",
      "Test MAE: $103,306.94\n",
      "Test RÂ²: 0.9621\n",
      "Train RÂ²: 1.0000\n",
      "Overfit Gap: 0.0379\n",
      "CV MAE: $148,915.00 Â± $51,385.65\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Test MAE: $404,084.43\n",
      "Test RÂ²: 0.8409\n",
      "Train RÂ²: 0.8429\n",
      "Overfit Gap: 0.0020\n",
      "CV MAE: $461,822.77 Â± $88,381.79\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL RESULTS\n",
      "============================================================\n",
      "Gradient Boosting    | MAE: $36,246.89 | RÂ²: 0.9928 | Overfit: 0.0035\n",
      "XGBoost              | MAE: $103,306.94 | RÂ²: 0.9621 | Overfit: 0.0379\n",
      "Random Forest        | MAE: $404,084.43 | RÂ²: 0.8409 | Overfit: 0.0020\n",
      "\n",
      "BEST FINAL MODEL: Gradient Boosting\n",
      "   Test MAE: $36,246.89\n",
      "   Test RÂ²: 0.9928\n",
      "   CV MAE: $58,011.48 Â± $22,996.08\n",
      "   Overfit Gap: 0.0035\n",
      "\n",
      "4. FEATURE IMPORTANCE ANALYSIS (Top 20)\n",
      "============================================================\n",
      "         feature    importance\n",
      "0      feature_0  9.840183e-01\n",
      "11    feature_11  9.131890e-03\n",
      "8      feature_8  5.325407e-03\n",
      "2      feature_2  8.835328e-04\n",
      "4      feature_4  2.759252e-04\n",
      "3      feature_3  7.477626e-05\n",
      "9      feature_9  6.902562e-05\n",
      "7      feature_7  6.327545e-05\n",
      "13    feature_13  5.028323e-05\n",
      "1      feature_1  4.550116e-05\n",
      "5      feature_5  2.545018e-05\n",
      "12    feature_12  2.072693e-05\n",
      "62    feature_62  6.261541e-06\n",
      "150  feature_150  3.228901e-06\n",
      "163  feature_163  2.712721e-06\n",
      "32    feature_32  1.326497e-06\n",
      "118  feature_118  1.128187e-06\n",
      "10    feature_10  5.307312e-07\n",
      "155  feature_155  3.444975e-07\n",
      "136  feature_136  1.698774e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMWCAYAAADs4eXxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlu9JREFUeJzs3Ql4lOW5//E7rFL2sMguO4LsgiyCaI8oCLYpR+EgFFAEKgKWpbZYaUG2UqTK8bA2h11BQCkUBCoIIksRESotm4BIa0EQylZWIf/rd59r5j+JgQTIm0zC93Ndc5KZefPOM8O0Pb/3vp/niUlISEgwAAAAAACQ5rKl/SkBAAAAAIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbABAVYmJiUnVbu3ZtoOP4+9//bsOGDbP77rvPChcubEWLFrUHH3zQVq1alezxJ0+etJ49e1qxYsUsb9689tBDD9mnn36aqtfSefWeqlSpkuzz77//fvh9L1y40ILw3nvv2dChQ1N9vMZcs2ZNy6z++c9/+vvdvn174K917tw5f63Ufmd13LW+9//1X/8VyBh37tzpYzx48GAg5wcAmOXI6AEAACCzZ89OdH/WrFkeOpM+Xr169UDHsXjxYhszZozFxcVZ165d7dtvv/WxtGzZ0qZNm2ZPP/10+NirV69amzZt7C9/+Yv97Gc/84A+ceJED6Zbt269ZpiOdMcdd9i+ffvs448/9qAf6c033/TnL1y4YEFR6J4wYcINBe/MTKFbF1XKly9vdevWDTx067VE34nU6tevnzVs2DDRYxpvUKFbY9T4gnoNALjdEboBAFGhc+fOie7/+c9/9tCd9PGgqVJ96NAhD9AhP/nJTzyg/epXv0oUulV93rhxoy1YsMCeeOIJf6x9+/ZWtWpV+/Wvf21vvfVWiq9XqVIlD/Zz585NFLoVtBctWuSh/p133knz93m70WesiySZQfPmzcPfp8zq3//+t3d+AABoLwcAZLL/R37gwIFWtmxZy507t1WrVs1effVVS0hISHSc2nH79OnjlWIdo2rxvffea+vWrUvxNe65555EgVv0Wo899pj94x//sDNnziQK3Xfeeae1a9cu/JjazBW8VTG/ePFiqt5Xx44d7e23304UCv/4xz96pVTnSs62bdusdevWVqBAAcuXL5/9x3/8h1+oiHT58mWvYqrirs+gSJEi1qxZM7+YId26dfMqd+gzC91uVOjz1sWHGjVqWJ48eaxJkya2Y8cOf37KlClWuXJlH4MqqklbmUMt6+oOaNq0qf99hQoVbPLkyd95raNHj1r37t39c9f56tSpYzNnzkx0jM6vMem78frrr/uFDf0bqgshVEHWxZPQ+50xY4Y/9tFHH9mTTz5p5cqV8+P1Pevfv7+dP38+0fn1uekz/+qrr7wjQr/r333QoEF25cqV8Bj0mOjfIPRaadFRsHnzZmvVqpUVLFjQvve971mLFi1sw4YNiY758ssvrXfv3v791+epf3u9t8jPXu9bj4UuNiWdwnGt8aoirs8g8jw69sMPP/TXLF68uJUpUyb8/PLly/1CgkJ4/vz5/ULS3/72t0TnPHLkiP+b6O/02ZcsWdJ++MMf0vYOIEug0g0AyBQUrH/wgx/YmjVrPHSp8rxy5Upv61b4ee211xIdrwCgIKtW3VDgUlBRG/fNzElWKFDA0S0y+NavX9+yZUt8DVsV66lTp9revXutVq1aKZ77qaeeCs/9/f73v++PqUquIK0Ak5QCi0KMAveLL75oOXPm9GCr8Kr33ahRIz9O5xw9erQ9++yzPqbTp0/bJ5984nPO1S7fq1cvb7dOro3/RimwLlmyxJ5//nm/r9dt27atj0+fvcLYv/71L/vtb39rzzzzjH3wwQeJ/l7P6cKGLjLoIsT8+fPtueees1y5cvnxovCr96h2fIV8BXMFfQVAza1/4YUXEp1z+vTp3jGgOff6DvzoRz/yiybqWNBj+gxFQV90Ll3o0OsqpOq78sYbb/jFFj0XSeH60Ucf9c9a4V5z/seNG+cBX3+vwD1p0iT/Xa8bujBTu3btFD9LjfGbb75J9FhsbKx/z/S56WKLLiKpm0KP6X3qe6N/g1C3xJYtW7wLQ3PBFWQVXjUefX5qKdf3+IEHHvD/fPz3f/+3vfTSS+GpGzc7hUP/xnrf+nx1gUz0vdI0DX1Wmrahz1fj0MUf/ecn1NL+n//5n/697tu3rz+miyv6XqrrhLZ3AJleAgAAUej5559X+Tp8/w9/+IPfHzFiRKLjnnjiiYSYmJiEffv2hR/Tcbp98skn4ce+/PLLhDvuuCPhRz/60Q2P5fPPP/e//fGPf5zo8bx58yY888wz3zl+2bJl/vorVqy47nlbtGiRcM899/jvDRo0SOjevbv//q9//SshV65cCTNnzkxYs2aNn2vBggXhv4uLi/Pn9+/fH37sn//8Z0L+/PkTHnjggfBjderUSWjTps0Nfc4piRxziP4+d+7cCV988UX4sSlTpvjjJUqUSDh9+nT48cGDB/vjkcfqnHps3Lhx4ccuXryYULdu3YTixYsnXLp0yR97/fXX/bg5c+aEj9NzTZo0SciXL1/4dXRuHVegQIGEo0ePJhrrli1b/Lnp06d/572dO3fuO4+NHj3av1/6/oR07drVz/HKK68kOrZevXoJ9957b/j+sWPH/Lhf//rXCakR+rdO7qb3dPXq1YQqVaokPProo/575LgrVKiQ0LJly+u+l02bNvm5Zs2aFX5M3ys9ptdO6lpjv+uuu/wzCNFnqWObNWuW8O2334YfP3PmTEKhQoUSevTokejvjxw5klCwYMHw4/q+6+/Hjh2bqs8JADIb2ssBAJmCFvzKnj27V+Yiqd1c+UAtrJHU3qxqYIhahtWuqup4qAU4NVSZUwuuWnR/85vfJHpOlVdVUJNS23Po+dRStfvdd9+1S5cuedu63qsqpElp7H/605+8rblixYrhx9WOq3OsX7/eK9pSqFAhrx5+/vnnFjRV5SMrkqFquyqYailO+viBAwcS/X2OHDm88h6iCrfuq+KptvPQd6BEiRJeCQ9RlV/fibNnz3qVP5JeO9TinRr6Nw5RpVbVZlXB9f1SVTYpzfWPpMp50vd1M1QpVpU38qb3rRXX9W+pf+fjx4/7+HTTWPX5a/pEaIpC5HvRNAMdrxZ/fSdSu7r+jerRo4d/b0M0bnUg6N8rNFbddIy+B+paCY1V/97q9FDHAwBkNbSXAwAyBc1RLVWqVKIAF9kKq+cjJbdyuBY4U4g+duyYh5iUKOCqPVftuAr1ev1ICgvJzdsOrTYeGXxSotfRnGC9juaiqzU76XsVjV3vQXN1k9JnodClbc80N/2VV17xCw1632qpV3v9j3/841S1ON8oXdSIpPnGonnRyT2eNFzps0268JbGLWqNbty4sf8b6981aTv/tb4Daj+/EWplVuBVm3zS8Z06deo7F1aSBnptMZcWoVFTEh5++OHvPB66eKJ27WvRODUOXfBRi79azzX9InLdg6TvJa0k/bxD4w1NmUhK0yNEF67Ueq4LaJqrr39rff+7dOmSqv+cAkC0I3QDAHCdyt3SpUs9BCcXHFRdPnz48HceDz2WNKRfj86l+baaF6xFsdJixXLN2d2/f78v6qbqeHx8vM991wJlmuedliIrnKl5POnid0G4kYseusCiee4nTpywn//853b33Xf7RQAFVs0ZT7ry+bXeV5BCYxg7duw1tzvTom6iudEK3D/96U+960MXO0L7fd/qKu7X6hRJ+nmHXkfzupMLz+puCNE4H3/8cfvDH/7g3ShDhgzxiwaaw16vXr1bGi8AZDRCNwAgU7jrrrt8sSotMhVZAd69e3f4+UjJtVRrYTMtIJWalmMt0KbQotWvI9uZIyn4aPEqhYvI6qtWl9brhCq1qaW2YYVhtQBrUbHkaOw69549e77znD4LjSOyuqwFuLQqtG5qwVYQ1wJrodB9M6uVB0ELuiXdZkr/XhJqW9e/8Wefffadz/ta34HkXOv9aqV1vZ5WQleFNSS00vvNSOvPVou0hSrEyVXCI2mKgiriuogT2YGhdu/UjlEV86THa/pDchearjdeLQaY0nhDx6varZv+86v/fGn8c+bMSdXrAUC0Yk43ACBTUAhVhe1//ud/Ej2uyq2Cg1Z0jrRp06ZEc1fVcq2K7yOPPJJilVKVRK1IrRWdk66IHUl7KX/99dc+FztEc1a10rWqdsnN974enU8rUmu1b81xTY7Grveg9xK5nZLGoRXPtSp0qG1X83iTVkE1rzeyJT4UcpOGq4zYR1srsEeGO93XRYbQ3Hx9B7SKvFalj/w7rTCu96ats1Jyrfcb+k5EVuD1+/jx42/6PYVWuk+rz1afg4Kpvpu6gJLc1IPI95O0m0CfU9Iq9fX+/fVaSbfZ06r8qV0TQSuW67s4atQon1d+rfFqukRoSkbka+viWmq33QOAaEalGwCQKSjEai/hX/7ylx42tT+zWqYVPtWaGqqqhWgOs/6f/sgtw0J7Jl/PokWLfJsrzR3WXOGkVTa1IGveaSgka/6pqsia9639vfU6CiUpvU5y1AKcmn2cR4wY4RVYBWxt06Q2XQVUBRRtyRWiPbPVsq6wpoq3tgtTBVTbbYWEAq0+J31eCmtqQU5vasXXvF7926pDQMFaC4cp5GmxNNE2X3qfavfW4mqqgOv9qB1fHQnJzYFPSt8TdRKoxV7HK3RqUS+1k+s5zatXS7nColr8b2WOttqt9W+g96L3pH8DfS9vZss6UXVfUwR0gUlz9vW9K126tI9Xi5JpzNrfXTQnWm3d+k5pDLoIpU4RbYUWSdVk/Zvrs9dcb/1nRVMpVJ1WN4QWi9OCdPre/+Uvf/HW76T72F+LxqPtwbSOgLbW0/dKF1E0d37ZsmV2//33+0U0dRhoIThtF6ex6vus/xzqQlJGfBcBIM1l9PLpAACkdisrbUHUv3//hFKlSiXkzJnTt0/SNkOR2yeJ/k5/r62ldIy2s9J2Tslti5SUtki61rZNyW2tdOLECd/qq0iRIgnf+973fPsrbUt1s9tvJZXclmHy6aef+tZR2ipLr/vQQw8lbNy4MdEx2l7tvvvu822b8uTJk3D33XcnjBw5MrwFl2iLp759+yYUK1bMt8ZK6f81uNaWYfq8I4W27Uq6DVRy7yd0Tm3xpu2/tD2btqX6n//5n++8/tdff53w9NNPJxQtWtS3TatVq9Z3tv+61muHLF68OKFGjRoJOXLkSLR92M6dOxMefvhh/0x1fm1p9Ze//OU7W4xpuyxtF3et704k/ZtoGzGNNaXtw671b53Utm3bEtq1a+ffOX239Vm1b98+YfXq1eFjtA1X6HPS+9F3Zffu3d/Z7kt+//vfJ1SsWDEhe/bsib7jV65cSfj5z3/u59B3TOfQ1nzX2jLsWt97nU9/q23C9G9bqVKlhG7duoW39Pvmm2/8+6Pvpz5XHdeoUaOE+fPnX/dzAIDMIkb/J+2jPAAAGUft5s8///x3WtERnVSNV1v+X//614weCgAAaY453QAAAAAABITQDQAAAABAQAjdAAAAAAAEhDndAAAAAAAEhEo3AAAAAAABIXQDAAAAABCQHEGdGNHl6tWr9s9//tPy58/vW+kAAAAAAG6eZmqfOXPGSpUqZdmyXbueTei+TShwly1bNqOHAQAAAABZyt///ncrU6bMNZ8ndN8mVOEOfSEKFCiQ0cMBAAAAgEzt9OnTXtgMZa1rIXTfJkIt5QrchG4AAAAASBspTd9lITUAAAAAAAJCpfs288DLcy177jwZPQwAAAAAuK6tY7tYVkClGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAISJYI3QkJCdazZ0+LjY21mJgY2759e0YPCQAAAACArBG6V6xYYTNmzLClS5fa4cOHrWbNmrd8zm7dullcXJxFi88++8yaN29ud9xxh5UtW9Z++9vfZvSQAAAAAAApyGFZwP79+61kyZLWtGlTizZXrlzx6nu2bDd/feP06dP2yCOP2MMPP2yTJ0+2HTt22DPPPGOFChXyCj8AAAAAIDpl+kq3KtJ9+/a1Q4cOebgtX768Xb161UaPHm0VKlSwPHnyWJ06dWzhwoWJgnD37t3Dz1erVs3Gjx8ffn7o0KE2c+ZMW7x4sZ9Tt7Vr1/pNv588eTJ8rFrZ9djBgwf9viruCsNLliyxGjVqWO7cuX1sFy9etEGDBlnp0qUtb9681qhRIz9farz55pt26dIlmzZtmt1zzz32X//1X9avXz/73e9+l6afJQAAAAAgbWX6SrfCcqVKlWzq1Km2ZcsWy549uwfuOXPmeFW4SpUqtm7dOuvcubMVK1bMWrRo4aG8TJkytmDBAitSpIht3LjRK8aqlrdv397D8a5du7zCPH36dH8dzRfXcalx7tw5GzNmjMXHx/v5ixcvbn369LGdO3favHnzrFSpUrZo0SJr1aqVV601xuvZtGmTPfDAA5YrV67wY48++qi/xr/+9S8rXLjwd/5GIV+3EL0XAAAAAED6yvShu2DBgpY/f34P2yVKlPCgOWrUKFu1apU1adLEj6lYsaKtX7/epkyZ4qE7Z86cNmzYsPA5VPFWsJ0/f76H7nz58nkFXOfSOW/U5cuXbeLEiV5hF1W6Fd71U4FbFOw1F12Pa7zXc+TIER9jpDvvvDP8XHKhWxceIt8jAAAAACD9ZfrQndS+ffu80tyyZctEj6s9u169euH7EyZM8HZtBeHz58/783Xr1k2TMagiXbt27fB9VbPV0l61atVExynUqxIehMGDB9uAAQMSVbq1ABsAAAAAIP1kudB99uxZ/7ls2TKfPx1J86tFLd6qNI8bN86r4aqUjx071jZv3nzdc4cWQ9MWZZFV7aRUJdc878gxqRK/detW/xlJVfWUqNr+9ddfJ3osdP9alXi919D7BQAAAABkjCwXuiMXL1MreXI2bNjgK5337t070QroSavVqk5H0pxw0bZkoZbu1OwJrgq7znX06FHf9utG6cLAL3/5Sw/4ao2X999/3xeAS661HAAAAAAQHTL96uVJqWqtKnb//v19BXKF6U8//dTeeOMNvy9auOyTTz6xlStX2t69e23IkCG+CFskrYKuvbH37Nlj33zzjQfeypUre4u2Vjf//PPPvZquanlK1FbeqVMn69Kli7377rv2xRdf2Mcff+zzrnWOlDz11FN+EUArrv/tb3+zt99+2xeQi2wfBwAAAABEnywXumX48OEepBVqq1ev7quEK9yGFiPr1auXtWvXzjp06OBbdx0/fjxR1Vt69OjhleQGDRp4hVvVcVWZ586da7t37/Y521o9fMSIEakakxZMU+geOHCgnzcuLs6Dfrly5VK1WNyf/vQnD+v33nuvn+NXv/oVe3QDAAAAQJSLSYicoIwsSwupKbzX6TvZsufOk9HDAQAAAIDr2jq2i2WGjHXq1CkrUKDA7VXpBgAAAAAgGhC6o0Dr1q19FfPkbint4Q0AAAAAiF5ZbvXyzCg+Pt73Ck9ObGxsuo8HAAAAAJA2CN1RIOl+4gAAAACArIH2cgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACwpzu28y6ER2vu4ccAAAAACDtUOkGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACwpZht5kHXp5r2XPnSfa5rWO7pPt4AAAAACAro9INAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAANyuoTshIcF69uxpsbGxFhMTY9u3b8/oIQEAAAAAkDVC94oVK2zGjBm2dOlSO3z4sNWsWfOWz9mtWzeLi4uzaHDhwgUfT61atSxHjhzJjkvv+6mnnrKqVatatmzZ7Kc//WmGjBUAAAAAkMVC9/79+61kyZLWtGlTK1GihAfTaHHlyhW7evXqLZ8jT5481q9fP3v44YeTPebixYtWrFgxe/nll61OnTq39HoAAAAAgPQT1aFbFeC+ffvaoUOHvLW8fPnyHnJHjx5tFSpU8LCqELpw4cJEIbZ79+7h56tVq2bjx48PPz906FCbOXOmLV682M+p29q1a/2m30+ePBk+Vq3seuzgwYN+XxX3QoUK2ZIlS6xGjRqWO3duH5tC8aBBg6x06dKWN29ea9SokZ8vNXT8pEmTrEePHn5RITl633oPXbp0sYIFC97CJwoAAAAASE/RUzZOhoJmpUqVbOrUqbZlyxbLnj27B+45c+bY5MmTrUqVKrZu3Trr3LmzV4JbtGjhobxMmTK2YMECK1KkiG3cuNHnhKta3r59ew/Hu3btstOnT9v06dP9dTRfXMelxrlz52zMmDEWHx/v5y9evLj16dPHdu7cafPmzbNSpUrZokWLrFWrVrZjxw4fY0bQhQDdQvR+AQAAAADpK6pDt6q6+fPn97CtKrBC5KhRo2zVqlXWpEkTP6ZixYq2fv16mzJliofunDlz2rBhw8LnUMV706ZNNn/+fA/d+fLl8wq4znWtyvL1XL582SZOnBhu81alW+FdPxW4RcFec9H1uMabEXRxIvJzAAAAAACkv6gO3Unt27fPK80tW7ZM9PilS5esXr164fsTJkywadOmeRA+f/68P1+3bt00GUOuXLmsdu3a4fuqZqulXYucRVKoVyU8owwePNgGDBiQqNJdtmzZDBsPAAAAANyOMlXoPnv2rP9ctmyZz5+OpPnVohZvVZrHjRvn1XBVyseOHWubN2++7rm1Knhoi7LIqnZSqpJrnnfkmFSJ37p1q/+MpKp6RtHnEfpMAAAAAAAZI1OF7sjFy9RKnpwNGzb4Sue9e/dOtAJ60mq1qtORNCc8tD1X4cKF/ffU7AmuCrvOdfToUWvevPlNvS8AAAAAQNaUqUK3qtaqYvfv398XTGvWrJmdOnXKg3aBAgWsa9euvnDZrFmzbOXKlT6fe/bs2b4Im36PXA1cz+/Zs8dbwDV3vHLlyt5+rdXNR44caXv37vVqeUrUVt6pUydfWVzHK4QfO3bMVq9e7W3obdq0SfEcWoRNLfAnTpywM2fOhMN+ZEt86DFV1nV+3dfFA12IAAAAAABEp0wVumX48OFeldZCYQcOHPAtvOrXr28vvfSSP9+rVy/btm2bdejQwdvAO3bs6FXv5cuXh8+h7bm0pVeDBg08xK5Zs8YefPBBmzt3rj333HMelhs2bGgjRoywJ598MsUxacE0HTtw4ED76quvrGjRota4cWNr27Ztqt7TY489Zl9++WX4fmh+emSre+ScdbWyv/XWW3bXXXeFtzMDAAAAAESfmITIZIcsSwupqaJfp+9ky547T7LHbB3bJd3HBQAAAACZOWOp+1qd19fyf6uHAQAAAACANEfoDljr1q19FfPkbhm1hzcAAAAAIH1kujndmU18fLzvFZ6c2NjYdB8PAAAAACD9ELoDlnQ/cQAAAADA7YP2cgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACwpzu28y6ER2vu4ccAAAAACDtUOkGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACwpZht5kHXp5r2XPnCd/fOrZLho4HAAAAALIyKt0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQECyROhOSEiwnj17WmxsrMXExNj27dszekgAAAAAAGSN0L1ixQqbMWOGLV261A4fPmw1a9a85XN269bN4uLiLFqsXLnSGjdubPnz57dixYrZf/7nf9rBgwczelgAAAAAgKweuvfv328lS5a0pk2bWokSJSxHjhwWLa5cuWJXr169pXN88cUX9sMf/tC+//3vexVfAfybb76xdu3apdk4AQAAAABpL9OHblWk+/bta4cOHfLW8vLly3vIHT16tFWoUMHy5MljderUsYULFyYKwt27dw8/X61aNRs/fnz4+aFDh9rMmTNt8eLFfk7d1q5d6zf9fvLkyfCxCsF6LFR1VsW9UKFCtmTJEqtRo4blzp3bx3bx4kUbNGiQlS5d2vLmzWuNGjXy86XG1q1bfcwjRoywSpUqWf369f1ceu3Lly+n6ecJAAAAAEg70VMSvkkKywqiU6dOtS1btlj27Nk9cM+ZM8cmT55sVapUsXXr1lnnzp29LbtFixYeysuUKWMLFiywIkWK2MaNG31OuKrl7du390C7a9cuO336tE2fPt1fR/PFdVxqnDt3zsaMGWPx8fF+/uLFi1ufPn1s586dNm/ePCtVqpQtWrTIWrVqZTt27PAxXs+9995r2bJl87HoIsPZs2dt9uzZ9vDDD1vOnDmT/RuFfN1C9F4AAAAAAOkr04fuggUL+jxnhW21litojho1ylatWmVNmjTxYypWrGjr16+3KVOmeOhWUB02bFj4HKp4b9q0yebPn++hO1++fF4B17l0zhul6vPEiRO9wi6qdCsw66cCtyjYay66Htd4r0fj+9Of/uRj69Wrl1e99d7ee++9a/6NLjxEvkcAAAAAQPrL9O3lSe3bt88rzS1btvTwHLrNmjXL536HTJgwwSvIqn7reVXKFYrTQq5cuax27drh+6pmKyhXrVo10Zg+/PDDRGO6liNHjliPHj2sa9euXs3X3+k1nnjiCV+5PTmDBw+2U6dOhW9///vf0+S9AQAAAABuo0p3Umq9lmXLlvn86UiaXy1q8Valedy4cV4xVqV87Nixtnnz5uueWy3eEhl0k5tTrSq55nlHjkmVeM3N1s9ICt8p0QUCVfR/+9vfhh9T+3zZsmV9zFrVPCm919D7BQAAAABkjCwXuiMXL1MreXI2bNjgK5337t07/FjSirMqyapOR1JVXLQtWeHChf331OwJXq9ePT/X0aNHrXnz5jf8nlS5DwX+kFB4v9WV0QEAAAAAwcly7eWqWquK3b9/f1+BXGH6008/tTfeeMPvixYu++STT3zrrb1799qQIUO8bTuSVkH/7LPPbM+ePb49lyralStX9uqyVjf//PPPvZquanlK1FbeqVMn69Kli7377ru+BdjHH3/s8651jpS0adPGx/fKK6/46+r9PP3003bXXXd5oAcAAAAARKcsF7pl+PDhHqQVaqtXr+6rhCvcakEy0WJk2uO6Q4cOvnXX8ePHE1W9RXOotZVYgwYNvMKt6rgWYJs7d67t3r3b52xrhXJt45UaWjBNoXvgwIF+3ri4OA/S5cqVS/FvtT/3W2+9ZX/4wx88ZOv9qJqvhdjUyg4AAAAAiE4xCddaiQtZirYM07zwOn0nW/bc/z+obx3bJUPHBQAAAACZOWNp4eoCBQrcXpVuAAAAAACiAaE7CrRu3TrRVmKRt5T28AYAAAAARK8st3p5ZhQfH2/nz59P9rnY2Nh0Hw8AAAAAIG0QuqNA0v3EAQAAAABZA+3lAAAAAAAEhNANAAAAAEBACN0AAAAAAASEOd23mXUjOl53DzkAAAAAQNqh0g0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQLJE6E5ISLCePXtabGysxcTE2Pbt2zN6SAAAAAAAZI3QvWLFCpsxY4YtXbrUDh8+bDVr1rzlc3br1s3i4uIsGqxdu9Z++MMfWsmSJS1v3rxWt25de/PNNzN6WAAAAACAFOSwLGD//v0eSJs2bWrR5sqVK159z5bt5q9vbNy40WrXrm0///nP7c477/SLC126dLGCBQta27Zt03S8AAAAAIC0k+kr3apI9+3b1w4dOuThtnz58nb16lUbPXq0VahQwfLkyWN16tSxhQsXJgrC3bt3Dz9frVo1Gz9+fPj5oUOH2syZM23x4sV+Tt1UbdZNv588eTJ8rFrZ9djBgwf9viruhQoVsiVLlliNGjUsd+7cPraLFy/aoEGDrHTp0l6tbtSokZ8vNV566SUbPny4X1SoVKmSvfDCC9aqVSt799130/SzBAAAAACkrUxf6VZYVhCdOnWqbdmyxbJnz+6Be86cOTZ58mSrUqWKrVu3zjp37mzFihWzFi1aeCgvU6aMLViwwIoUKeKVZM0JV7W8ffv2Ho537dplp0+ftunTp/vraL64jkuNc+fO2ZgxYyw+Pt7PX7x4cevTp4/t3LnT5s2bZ6VKlbJFixZ5cN6xY4eP8UadOnXKqlevfsN/BwAAAABIP5k+dKvFOn/+/B62S5Qo4RXlUaNG2apVq6xJkyZ+TMWKFW39+vU2ZcoUD905c+a0YcOGhc+hivemTZts/vz5Hrrz5cvnFXCdS+e8UZcvX7aJEyd6hV1U6VZ4108FblGw11x0Pa7x3giNUxcY9H6uRWPXLUQXEAAAAAAA6SvTh+6k9u3b55Xmli1bJnr80qVLVq9evfD9CRMm2LRp0zwInz9/3p/XAmVpIVeuXD4HO0TVbLW0V61aNdFxCsWqhN+INWvW2NNPP22///3v7Z577rnmcar2R15YAAAAAACkvywXus+ePes/ly1b5vOnI2l+tajFW5XmcePGeTVclfKxY8fa5s2br3vu0GJo2qIssqqdlKrkmucdOSZV4rdu3eo/I6mqnloffvihPf744/baa6/5QmrXM3jwYBswYECiSnfZsmVT/VoAAAAAgFuX5UJ35OJlaiVPzoYNG3xRst69eydaAT1ptVrV6UiaEy7alqxw4cL+e2r2BFeFXec6evSoNW/e/KbelxZd00rlmiuu+ecp0WcQusgAAAAAAMgYWS50q2qtKnb//v19wbRmzZr5omMK2gUKFLCuXbv6wmWzZs2ylStX+nzu2bNn+xxp/R6iVdD1/J49e7wFXHPHK1eu7NVirW4+cuRI27t3r1fLU6K28k6dOnl1WscrhB87dsxWr17tbeht2rRJsaVcgVurlv/nf/6nHTlyJHxhQAu8AQAAAACiU6bfMiw52l5ryJAhPq9ZK3xrlXC1m4dCda9evaxdu3bWoUMH37rr+PHjiare0qNHD99KrEGDBl7hVmjXAmxz58613bt3e1hW1XnEiBGpGpMWTFPoHjhwoJ83Li7Og365cuVS/FttX6Z56no/WmE9dNN7AAAAAABEr5iEyAnKyLI0p1vVelX9VfEHAAAAAASfsbJkpRsAAAAAgGhA6I4CrVu39lXMk7vd6B7eAAAAAIDokeUWUsuM4uPjfa/w5LBQGgAAAABkXoTuKJB0P3EAAAAAQNZAezkAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAHJEqE7ISHBevbsabGxsRYTE2Pbt2/P6CEBAAAAAJA1QveKFStsxowZtnTpUjt8+LDVrFnzls/ZrVs3i4uLs2izb98+y58/vxUqVCijhwIAAAAAuB1C9/79+61kyZLWtGlTK1GihOXIkcOixZUrV+zq1atpcq7Lly9bx44drXnz5mlyPgAAAABAsDJ96FZFum/fvnbo0CFvLS9fvryH3NGjR1uFChUsT548VqdOHVu4cGGiINy9e/fw89WqVbPx48eHnx86dKjNnDnTFi9e7OfUbe3atX7T7ydPngwfq1Z2PXbw4EG/r4q7qtBLliyxGjVqWO7cuX1sFy9etEGDBlnp0qUtb9681qhRIz/fjXj55Zft7rvvtvbt26fJZwcAAAAACFb0lIRvksJypUqVbOrUqbZlyxbLnj27B+45c+bY5MmTrUqVKrZu3Trr3LmzFStWzFq0aOGhvEyZMrZgwQIrUqSIbdy40eeEq1quQKtwvGvXLjt9+rRNnz7dX0fzxXVcapw7d87GjBlj8fHxfv7ixYtbnz59bOfOnTZv3jwrVaqULVq0yFq1amU7duzwMabkgw8+8PEq5L/77ru3/LkBAAAAAIKX6UN3wYIFfY6zwrZay1VRHjVqlK1atcqaNGnix1SsWNHWr19vU6ZM8dCdM2dOGzZsWPgcqnhv2rTJ5s+f76E7X758XgHXuXTOm2kDnzhxolfYRZVuhXf9VOAWBXvNRdfjGu/1HD9+3Cv6upBQoECBVI1BY9ctRBcQAAAAAADpK9OH7uQWGlOluWXLlokev3TpktWrVy98f8KECTZt2jQPwufPn/fn69atmyZjyJUrl9WuXTt8X9VstbRXrVo10XEKxaqEp6RHjx721FNP2QMPPJDqMajaH3lhAQAAAACQ/rJc6D579qz/XLZsmc+fjqT51aIWb1Wax40b59VwVcrHjh1rmzdvvu65s2XLFt6iLLKqnZSq5JrnHTkmVeK3bt3qPyOpqp6a1nLNEX/11VfDr68WeS0Yp7b6Z5555jt/M3jwYBswYECiSnfZsmVTfC0AAAAAQNrJcqE7cvEytZInZ8OGDb7See/evROtgJ60Wq3qdCTNCRdtS1a4cGH/PTV7gqvCrnMdPXr0plYeV+t75Fi0wJvmjGuOedILCyH6DEIXGQAAAAAAGSPLhW5VrVXF7t+/v1eDmzVrZqdOnfKgrfnQXbt29YXLZs2aZStXrvT53LNnz/ZF2PR7iFZB1/N79uzxFnDNHa9cubJXi7W6+ciRI23v3r1eLU+J2so7depkXbp08eMVwo8dO2arV6/2NvQ2bdpc9++rV6+e6P4nn3ziVfe02I8cAAAAABCcTL9lWHKGDx9uQ4YM8XnNCqxaJVzt5qFQ3atXL2vXrp116NDBt+7SQmWRVe/QPGptJdagQQOvcCu0awG2uXPn2u7duz0sq9o8YsSIVI1JC6YpdA8cONDPGxcX50G/XLlygXwGAAAAAICMF5MQOUEZWZbmdKtar6p/aldABwAAAADcWsbKkpVuAAAAAACiAaE7CrRu3dpXMU/ultIe3gAAAACA6JXlFlLLjOLj432v8OTExsam+3gAAAAAAGmD0B0FrrXtFwAAAAAgc6O9HAAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAISJYI3QkJCdazZ0+LjY21mJgY2759e0YPCQAAAACArBG6V6xYYTNmzLClS5fa4cOHrWbNmrd8zm7dullcXJxFgz179thDDz1kd955p91xxx1WsWJFe/nll+3y5csZPTQAAAAAwHXksCxg//79VrJkSWvatKlFmytXrnj1PVu2m7++kTNnTuvSpYvVr1/fChUqZH/5y1+sR48edvXqVRs1alSajhcAAAAAkHYyfaVbFem+ffvaoUOHPNyWL1/ew+jo0aOtQoUKlidPHqtTp44tXLgwURDu3r17+Plq1arZ+PHjw88PHTrUZs6caYsXL/Zz6rZ27Vq/6feTJ0+Gj1Urux47ePCg31fFXcF4yZIlVqNGDcudO7eP7eLFizZo0CArXbq05c2b1xo1auTnSw1Vtp9++ml/H3fddZf94Ac/sE6dOtlHH32Upp8lAAAAACBtZfpKt8JypUqVbOrUqbZlyxbLnj27B+45c+bY5MmTrUqVKrZu3Trr3LmzFStWzFq0aOGhvEyZMrZgwQIrUqSIbdy40eeEq1revn17D8e7du2y06dP2/Tp0/11NF9cx6XGuXPnbMyYMRYfH+/nL168uPXp08d27txp8+bNs1KlStmiRYusVatWtmPHDh/jjdi3b5+31Ldr1+6axyjk6xai9wIAAAAASF+ZPnQXLFjQ8ufP72G7RIkSHjTVcr1q1Spr0qRJuFK8fv16mzJliodutWsPGzYsfA5VvDdt2mTz58/30J0vXz6vgOtcOueN0lzriRMnemVaVOlWeNdPBW5RsFdw1uOpbRFX+/ynn37q49JFgldeeeWax+rCQ+R7BAAAAACkv0wfupOrAqvS3LJly0SPX7p0yerVqxe+P2HCBJs2bZoH4fPnz/vzdevWTZMx5MqVy2rXrh2+r2q2WtqrVq2a6DiFZ1XCU+vtt9+2M2fO+Jzun/3sZ/bqq6/aiy++mOyxgwcPtgEDBiSqdJctW/am3g8AAAAA4OZkudB99uxZ/7ls2TKfPx1J86tFLd6qNI8bN86r4aqUjx071jZv3nzdc4cWQ9MWZSHJrSCuKrnmeUeOSZX4rVu3+s9IqqqnVig0a664Qryq3QMHDvzOOUPvNfR+AQAAAAAZI8uF7sjFy9RKnpwNGzZ4q3bv3r0TrYCetFqtYBtJc8JF25IVLlzYf0/NnuCqsOtcR48etebNm1ta0Lx0BX79TC50AwAAAAAyXpYL3apaq4rdv39/D6TNmjWzU6dOedAuUKCAde3a1RcumzVrlq1cudLnc8+ePdsXYdPvIVoFXc9rj2y1gGvueOXKlb3arNXNR44caXv37vVqeUrUVq7VxrXtl45XCD927JitXr3a29DbtGlz3b9/8803fR56rVq1/ILCJ5984u3jHTp08McBAAAAANEpy4VuGT58uFeltZjYgQMHfAsv7XH90ksv+fO9evWybdu2eWhVG3jHjh296r18+fLwObQPtrb0atCggbeHr1mzxh588EGbO3euPffccx6WGzZsaCNGjLAnn3wyxTFpwTQdq3bwr776yooWLWqNGze2tm3bpvi3OXLk8NXQFfLV2q5tw7Qaui4sAAAAAACiV0xC5ARlZFlaSE3VelX9VfEHAAAAAASfsf5vZTAAAAAAAJDmCN1RoHXr1r6KeXK31O7hDQAAAACIPllyTndmEx8f73uFJyc2NjbdxwMAAAAASBuE7iiQdD9xAAAAAEDWQHs5AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAAByRKhOyEhwXr27GmxsbEWExNj27dvz+ghAQAAAACQNUL3ihUrbMaMGbZ06VI7fPiw1axZ85bP2a1bN4uLi7NoMX/+fKtbt65973vfs7vuusvGjh2b0UMCAAAAAKQgh2UB+/fvt5IlS1rTpk0t2ly5csWr79my3fz1jeXLl1unTp3sjTfesEceecR27dplPXr0sDx58lifPn3SdLwAAAAAgLST6Svdqkj37dvXDh065OG2fPnydvXqVRs9erRVqFDBg2mdOnVs4cKFiYJw9+7dw89Xq1bNxo8fH35+6NChNnPmTFu8eLGfU7e1a9f6Tb+fPHkyfKxa2fXYwYMH/b4q7oUKFbIlS5ZYjRo1LHfu3D62ixcv2qBBg6x06dKWN29ea9SokZ8vNWbPnu1V95/85CdWsWJFa9OmjQ0ePNjGjBnjrfUAAAAAgOiU6SvdCsuVKlWyqVOn2pYtWyx79uweuOfMmWOTJ0+2KlWq2Lp166xz585WrFgxa9GihYfyMmXK2IIFC6xIkSK2ceNGnxOuann79u09HKuafPr0aZs+fbq/juaL67jUOHfunAfi+Ph4P3/x4sW9Ir1z506bN2+elSpVyhYtWmStWrWyHTt2+BivR4FdbeWRdLHgH//4h3355Zd+oSG5v9EtRO8FAAAAAJC+Mn3oLliwoOXPn9/DdokSJTxojho1ylatWmVNmjTxY1QdXr9+vU2ZMsVDd86cOW3YsGHhc6jivWnTJp83rdCdL18+D7U6l855oy5fvmwTJ070Cruo0q3wrp8K3KJgr7noelzjvZ5HH33U+vfv71X9hx56yPbt22fjxo3z5zSHPbnQrQsPke8RAAAAAJD+Mn3oTkqBVJXmli1bJnr80qVLVq9evfD9CRMm2LRp0zwInz9/3p/XQmVpIVeuXFa7du3wfVWz1dJetWrVRMcp1KsSnhLN39a89bZt23qgL1CggL3wwgveBn+tueJqPx8wYECiSnfZsmVv6X0BAAAAAG7z0H327Fn/uWzZMp8/HUnzq0Ut3qo0q1qsargq5VoNfPPmzdc9dyjgRs6jVghOSlVyzfOOHJMq8Vu3bvWfkVRVT4nOpXZ1VcSPHDnibfKrV68OV/GTo/caer8AAAAAgIyR5UJ35OJlaiVPzoYNG3yl8969e4cfUyU5abVa1elICruhlu7ChQv776nZE1wVdp3r6NGj1rx5c7tZCuyhCwlz5871CwahMQEAAAAAok+WC92qWquKrTnQWjCtWbNmdurUKQ/aasvu2rWrL1w2a9YsW7lypc/n1urgWoRNv4donrSe37Nnj7eAa+545cqVvUVbbd0jR460vXv3hudWX4/ayrXlV5cuXfx4hfBjx455tVpt6FqN/Hq++eYbX339wQcftAsXLvg8cC0C9+GHH6bJZwYAAAAACEam3zIsOcOHD7chQ4b4YmLVq1f3VcLVbh4K1b169bJ27dpZhw4dfOuu48ePJ6p6h+ZRayuxBg0aeDVZoV0LsKnCvHv3bg/LavkeMWJEqsakoKzQPXDgQD+vtgBT0C9Xrlyq/l5bmGks999/v/3tb3/z7cbuu+++m/h0AAAAAADpJSaBjZ5vC1pITdV6Vf1V8QcAAAAABJ+xsmSlGwAAAACAaEDojgKtW7f2VcyTu6W0hzcAAAAAIHpluYXUMqP4+HjfKzw5sbGx6T4eAAAAAEDaIHRHgaT7iQMAAAAAsgbaywEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACEiWCN0JCQnWs2dPi42NtZiYGNu+fXtGDwkAAAAAgKwRulesWGEzZsywpUuX2uHDh61mzZq3fM5u3bpZXFycRYOhQ4f6xYSkt7x582b00AAAAAAA15HDsoD9+/dbyZIlrWnTphZtrly54gE5W7abv74xaNAg+8lPfpLosf/4j/+whg0bpsEIAQAAAABByfSVblWk+/bta4cOHfJwW758ebt69aqNHj3aKlSoYHny5LE6derYwoULEwXh7t27h5+vVq2ajR8/PlFleebMmbZ48eJwVXnt2rV+0+8nT54MH6tWdj128OBBv6+Ke6FChWzJkiVWo0YNy507t4/t4sWLHp5Lly7tFepGjRr5+VIjX758VqJEifDt66+/tp07d/p7AAAAAABEr0xf6VZYrlSpkk2dOtW2bNli2bNn98A9Z84cmzx5slWpUsXWrVtnnTt3tmLFilmLFi08lJcpU8YWLFhgRYoUsY0bN/qccFXL27dv7+F4165ddvr0aZs+fbq/juaL67jUOHfunI0ZM8bi4+P9/MWLF7c+ffp4UJ43b56VKlXKFi1aZK1atbIdO3b4GG+Ezlu1alVr3rz5NY9RyNctRO8FAAAAAJC+Mn3oLliwoOXPn9/DtqrACpqjRo2yVatWWZMmTfyYihUr2vr1623KlCkeunPmzGnDhg0Ln0MV702bNtn8+fM9dKuyrAq4zqVz3qjLly/bxIkTvcIuqnQrvOunArco2Gsuuh7XeFPrwoUL9uabb9ovfvGL6x6nCw+R7xEAAAAAkP4yfehOat++fV5pbtmyZaLHL126ZPXq1QvfnzBhgk2bNs2D8Pnz5/35unXrpskYcuXKZbVr1w7fVzVbLe2qTkdSqFcl/EaoQn7mzBnr2rXrdY8bPHiwDRgwIFGlu2zZsjf0WgAAAACAW5PlQvfZs2f957Jly3z+dCTNrxa1eKvSPG7cOK+Gq1I+duxY27x583XPHVoMTVuURVa1k1KVXPO8I8ekSvzWrVv9ZyRV1W+0tbxt27Z25513Xvc4vdfQ+wUAAAAAZIwsF7ojFy9TK3lyNmzY4Cud9+7dO9EK6Emr1apOR9KccNG2ZIULF/bfU7MnuCrsOtfRo0evOw87JV988YWtWbPGF2kDAAAAAES/LBe6VbVWFbt///6+YFqzZs3s1KlTHrQLFCjgbdlauGzWrFm2cuVKn889e/ZsX4RNv4doFXQ9v2fPHm8B19zxypUre4u2VjcfOXKk7d2716vlKVFbeadOnaxLly5+vEL4sWPHbPXq1d6G3qZNm1S9N7XDa7G31q1b39JnBAAAAABIH5l+y7DkDB8+3IYMGeKLiVWvXt1XCVe7eShU9+rVy9q1a2cdOnTwrbuOHz+eqOotPXr08K3EGjRo4BVuhXYtwDZ37lzbvXu3h2WtUD5ixIhUjUkLpil0Dxw40M8bFxfnQb9cuXKp+ntdQNB2ZNoiLWmLOgAAAAAgOsUkRE5QRpalhdRUrVfVXxV/AAAAAEDwGStLVroBAAAAAIgGhO4ooDnaWsU8uduN7OENAAAAAIguWW4htcxI24Bpr/DkxMbGpvt4AAAAAABpg9AdBZLuJw4AAAAAyBpoLwcAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAA4HYN3QkJCdazZ0+LjY21mJgY2759e0YPCQAAAACArBG6V6xYYTNmzLClS5fa4cOHrWbNmrd8zm7dullcXJxFgwsXLvh4atWqZTly5Eh2XOvXr7f777/fihQpYnny5LG7777bXnvttQwZLwAAAAAg9XJYlNu/f7+VLFnSmjZtatHmypUrXn3Pli3bLZ1DQbpfv372zjvvJHtM3rx5rU+fPla7dm3/XSG8V69e/ru6AAAAAAAA0SmqK92qAPft29cOHTrk4bZ8+fJ29epVGz16tFWoUMHDap06dWzhwoWJQmz37t3Dz1erVs3Gjx8ffn7o0KE2c+ZMW7x4sZ9Tt7Vr1/pNv588eTJ8rFrZ9djBgwf9viruhQoVsiVLlliNGjUsd+7cPraLFy/aoEGDrHTp0h6EGzVq5OdLDR0/adIk69Gjh5UoUSLZY+rVq2cdO3a0e+65xz+Dzp0726OPPmofffTRLXy6AAAAAIDbutKtsFypUiWbOnWqbdmyxbJnz+6Be86cOTZ58mSrUqWKrVu3zkNosWLFrEWLFh7Ky5QpYwsWLPB27I0bN3o1WNXy9u3bezjetWuXnT592qZPn+6vo/niOi41zp07Z2PGjLH4+Hg/f/Hixb0KvXPnTps3b56VKlXKFi1aZK1atbIdO3b4GNPatm3bfLwjRoxI83MDAAAAAG6T0F2wYEHLnz+/h21VgVVRHjVqlK1atcqaNGnix1SsWNHbradMmeKhO2fOnDZs2LDwOVTx3rRpk82fP99Dd758+bwCrnNdq7J8PZcvX7aJEyd6hV1U6VZ4108FblGw11x0Pa7xphVdTDh27Jh9++23XrF/9tlnr3ms3p9uIbrIAAAAAABIX1EdupPat2+fV5pbtmyZ6PFLly55C3bIhAkTbNq0aR6Ez58/78/XrVs3TcaQK1cun1sdomq2WtqrVq2a6DgFXlXC05Layc+ePWt//vOf7Re/+IVVrlzZ286To46AyIsPAAAAAID0l6lCtwKnLFu2zOdPR9L8alGLtyrN48aN82q4KuVjx461zZs3X/fcocXQtEVZZFU7KVXJNc87ckyqxG/dutV/RlJVPS2pai9a6fzrr7/2ave1QvfgwYNtwIABiSrdZcuWTdPxAAAAAACyUOiOXLxMreTJ2bBhg6903rt370QroCetVqs6HUlzwkXbkhUuXNh/T82e4Kqw61xHjx615s2bW3rR3PXI9vGk9DmFLkQAAAAAADJGpgrdqlqrit2/f38Pnc2aNbNTp0550C5QoIB17drVFy6bNWuWrVy50ivDs2fP9kXYQlVi0Qrgen7Pnj3eAq6542rVViVY1eORI0fa3r17vVqeErWVd+rUybp06eLHK4Rr3vXq1au9Db1NmzYpnkOLsKkF/sSJE3bmzJlw2A+1xKtdvly5cr4/t2jxuFdffdW3GQMAAAAARK9MFbpl+PDhXpXWnOUDBw74Fl7169e3l156yZ/X/tVa3btDhw7eBq72a1W9ly9fHj6HtufSll4NGjTw9vA1a9bYgw8+aHPnzrXnnnvOw3LDhg19dfAnn3wyxTFpwTQdO3DgQPvqq6+saNGi1rhxY2vbtm2q3tNjjz1mX375Zfh+aH56qNVdFxjULv7FF19Yjhw5fEV3raCu9woAAAAAiF4xCZGTmJFlaU63KvrqDFBXAAAAAAAg+Iz1f6uHAQAAAACANEfoDljr1q19FfPkbmm5hzcAAAAAIPpkujndmU18fLzvFZ6c2NjYdB8PAAAAACD9ELoDlnQ/cQAAAADA7YP2cgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgmkL3/v377eWXX7aOHTva0aNH/bHly5fb3/72N8sICQkJ1rNnT4uNjbWYmBjbvn17howDAAAAAIBbCt0ffvih1apVyzZv3mzvvvuunT171h//y1/+Yr/+9a8tI6xYscJmzJhhS5cutcOHD1vNmjVv+ZzdunWzuLg4iwYXLlzw8ehzz5EjR9SMCwAAAACQxqH7F7/4hY0YMcLef/99y5UrV/jx73//+/bnP//ZMoIq7yVLlrSmTZtaiRIlPJhGiytXrtjVq1dv+Rx58uSxfv362cMPP5xmYwMAAAAARFno3rFjh/3oRz/6zuPFixe3b775xtKbKsB9+/a1Q4cOeWt5+fLlPeSOHj3aKlSo4GG1Tp06tnDhwkQhtnv37uHnq1WrZuPHjw8/P3ToUJs5c6YtXrzYz6nb2rVr/abfT548GT5Wrex67ODBg35fFfdChQrZkiVLrEaNGpY7d24f28WLF23QoEFWunRpy5s3rzVq1MjPlxo6ftKkSdajRw+/qAAAAAAAyBxuuCSsQKkWbgXWSNu2bfNAmd4UlitVqmRTp061LVu2WPbs2T1wz5kzxyZPnmxVqlSxdevWWefOna1YsWLWokULD+VlypSxBQsWWJEiRWzjxo0+J1zV8vbt23s43rVrl50+fdqmT5/ur6P54jouNc6dO2djxoyx+Ph4P78uSPTp08d27txp8+bNs1KlStmiRYusVatWfhFDY0xrCvm6hei9AAAAAACiPHT/13/9l/385z/3wKoKrwLshg0bPKh26dLF0lvBggUtf/78HrZVBVbQHDVqlK1atcqaNGnix1SsWNHWr19vU6ZM8dCdM2dOGzZsWPgcuoCwadMmmz9/vofufPnyeQVc57qZyvLly5dt4sSJXmEXVboV3vVTgVv0eWkuuh7XeNOaLjxEvkcAAAAAQCYI3QqIzz//vJUtW9bbtNVCrZ9PPfWUr2ie0fbt2+eV5pYtWyZ6/NKlS1avXr3w/QkTJti0adM8CJ8/f96fr1u3bpqMQXPda9euHb6varY+o6pVqyY6TqFelfAgDB482AYMGJCo0q1/MwAAAABAlIZubc115MgR++///m/71a9+5WFSq5crzAbRIn0zQqupL1u27Dvt7ppfLWrxVqV53LhxXg1XpXzs2LG+Ivv1ZMuWLfw5RFa1k1KVXF0AkWNSJX7r1q3+M5Kq6kHQew29XwAAAABAJgndlStX9v24FbKjsXIauXiZWsmTo3Z4rXTeu3fvRCugJ61WqzodSXPCRXPaCxcu7L+nZk9wXZTQubSnefPmzW/qfQEAAAAAsnjoVqVXYfv48eNRU9lOSlVrVbH79+/v882bNWtmp06d8qBdoEAB69q1q4991qxZtnLlSp/PPXv2bF+ELXJxOK2Cruf37NnjLeCaO64LDrrQoNXNR44caXv37vVqeUrUVt6pUyef867jFcKPHTtmq1ev9jb0Nm3apHgOLcKmFvgTJ07YmTNnwmE/rVriAQAAAABRsGXYb37zG/vZz35mf/3rXy1aDR8+3IYMGeKLiVWvXt1XCVe7eShU9+rVy9q1a2cdOnTwrbt0ESGy6i3anktbiTVo0MAr3ArtWoBt7ty5tnv3bg/LWqFce5anhhZMU+geOHCgnzcuLs6Dfrly5VL194899piH9T/+8Y++1Zh+j5yjDgAAAACIPjEJkROUU0Ft1Vqo7Ntvv/UWbM1fjqRKLKKPFlJTtV5Vf1X8AQAAAADBZ6wbXr389ddfv4VhAQAAAABw+7jh0K050UhbrVu3to8++ijZ51566SW/AQAAAABug9CtVcGvJ7VzlPH/xcfH+17hyYmNjU338QAAAAAAMih0a1XvyD2ok0q6zRZSlnQ/cQAAAADAbRq6t23bluj+5cuX/bHf/e53vo0WAAAAAAC4ydBdp06d7zymbbVKlSplY8eO9a24AAAAAADATezTfS3ae1r7TgMAAAAAgJusdGsvskja5vvw4cM2dOhQq1Klyo2eDgAAAACALOuGQ3ehQoW+s5CagnfZsmVt3rx5aTk2AAAAAABur9C9Zs2aRPezZctmxYoVs8qVK1uOHDd8OgAAAAAAsqwbTsmqcjdt2vQ7Afvbb7+1devW2QMPPJCW4wMAAAAA4PZZSO2hhx6yEydOfOfxU6dO+XMAAAAAAOAmQ7fmbyed0y3Hjx+3vHnz3ujpAAAAAADIslLdXh7af1uBu1u3bpY7d+7wc1euXLHPPvvM284BAAAAAMANhu6CBQuGK9358+e3PHnyhJ/LlSuXNW7c2Hr06JHa0wEAAAAAkOWlOnRPnz7df5YvX94GDRpEKzkAAAAAACmISVDpGlne6dOnvVtBC94VKFAgo4cDAAAAALdFxrqpjbUXLlxo8+fPt0OHDtmlS5cSPffpp5/ezCkBAAAAAMhybnj18v/+7/+2p59+2u68807btm2b3XfffVakSBE7cOCAtW7dOphRAgAAAABwO4TuiRMn2tSpU+2NN97wBdRefPFFe//9961fv35eVgcAAAAAADcZutVSHtoaTCuYnzlzxn//8Y9/bHPnzr3R0wEAAAAAkGXdcOguUaKEnThxwn8vV66c/fnPf/bfv/jiC99ODAAAAAAA3GTo/v73v29Llizx3zW3u3///tayZUvr0KGD/ehHP7rR0wEAAAAAkGXd8JZhV69e9VuOHP+38Pm8efNs48aNVqVKFevVq5fP805vegt6ba2q/q9//csXeKtbt266jyOasWUYAAAAAKR/xsoS+3QvX77cfvjDH9ratWutYsWKVrRo0fBFgZvVrVs3O3nypP3hD3+wjHbw4EGrUKHCdx7ftGmTNW7cOFXnIHQDAAAAQNpJbca64fZy+eijj6xz587WpEkT++qrr/yx2bNn2/r16y0j7N+/30qWLOkLvGnO+a0G7rR05coV7wxIC6tWrbLDhw+Hb/fee2+anBcAAAAAEIwbDt3vvPOOPfroo75yudq4L1686I8r3Y8aNcrSmyrSffv29VXVY2JirHz58h5yR48e7dVhjbNOnTreeh4ZhLt37x5+vlq1ajZ+/Pjw80OHDrWZM2fa4sWL/Zy6qYqum35XBTxk+/bt/piq0TJjxgwrVKiQz3uvUaOG5c6d28emz2nQoEFWunRpy5s3rzVq1MjPdyO0H7ouKoRuOXPmTJPPEAAAAAAQjBsuCY8YMcImT55sXbp08fncIffff78/l94UlitVquR7h2/ZssWyZ8/ugXvOnDk+Ts01X7dunVfmixUrZi1atPBQXqZMGVuwYIEHWc1J79mzp1fL27dv7+F4165d3i4wffp0f53Y2Fg/LjXOnTtnY8aMsfj4eD9/8eLFrU+fPrZz507/zEqVKmWLFi2yVq1a2Y4dO3yMqfGDH/zALly4YFWrVvX90XX/WhTyQxdERO8FAAAAABDloXvPnj32wAMPfOdx9bJHVoDTi143f/78HrZV/VXQVMVdrdhqfxfN81br+5QpUzx0q0I8bNiw8DlU8db86Pnz53vozpcvn1fAdS6d80ZdvnzZJk6c6BV2UaVb4V0/FbhFwX7FihX+eEodAhrPuHHj/MJGtmzZvNsgLi7O55tfK3jrwkPkewQAAAAAZILQrRC6b98+b+OOpFCrcJvRNDZVmrWNWaRLly5ZvXr1wvcnTJhg06ZN8yB8/vx5fz6tVjzXCu61a9cO31c1Wy3tqlBHUqhXJTwlWhhuwIAB4fsNGza0f/7znzZ27Nhrhu7Bgwcn+htVusuWLXuT7wgAAAAAkC6hu0ePHvbCCy94YNVcZoU/VYlVuR0yZIhltLNnz/rPZcuW+fzpSJpfLWrx1nhVPVY1XJVyBdjNmzdf99yqMkvkgu+qaielKrk+m8gxqRK/detW/5m0in0zNCf8/fffv+bzeq+h9wsAAAAAiOLQ/dlnn1nNmjU9dKqCqjnR//Ef/+EVZbWaK9wpxGpBs4wWuXiZWsmTs2HDBl/pvHfv3olWQE9arVZ1OpLmhItWDi9cuHB4IbWUqMKucx09etSaN29uaUGvqznoAAAAAIBMHroVGhU0tSCYWsi1YNnPfvYzb+VWFVdB92YrtmlNVWtdAOjfv79fHGjWrJmvrK6grb3Tunbt6guXzZo1y1auXOnzubXdmd5T5F7Yap/X85rDrhZwzR2vXLmyt2hrdfORI0fa3r17vVqeErWVd+rUyRef0/H6PI8dO2arV6/2NvQ2bdpc9++1krouAoTa4999913vNNBCbQAAAACATB66tQXWF1984aFbW2MpzCoEKmxHo+HDh3tVWouJHThwwMdfv359e+mll/z5Xr16+XZnHTp08Dbwjh07etV7+fLlidrotaVXgwYN/MLCmjVr7MEHH7S5c+fac88952FZc6u1YvuTTz6Z4pi0YJqOHThwoO9trnnajRs3trZt26b6PX355Ze+B/ndd99tb7/9tj3xxBO38CkBAAAAAIIWkxA5QfkatJ2WKsNqZ1bbtrbbSjo3OUQhF9FHC6mpWq+qvyr+AAAAAIDgM1aqKt3aA7tdu3beTt6vXz+vAquNGwAAAAAApMHq5a1atfKfWoFbq5cTutNO69at7aOPPkr2ObXEh9riAQAAAABZsL0cwdIcb+0VnpzY2Fi/3SraywEAAAAgStvLEayk+4kDAAAAALKGbBk9AAAAAAAAsipCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAcLuG7oSEBOvZs6fFxsZaTEyMbd++PaOHBAAAAABA1gjdK1assBkzZtjSpUvt8OHDVrNmzVs+Z7du3SwuLs6iwYULF3w8tWrVshw5ciQ7rnfffddatmxpxYoVswIFCliTJk1s5cqVGTJeAAAAAEAWCt379++3kiVLWtOmTa1EiRIeTKPFlStX7OrVq7d8jjx58li/fv3s4YcfTvaYdevWeeh+7733bOvWrfbQQw/Z448/btu2bbul1wYAAAAA3MahWxXgvn372qFDh7y1vHz58h5yR48ebRUqVPCwWqdOHVu4cGGiENu9e/fw89WqVbPx48eHnx86dKjNnDnTFi9e7OfUbe3atX7T7ydPngwfq1Z2PXbw4EG/r4p7oUKFbMmSJVajRg3LnTu3j+3ixYs2aNAgK126tOXNm9caNWrk50sNHT9p0iTr0aOHX1RIzuuvv24vvviiNWzY0KpUqWKjRo3yn3/84x9v4dMFAAAAAAQtesrGyVBYrlSpkk2dOtW2bNli2bNn98A9Z84cmzx5sgdPVYE7d+7srdctWrTwUF6mTBlbsGCBFSlSxDZu3OhzwlUtb9++vYfjXbt22enTp2369On+OpovruNS49y5czZmzBiLj4/38xcvXtz69OljO3futHnz5lmpUqVs0aJF1qpVK9uxY4ePMa3pPZ45c8bHfS26EKBbiN4vAAAAACB9RXXoLliwoOXPn9/DtqrACpGq8q5atcrnNUvFihVt/fr1NmXKFA/dOXPmtGHDhoXPoYr3pk2bbP78+R668+XL5xVwnetaleXruXz5sk2cONEr7KJKt8K7fipwi4K95qLrcY03rb366qt29uxZfz/XoosTkZ8DAAAAACD9RXXoTmrfvn1eadb85kiXLl2yevXqhe9PmDDBpk2b5kH4/Pnz/nzdunXTZAy5cuWy2rVrh++rmq2W9qpVqyY6TqFelfC09tZbb3mYVnu8quzXMnjwYBswYECiSnfZsmXTfDwAAAAAgCwSulXdlWXLlvn86UiaXy1q8Valedy4cV4NV6V87Nixtnnz5uueO1u2bOEtyiKr2kmpSq553pFjUiVeC5zpZyRV1dOS3tuzzz7rrfPXWnQt8vMIfSYAAAAAgIyRqUJ35OJlaiVPzoYNG3yl8969eydaAT1ptVrV6UiaEy7alqxw4cL+e2r2BFeFXec6evSoNW/e3IIyd+5ce+aZZzx4t2nTJrDXAQAAAADcpqFbVWtVsfv37++LiTVr1sxOnTrlQVv7V3ft2tUXLps1a5bvY6353LNnz/ZF2PR7iFZB1/N79uzxFnDNHa9cubK3X2t185EjR9revXu9Wp4StZV36tTJunTp4scrhB87dsxWr17tbeipCchahE0t8CdOnPAF0kJhP9QSr5ZyvTctLKeV0Y8cORKuumvsAAAAAIDoFNVbhiVn+PDhNmTIEF8orHr16r5KuNrNQ6G6V69e1q5dO+vQoYMH1OPHjyeqeou259JWYg0aNPAKt0K7FmBTNXn37t0elrVC+YgRI1I1Ji2YptA9cOBAP29cXJwH/XLlyqXq7x977DEP69oCTFuN6ffIOepavf3bb7+1559/3ldhD91eeOGFG/rsAAAAAADpKyYhchIzsiwtpKaquDoD1BUAAAAAAAg+Y2W6SjcAAAAAAJkFoTtgrVu39lXMk7sFsYc3AAAAACB6ZKqF1DKj+Ph43ys8ObGxsek+HgAAAABA+iF0ByzpfuIAAAAAgNsH7eUAAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAANyuoTshIcF69uxpsbGxFhMTY9u3b8/oIQEAAAAAkDVC94oVK2zGjBm2dOlSO3z4sNWsWfOWz9mtWzeLi4uzaLqw8Oqrr1rVqlUtd+7cVrp0aRs5cmT4+XfffddatmxpxYoVswIFCliTJk1s5cqVGTpmAAAAAEDKcliU279/v5UsWdKaNm1q0ebKlStefc+W7dauXbzwwgv2pz/9yYN3rVq17MSJE34LWbdunYfuUaNGWaFChWz69On2+OOP2+bNm61evXpp8E4AAAAAALddpVsV6b59+9qhQ4c83JYvX96uXr1qo0ePtgoVKliePHmsTp06tnDhwkRBuHv37uHnq1WrZuPHjw8/P3ToUJs5c6YtXrzYz6nb2rVr/abfT548GT5Wrex67ODBg35fFXeF3iVLlliNGjW8Kq2xXbx40QYNGuQV6rx581qjRo38fKmxa9cumzRpko/nBz/4gY/73nvv9ZAd8vrrr9uLL75oDRs2tCpVqnj41s8//vGPafRJAwAAAABuu0q3wnKlSpVs6tSptmXLFsuePbsH7jlz5tjkyZM9eKoK3LlzZ2+9btGihYfyMmXK2IIFC6xIkSK2ceNGnxOuann79u09HCvonj592ivGovniOi41zp07Z2PGjLH4+Hg/f/Hixa1Pnz62c+dOmzdvnpUqVcoWLVpkrVq1sh07dvgYr0fBuWLFit4+r79Rq/nDDz9sv/3tb31cydF7PHPmzDWfBwAAAABEh6gO3QULFrT8+fN72C5RooRXlFXlXbVqlc9rFgXW9evX25QpUzx058yZ04YNGxY+hyrHmzZtsvnz53vozpcvn1fAdS6d80ZdvnzZJk6c6BV2UaVb4V0/FbhFwV5z0fW4xns9Bw4csC+//NIvEsyaNcsr9f3797cnnnjCPvjgg2T/Rm3oZ8+e9fdzLXp/uoXoIgMAAAAAIH1FdehOat++fV5pjmy9lkuXLiWa2zxhwgSbNm2aB+Hz58/783Xr1k2TMeTKlctq164dvq9qtoKyFkGLpMCrSnhKVLXWsQrcoXP87//+r7eY79mzx9vjI7311lt+UUHt6KqyX4s6AiIvPgAAAAAA0l+mCt2q7sqyZct8/nQkza8WtXir0jxu3DivhqtSPnbsWF907HpCi6GpvTuyqp2UquSa5x05JlXit27d6j8jqaqeErW958iRI1For169uv/URYPI0K339uyzz3pVXC3o1zN48GAbMGBAokp32bJlUxwPAAAAAOA2Dd2Ri5eplTw5GzZs8JXOe/funWgF9KTValWnI2lOuGhbssKFC/vvqdkTXBV2nevo0aPWvHnzG35P999/v3377bc+Rs1fl7179/rPu+66K3zc3Llz7ZlnnvHg3aZNmxTPq88pdCECAAAAAJAxMlXoVtVaVWzNeVZbdrNmzezUqVMetLV/ddeuXX3hMrVqax9rzeeePXu2L8Km30O0CrqeV/u2WsA1d7xy5cpeCdbq5tojW8FX1fKUqELdqVMn69Klix+vEH7s2DFbvXq1t6GnFJBVsa5fv74Haq1Srvf1/PPPewt9qPqtlnK9Ny0sp5XRjxw5Eq66a+wAAAAAgOgU1VuGJWf48OE2ZMgQn7OsNmyt+K1281Co7tWrl7Vr1846dOjgAfX48eOJqt7So0cPb9tu0KCBV7gV2rUAm6rJu3fv9rCsFcpHjBiRqjFpwTSF7oEDB/p54+LiPOiXK1cuxb9VW7tWMC9atKg98MADHtL1vlTRDtHq7aqGK4yrHT100/7eAAAAAIDoFZMQOYkZWZbmdKsqrs4AdQUAAAAAAILPWJmu0g0AAAAAQGZB6A5Y69atfRXz5G4p7eENAAAAAMjcMtVCaplRfHy87xWenNjY2HQfDwAAAAAg/RC6A5Z0P3EAAAAAwO2D9nIAAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAghG4AAAAAALJi6E5ISLCePXtabGysxcTE2Pbt2zNyOAAAAAAAZJ3QvWLFCpsxY4YtXbrUDh8+bDVr1rzlc3br1s3i4uIsGly4cMHHU6tWLcuRI0ey41q7dq1fcEh6O3LkSKLjJkyYYOXLl7c77rjDGjVqZB9//HE6vhMAAAAAQKYL3fv377eSJUta06ZNrUSJEh5Mo8WVK1fs6tWrt3yOPHnyWL9+/ezhhx++7rF79uzxCw+hW/HixcPPvf322zZgwAD79a9/bZ9++qnVqVPHHn30UTt69OgtjQ8AAAAAkEVDtyrAffv2tUOHDnllV1VchdzRo0dbhQoVPKwqXC5cuDBRiO3evXv4+WrVqtn48ePDzw8dOtRmzpxpixcvDleMVUkOVZNPnjwZPlat7Hrs4MGDfl8V90KFCtmSJUusRo0aljt3bh/bxYsXbdCgQVa6dGnLmzevV5l1vtTQ8ZMmTbIePXr4RYXrUcjWMaFbtmz//5/md7/7nZ/j6aef9rFNnjzZvve979m0adNu6DMHAAAAAKSvDCstKyxXqlTJpk6dalu2bLHs2bN74J4zZ46HyipVqti6deusc+fOVqxYMWvRooWH8jJlytiCBQusSJEitnHjRp8Trmp5+/btPRzv2rXLTp8+bdOnT/fX0XxxHZca586dszFjxlh8fLyfX0G4T58+tnPnTps3b56VKlXKFi1aZK1atbIdO3b4GNNK3bp1PeCrxV4XD+6//35//NKlS7Z161YbPHhw+FgFclXON23alGavDwAAAADIQqG7YMGClj9/fg/bquwqcI4aNcpWrVplTZo08WMqVqxo69evtylTpnjozpkzpw0bNix8DlW8FTznz5/voTtfvnxeAde5UqosJ+fy5cs2ceJEr7CLKt0K7/qpwC0K9pqLrsc13lulCwa6yNCgQQMftwL/gw8+aJs3b7b69evbN9984xX+O++8M9Hf6f7u3buveV6dS7cQXYgAAAAAAKSvqJlEvW/fPq80t2zZMtHjqvTWq1cv0YJiaqtWED5//rw/rypxWsiVK5fVrl07fF/VbAXeqlWrJjpOYVaV8LSgFnndQjS/XXPdX3vtNZs9e/ZNn1ddA5EXKAAAAAAAt3HoPnv2rP9ctmyZz5+OpPnVohZvVZrHjRvn1XBVyseOHetV4esJzY/WFmWRVe2kVCXXPO/IMakSr/Zu/YykqnpQ7rvvPq/wS9GiRf21v/7660TH6P71qvlqR9fia5GV7rJlywY2ZgAAAABAFIfuyMXL1EqenA0bNngluHfv3uHHVBVOWq1WdTqS5oSLVgUvXLiw/56aPcFVYde5tEp48+bNLb1obGo7D72fe++911avXh3eckxz23Vf882vRZ9l6GIFAAAAAOA2D92qWquK3b9/fw+VzZo1s1OnTnnQLlCggHXt2tUXLps1a5atXLnS53Or/VqLsOn3EK2Crue1BZdawDV3vHLlyl7l1QJlI0eOtL1793q1PCVqK+/UqZN16dLFj1cIP3bsmAdetaG3adMmxXNoETa1wJ84ccLOnDkTDvuhlvjXX3/dx3/PPff4vt6a0/3BBx/Yn/70p/A5VLHW+9e8b1XB9Tf//ve/fTVzAAAAAED0iprQLcOHD/eqtOYjHzhwwLfw0mJiL730kj/fq1cv27Ztm3Xo0MHbwDt27OhV7+XLl4fPoa21tKWXAqraw9esWeMLk82dO9eee+45D8sNGza0ESNG2JNPPpnimLRgmo4dOHCgffXVV97u3bhxY2vbtm2q3tNjjz1mX375Zfh+aH56qNVdgTx0bm0DpvFpMbmHHnoo/Dd6vwr7v/rVr+zIkSMe2LWYW9LF1QAAAAAA0SUmIXKiM7IszelW1V/dA+ocAAAAAAAEn7H+b4UxAAAAAACQ5gjdt6B169a+inlyt7TYwxsAAAAAkLlF1ZzuzEaLnmmv8OTExsam+3gAAAAAANGF0H0Lku4nDgAAAABAJNrLAQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICsGLoTEhKsZ8+eFhsbazExMbZ9+/aMHA4AAAAAAFkndK9YscJmzJhhS5cutcOHD1vNmjVv+ZzdunWzuLg4iwYXLlzw8dSqVcty5MhxzXFdvHjRfvnLX9pdd91luXPntvLly9u0adPCz7/77rvWoEEDK1SokOXNm9fq1q1rs2fPTsd3AgAAAAC4GTksA+3fv99KlixpTZs2tWhz5coVr75ny5btls6RJ08e69evn73zzjvXPK59+/b29ddf2//+7/9a5cqV/QLE1atXw8+rE0Ch/O6777ZcuXL5RYqnn37aihcvbo8++uhNjw8AAAAAkEUr3aoA9+3b1w4dOuThVtVdBc3Ro0dbhQoVPKzWqVPHFi5cmCjEdu/ePfx8tWrVbPz48eHnhw4dajNnzrTFixf7OXVbu3at3/T7yZMnw8eqlV2PHTx40O+r4q5K8pIlS6xGjRpecdbYVIUeNGiQlS5d2qvMjRo18vOlho6fNGmS9ejRw0qUKHHNav+HH35o7733nj388MP+OTRp0sTuv//+8DEPPvig/ehHP7Lq1atbpUqV7IUXXrDatWvb+vXrb+qzBwAAAABk8Uq3wrIC5NSpU23Lli2WPXt2D9xz5syxyZMnW5UqVWzdunXWuXNnK1asmLVo0cJDeZkyZWzBggVWpEgR27hxo88JV7Vc1WKF4127dtnp06dt+vTp4SqxjkuNc+fO2ZgxYyw+Pt7Pr0pynz59bOfOnTZv3jwrVaqULVq0yFq1amU7duzwMd4qhXy1jv/2t7/1lnEF9R/84Ac2fPhwv7CQ3Dz4Dz74wPbs2eNjvRZdLNAtRJ8JAAAAAOA2Cd0FCxa0/Pnze9hWFVgBcdSoUbZq1Sqv9ErFihW9mjtlyhQP3Tlz5rRhw4aFz6GK96ZNm2z+/PkeuvPly+dBVee6VmX5ei5fvmwTJ070Cruo0q3wrp8K3KJgr+q0Htd4b9WBAwf8Pd5xxx0e6L/55hvr3bu3HT9+PHzhQE6dOuXVdr03fWYaZ8uWLa95Xl3AiPysAAAAAAC32ZzuSPv27fNKc9IgeenSJatXr174/oQJE3yRMQXh8+fP+/NaWCwtaL602rZDVM1WS3vVqlUTHafgq0p4WlD1Xm3ub775pl+IkN/97nf2xBNPeLAOVbt1gUIt8WfPnrXVq1fbgAED/KKEWs+TM3jwYD8mstJdtmzZNBkzAAAAACCThW6FSVm2bJlXdCNpfrWoxVuV5nHjxnk1XEF07Nixtnnz5uueO7QYmlqzI6vaSSngKgBHjklV5a1bt/rPSKqqpwW1xuv9hgK3aO62xvqPf/wj3MKu96BF1kQXGdRGr2r2tUK3PrPQ5wYAAAAAuM1Dd+TiZWolT86GDRt8pXO1X0eugJ60Wq3qdCTNCRetCl64cGH/PTV7gqvCrnMdPXrUmjdvbkHQgmmao66AHwrye/fu9ZCt+evXq5BHztkGAAAAAESfDN2nO5Kq1qpi9+/f31cgV5j+9NNP7Y033vD7oqrvJ598YitXrvRgOmTIEF+ELZJW//7ss898oTHNj1ZFWxVitVZrdfPPP//cq+mqlqdEbeWdOnWyLl26+F7ZX3zxhX388cdeYdY5UkOLsCngnzhxwudl6/fIwP/UU095q7q2ANOxWjzuZz/7mT3zzDPh1nK93vvvv+/zv1Xh1ti16JoWmQMAAAAARK+oqXSLVuxWVVohUwFTW3jVr1/fXnrpJX++V69etm3bNuvQoYO3gXfs2NGr3suXLw+fQ9tzaUsvrQiu6vGaNWu8BXvu3Ln23HPP+Zzthg0b2ogRI+zJJ59McUxazEzHDhw40L766isrWrSoNW7c2Nq2bZuq9/TYY4/Zl19+Gb4fmp8eanVXdVuBWtunacwK4FoUTq8Z8u9//9vfp9rNFcS1X7dWedfnAAAAAACIXjEJkROdkWVpITXNG1e1vUCBAhk9HAAAAAC4LTJW1LSXAwAAAACQ1RC6b0Hr1q29PTy5W1rs4Q0AAAAAyNyiak53ZhMfH+97hScnNjY23ccDAAAAAIguhO5bkHQ/cQAAAAAAItFeDgAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAADA7Rq6ExISrGfPnhYbG2sxMTG2ffv2jB4SAAAAAABZI3SvWLHCZsyYYUuXLrXDhw9bzZo1b/mc3bp1s7i4OIsGe/bssYceesjuvPNOu+OOO6xixYr28ssv2+XLl8PH/P73v7fmzZtb4cKF/fbwww/bxx9/nKHjBgAAAACkLIdFuf3791vJkiWtadOmFm2uXLni1fds2W7+2kXOnDmtS5cuVr9+fStUqJD95S9/sR49etjVq1dt1KhRfszatWutY8eO/hkomI8ZM8YeeeQR+9vf/malS5dOw3cEAAAAALhtKt2qSPft29cOHTrk4bZ8+fIeRkePHm0VKlSwPHnyWJ06dWzhwoWJgnD37t3Dz1erVs3Gjx8ffn7o0KE2c+ZMW7x4sZ9TN4Va3fT7yZMnw8eqlV2PHTx40O+r4q5gvGTJEqtRo4blzp3bx3bx4kUbNGiQB+C8efNao0aN/Hypocr2008/7e/jrrvush/84AfWqVMn++ijj8LHvPnmm9a7d2+rW7eu3X333RYfH++fw+rVq9PokwYAAAAA3HaVboXlSpUq2dSpU23Lli2WPXt2D9xz5syxyZMnW5UqVWzdunXWuXNnK1asmLVo0cLDaJkyZWzBggVWpEgR27hxo88JV7W8ffv2Ho537dplp0+ftunTp/vraL64jkuNc+fOeaVZwVfnL168uPXp08d27txp8+bNs1KlStmiRYusVatWtmPHDh/jjdi3b5+31Ldr1+66Y1D7ucZ9LboQoFuI3i8AAAAAIH1FdeguWLCg5c+f38N2iRIlPESq5XrVqlXWpEmTcKV4/fr1NmXKFA/datceNmxY+ByqeG/atMnmz5/voTtfvnxeAde5dM4bpbA7ceJEr0yLKt0K7/qpwC0K9grOejzUIp4StY5/+umnPi5dJHjllVeueezPf/5zfy3N7b4WXZyI/BwAAAAAAOkvqkN3clVgVXlbtmyZ6PFLly5ZvXr1wvcnTJhg06ZN8yB8/vx5f16t2WkhV65cVrt27fB9VbPV0l61atVExyk8qxKeWm+//badOXPG53T/7Gc/s1dffdVefPHF7xz3m9/8xivqal/X/O5rGTx4sA0YMCBRpbts2bKpHg8AAAAA4DYL3WfPnvWfy5Yt+84CYppfLQqkqjSPGzfOq+GqlI8dO9Y2b9583XOHFkPTFmUhkSuIh6hKrnnekWNSJX7r1q3+M5Kq6qkVCsSaK64Qr2r3wIEDE51TQVyhW5X+yOCfHH0eoc8EAAAAAJAxMlXojly8TK3kydmwYYO3amvhscgV0JNWqxVsI2lOuGhbMm3LJanZE1wVdp3r6NGjvq1XWtC8dAV+/QyF7t/+9rc2cuRIW7lypTVo0CBNXgcAAAAAEKxMFbpVtVYVu3///h5ImzVrZqdOnfKgXaBAAevatasvXDZr1iwPp5rPPXv2bF+ETb+HaBV0Pa89stUCrrnjlStX9mqzVjdXuN27d69Xy1OitnKtNq5tv3S8QvixY8d8ZXFVo9u0aXPdv9fK5JqHXqtWLb+g8Mknn3hreIcOHfxx0cJtv/rVr+ytt97ysR85ciRcSb+RajoAAAAAIH1F9ZZhyRk+fLgNGTLEFwqrXr26rxKudvNQqO7Vq5ev/K3Qqq27jh8/nqjqLdoHW1uJqWKsCrdCuwLu3Llzbffu3R6WFXRHjBiRqjFpwTSFbrWD67xxcXEe9MuVK5fi3+bIkcNf67777vPX1eJnWg1dq6OHTJo0yeelP/HEE74Ke+imdnMAAAAAQPSKSYicxIwsSwupqaKvzgB1BQAAAAAAgs9Yma7SDQAAAABAZkHoDljr1q3Dc6+T3lK7hzcAAAAAIHPKVAupZUaam629wpMTGxub7uMBAAAAAKQfQnfAku4nDgAAAAC4fdBeDgAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdAAAAAAAEhNANAAAAAEBACN0AAAAAAASE0A0AAAAAQEAI3QAAAAAABITQDQAAAABAVgzdCQkJ1rNnT4uNjbWYmBjbvn17Rg4HAAAAAICsE7pXrFhhM2bMsKVLl9rhw4etZs2at3zObt26WVxcnEWDCxcu+Hhq1aplOXLkSHZcet9PPfWUVa1a1bJly2Y//elPkz3X66+/btWqVbM8efJY2bJlrX///n5+AAAAAED0ypGRL75//34rWbKkNW3a1KLNlStXvPquIHwr51BI7tevn73zzjvJHnPx4kUrVqyYvfzyy/baa68le8xbb71lv/jFL2zatGn+We3du9fDvMb3u9/97qbHBwAAAADIopVuhca+ffvaoUOHPDyWL1/erl69aqNHj7YKFSp4WK1Tp44tXLgwUYjt3r17+HlVfsePHx9+fujQoTZz5kxbvHixn1O3tWvX+k2/nzx5MnysWtn12MGDB/2+Ku6FChWyJUuWWI0aNSx37tw+NoXiQYMGWenSpS1v3rzWqFEjP19q6PhJkyZZjx49rESJEskeo/et99ClSxcrWLBgssds3LjR7r//fq+I6/hHHnnEOnbsaB9//HGqP28AAAAAwG1U6VbQrFSpkk2dOtW2bNli2bNn98A9Z84cmzx5slWpUsXWrVtnnTt39kpwixYtPJSXKVPGFixYYEWKFPEwqjnhqpa3b9/ew/GuXbvs9OnTNn36dH8dzRfXcalx7tw5GzNmjMXHx/v5ixcvbn369LGdO3favHnzrFSpUrZo0SJr1aqV7dixw8eYHlTd1ueikH3ffffZgQMH7L333rMf//jH1/wbXSzQLUSfCQAAAADgNgndqurmz5/fw7aqwAqIo0aNslWrVlmTJk38mIoVK9r69ettypQpHrpz5sxpw4YNC59DFe9NmzbZ/PnzPXTny5fPK+A617Uqy9dz+fJlmzhxolfYRZVuhXf9VOAWBXvNRdfjGm96UIX7m2++sWbNmvnic99++6395Cc/sZdeeumaf6MLGJGfFQAAAADgNpvTHWnfvn1eaW7ZsmWixy9dumT16tUL358wYYLPbVYQPn/+vD9ft27dNBlDrly5rHbt2uH7qmarpV2LnEVSqFclPL2onV0BXxcE1N6uz+qFF16w4cOH25AhQ5L9m8GDB9uAAQMSVbq1ABsAAAAA4DYM3WfPnvWfy5Yt8/nTkTS/WtTirUrzuHHjvBquSvnYsWNt8+bN1z13aDE0VYkjq9pJqUqued6RY1IlfuvWrf4zkqrq6UXBWq3kzz77rN/Xauj//ve/vbX+l7/8ZbKLvekzC31uAAAAAIDbPHRHLl6mVvLkbNiwwec39+7dO9EK6Emr1apOR9Kc8ND2XIULF/bfU7MnuCrsOtfRo0etefPmllHUAZA0WIcuAkReSAAAAAAARJeoCd2qWquKrf2ntWCa5i+fOnXKg3aBAgWsa9euvnDZrFmzbOXKlT6fe/bs2b4Im34P0ereen7Pnj3eAq6545UrV/bWaq1uPnLkSN9yS9XylKitvFOnTr6yuI5XCD927JitXr3a29DbtGmT4jm0CJta4E+cOGFnzpwJh/3IlvjQY6qs6/y6r4sHuhAhjz/+uG8NptcPtZer+q3Hk1bgAQAAAADRI2pCt2iOsqrSWgRMK3RrC6/69euHFwzr1auXbdu2zTp06OBt4No2S1Xv5cuXh8+h7bk0B7pBgwYeYtesWWMPPvigzZ0715577jkPyw0bNrQRI0bYk08+meKYtGCajh04cKB99dVXVrRoUWvcuLG1bds2Ve/psccesy+//DJ8PzQ/PbJCHTlnXa3s2pf7rrvuCm9npj289X71U2PQZ6TArQsIAAAAAIDoFZNAf/JtQQupqeqv7gF1DgAAAAAAgs9Y312BCwAAAAAApAlC9y1o3bq1r2Ke3C299vAGAAAAAESvqJrTndnEx8f7XuHJiY2NTffxAAAAAACiC6H7FiTdTxwAAAAAgEi0lwMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAEBBCNwAAAAAAASF0AwAAAAAQEEI3AAAAAAABIXQDAAAAABAQQjcAAAAAAAEhdAMAAAAAcLuG7oSEBOvZs6fFxsZaTEyMbd++PaOHBAAAAABA1gjdK1assBkzZtjSpUvt8OHDVrNmzVs+Z7du3SwuLs6iwYULF3w8tWrVshw5clxzXGvXrrX69etb7ty5rXLlyv6ZAAAAAACiW9SH7v3791vJkiWtadOmVqJECQ+m0eLKlSt29erVWz5Hnjx5rF+/fvbwww8ne8wXX3xhbdq0sYceesgr/T/96U/t2WeftZUrV97SawMAAAAAbuPQrQpw37597dChQ95aXr58eQ+5o0ePtgoVKnhYrVOnji1cuDBRiO3evXv4+WrVqtn48ePDzw8dOtRmzpxpixcv9nPqpiqybvr95MmT4WMVcPXYwYMH/b6qy4UKFbIlS5ZYjRo1vOqssV28eNEGDRpkpUuXtrx581qjRo38fKmh4ydNmmQ9evTwiwrJmTx5sr+fcePGWfXq1a1Pnz72xBNP2GuvvXYLny4AAAAAIGjRUzZOhsJypUqVbOrUqbZlyxbLnj27B+45c+Z4EK1SpYqtW7fOOnfubMWKFbMWLVp4KC9TpowtWLDAihQpYhs3bvQ54aqWt2/f3sPxrl277PTp0zZ9+nR/Hc0X13Gpce7cORszZozFx8f7+YsXL+4heOfOnTZv3jwrVaqULVq0yFq1amU7duzwMd6qTZs2facK/uijj3rFGwAAAAAQvaI6dBcsWNDy58/vYVtVYFWUR40aZatWrbImTZr4MRUrVrT169fblClTPHTnzJnThg0bFj6HKsQKrfPnz/fQnS9fPq+A61zXqixfz+XLl23ixIleYRdVuhXe9VOBWxTsNRddj2u8t+rIkSN25513JnpM93Xh4Pz58/5+ktL70y1ExwIAAAAA0ldUh+6k9u3b55Xmli1bJnr80qVLVq9evfD9CRMm2LRp0zwIK5Tq+bp166bJGHLlymW1a9cO31c1Wy3tVatWTXScAq8q4RlFHQGRFx8AAAAAAOkvU4Xus2fP+s9ly5b5/OlIml8tavFWpVnzn1UNV6V87Nixtnnz5uueO1u2bOEtyiKr2kmpqqx53pFjUiV+69at/jOSquppQRX5r7/+OtFjul+gQIFkq9wyePBgGzBgQKJKd9myZdNkPAAAAACALBi6IxcvUyt5cjZs2OArnffu3TvRCuhJq9WqTkfSnHDRtmSFCxf231OzJ7gq7DrX0aNHrXnz5hYEXTx47733Ej32/vvvh1vsk6PPKXQhAgAAAACQMaJ69fKkVLVWFbt///6+ArnC9KeffmpvvPGG3xctXPbJJ5/4dlp79+61IUOG+CJskbQK+meffWZ79uyxb775xiva2vtalWCtbv755597NV3V8pSorbxTp07WpUsXe/fdd317r48//tjbu3WO1NAibAr4J06csFOnTvnvkYH/Jz/5iR04cMBefPFF2717t88p1xx1fQ4AAAAAgOiVqSrdMnz4cK9KK9QqiGoLr/r169tLL73kz/fq1cu2bdtmHTp08Dbwjh07etV7+fLl4XNoey5t6dWgQQNvD1+zZo09+OCDNnfuXHvuued8znbDhg1txIgR9uSTT6Y4Ji2YpmMHDhxoX331lRUtWtQaN25sbdu2TdV7euyxx+zLL78M3w/NTw+1umsxOAV4hWyt6K7V2bV6ulYwBwAAAABEr5iEyEnMyLI0p1urwauSrrngAAAAAIDgM1amai8HAAAAACAzIXQHrHXr1r6KeXK3tNjDGwAAAAAQvTLdnO7MRnOvtVd4cmJjY9N9PAAAAACA9EPoDljS/cQBAAAAALcP2ssBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgKwYuhMSEqxnz54WGxtrMTExtn379owcDgAAAAAAWSd0r1ixwmbMmGFLly61w4cPW82aNW/5nN26dbO4uDiLBhcuXPDx1KpVy3LkyJHsuNauXesXHJLejhw5Ej5m6NCh33n+7rvvTud3AwAAAAC4UTksA+3fv99KlixpTZs2tWhz5coVD7fZsmW7pXPkyZPH+vXrZ++88851j92zZ48VKFAgfL948eKJnr/nnnts1apV4fsK8QAAAACA6JZhlW5VgPv27WuHDh3ycFu+fHm7evWqjR492ipUqOBhtU6dOrZw4cJEIbZ79+7h56tVq2bjx49PVBGeOXOmLV68OFwRViU5VE0+efJk+Fi1suuxgwcP+n1V3AsVKmRLliyxGjVqWO7cuX1sFy9etEGDBlnp0qUtb9681qhRIz9fauj4SZMmWY8ePaxEiRLXPVYhW8eEbknDvkJ25PNFixZN9WcNAAAAAMgYGVYuVViuVKmSTZ061bZs2WLZs2f3wD1nzhybPHmyValSxdatW2edO3e2YsWKWYsWLTyUlylTxhYsWGBFihSxjRs3+pxwVcvbt2/v4XjXrl12+vRpmz59ur+O5ovruNQ4d+6cjRkzxuLj4/38CsJ9+vSxnTt32rx586xUqVK2aNEia9Wqle3YscPHmFbq1q3rAV8t9rp4cP/99yd6/vPPP/fXv+OOO6xJkyb+WZUrV+6a59O5dAvRZwIAAAAAuE1Cd8GCBS1//vwetlW5VUAcNWqUt1ArVErFihVt/fr1NmXKFA/dOXPmtGHDhoXPoYr3pk2bbP78+R668+XL5xVwnSulynJyLl++bBMnTvQKu6jSrfCunwq8omCvueh6XOO9VbpgoIsMDRo08HEr8D/44IO2efNmq1+/vh+j6roq8arsa+67PoPmzZvbX//6V/8Mk6NQHvlZAQAAAADSX9RMDN63b59Xmlu2bJno8UuXLlm9evXC9ydMmGDTpk3zIHz+/Hl/XlXitJArVy6rXbt2+L6q2Wppr1q1aqLjFI5VCU8LCtK6hWh+u+a6v/baazZ79mx/rHXr1uHnNT6F8LvuussvNqjdPjmDBw+2AQMGJKp0ly1bNk3GDAAAAADIZKH77Nmz/nPZsmU+fzqS5leLWrxVaR43bpxXw1XlHTt2rFeFryc0P1pblEVWtZNSlVzzvCPHpEr81q1b/WckVdWDct9993mF/1o091wXAnSh4lr0mYU+NwAAAADAbR66IxcvUyt5cjZs2OCV4N69e4cfU1U4abVa1elImhMuas0uXLiw/56aPcFVYde5jh496u3c6UVjU9v5tehigN73j3/843QbEwAAAAAgE4duVa1Vxe7fv78vmNasWTM7deqUB21tpdW1a1dfuGzWrFm2cuVKn8+t9mstwqbfQ7QKup7XFlxqAdfc8cqVK3trtRYoGzlypO3du9er5SlRNblTp07WpUsXP14h/NixY7Z69Wpv827Tpk2K59AibGqBP3HihJ05cyYc9kMt8a+//rqPX1uCaV9vzen+4IMP7E9/+lP4HPpcHn/8cW8p/+c//2m//vWvvfLesWPHm/y0AQAAAAC3VeiW4cOHe1Vai4AdOHDA26i1mNhLL73kz/fq1cu2bdtmHTp08DZwhU5VvZcvXx4+h7bn0pZeWphMFeE1a9b4wmRz58615557zsNyw4YNbcSIEfbkk0+mOCYtmKZjBw4caF999ZVv1dW4cWNr27Ztqt7TY489Zl9++WX4fmh+eqjVXYE8dO7vfe97Pj4tJvfQQw+F/+Yf//iHv9fjx4/756MLEn/+85/DFXwAAAAAQHSKSYic6IwsSwupqeqv7gF1DgAAAAAAgs9Y/7fCGAAAAAAASHOE7lugrby0inlyt7TYwxsAAAAAkLlF1ZzuzEaLnmmv8OTExsam+3gAAAAAANGF0H0Lku4nDgAAAABAJNrLAQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAAAICKEbAAAAAICAELoBAAAAAAgIoRsAAAAAgIAQugEAAAAACAihGwAAAACAgBC6AQAAAADIiqE7ISHBevbsabGxsRYTE2Pbt2/PyOEAAAAAAJB1QveKFStsxowZtnTpUjt8+LDVrFnzls/ZrVs3i4uLs2hw4cIFH0+tWrUsR44cyY5r/fr1dv/991uRIkUsT548dvfdd9trr732neO++uor69y5c/g4nfOTTz5Jp3cCAAAAALgZOSwD7d+/30qWLGlNmza1aHPlyhWvvmfLlu2WzqGA3K9fP3vnnXeSPSZv3rzWp08fq127tv+uEN6rVy//XV0A8q9//cuD+UMPPWTLly+3YsWK2eeff26FCxe+6bEBAAAAALJwpVsV4L59+9qhQ4c83JYvX96uXr1qo0ePtgoVKnhYrVOnji1cuDBRiO3evXv4+WrVqtn48ePDzw8dOtRmzpxpixcv9nPqtnbtWr/p95MnT4aPVSu7Hjt48KDfV8W9UKFCtmTJEqtRo4blzp3bx3bx4kUbNGiQlS5d2oNwo0aN/HypoeMnTZpkPXr0sBIlSiR7TL169axjx452zz33+Gegavajjz5qH330UfiYMWPGWNmyZW369Ol23333+ft/5JFHrFKlSjf12QMAAAAAsnilW2FZoXHq1Km2ZcsWy549uwfuOXPm2OTJk61KlSq2bt06D6Gq7LZo0cJDeZkyZWzBggXeZr1x40avBqta3r59ew/Hu3btstOnT3tAFc0X13Gpce7cOQ+48fHxfv7ixYt7FXrnzp02b948K1WqlC1atMhatWplO3bs8DGmtW3btvl4R4wYEX5MFwIUxJ988kn78MMP/QJA7969Pcxfiy4W6BaizwQAAAAAcJuE7oIFC1r+/Pk9bKsKrIA4atQoW7VqlTVp0sSPqVixordbT5kyxUN3zpw5bdiwYeFzqOK7adMmmz9/vofufPnyeQVc57pWZfl6Ll++bBMnTvQKu6jSrfCunwrcomCvueh6XONNK7qYcOzYMfv222+9Yv/ss8+Gnztw4IBXzAcMGGAvvfSSX6RQy3quXLmsa9euyZ5PFzAiPysAAAAAwG02pzvSvn37vNLcsmXLRI9funTJW7BDJkyYYNOmTfMgfP78eX++bt26aTIGhVjNrQ5RNVst7VWrVk10nEK9KuFpSe3kZ8+etT//+c/2i1/8wipXruxt56IKf4MGDcIhX5/HX//6V+8IuFboHjx4sIf0yEq3WtQBAAAAALdh6FbglGXLlnn7dCTNrxa1eKvSPG7cOK+Gq1I+duxY27x583XPHVoMTVuURVa1k1KVXPO8I8ekSvzWrVv9ZyRV1dOSqvaiVcm//vprr3aHQrfa5zXPPFL16tWvuThb6DMLfW4AAAAAgNs8dEcuXqZW8uRs2LDBVzrXfObIFdCTVqtVnY6kOeGibclCK36nZk9wVZR1rqNHj1rz5s0tvaiyHTkfWyuX79mzJ9Exe/futbvuuivdxgQAAAAAyMShW1VrVbH79+/vobNZs2Z26tQpD9oFChTwNmotXDZr1ixbuXKlV4Znz57t85tDVWLRCuB6XiFVLeCaO65WbbVWq3o8cuRID6yqlqdEbeWdOnWyLl26+PEK4Zp3vXr1am9Db9OmTYrn0CJsaoE/ceKEnTlzJhz2Qy3xapcvV66c788tWjzu1Vdf9TnbIfpMdLFB7eWau/7xxx/7AnS6AQAAAACiV9SEbhk+fLhXpbUImBYP0xZe9evX98XDRPtXa3XvDh06eBu42q9V9dbe1SFa0VtbemkOtNrD16xZYw8++KDNnTvXnnvuOQ/LDRs29NXBtRp4SrRgmo4dOHCgffXVV1a0aFFr3LixtW3bNlXv6bHHHrMvv/wyfD80Pz3U6q4LDJp//cUXX1iOHDl8RXetoK73GqLxatV0HffKK6/4RYbXX3/dLwgAAAAAAKJXTELkRGdkWVpITVV/dQ+ocwAAAAAAEHzG+r8VxgAAAAAAQJojdN+C1q1b+yrmyd3Scg9vAAAAAEDmFFVzujOb+Ph43ys8ObGxsek+HgAAAABAdCF034Kk+4kDAAAAABCJ9nIAAAAAAAJC6AYAAAAAICCEbgAAAAAAAkLoBgAAAAAgIIRuAAAAAAACQugGAAAAACAgbBl2m0hISPCfp0+fzuihAAAAAECmF8pWoax1LYTu28Tx48f9Z9myZTN6KAAAAACQZZw5c8YKFix4zecJ3beJ2NhY/3no0KHrfiGA9L46qAtBf//7361AgQIZPRzA8b1ENOJ7iWjE9xK3+3cyISHBA3epUqWuexyh+zaRLdv/Td9X4Oa/FBFt9J3ke4low/cS0YjvJaIR30vczt/JgqkoaLKQGgAAAAAAASF0AwAAAAAQEEL3bSJ37tz261//2n8C0YLvJaIR30tEI76XiEZ8LxFtckfpdzImIaX1zQEAAAAAwE2h0g0AAAAAQEAI3QAAAAAABITQDQAAAABAQAjdWciECROsfPnydscdd1ijRo3s448/vu7xCxYssLvvvtuPr1Wrlr333nvpNlbcPm7ke/n73//emjdvboULF/bbww8/nOL3GEiP/74MmTdvnsXExFhcXFzgY8Tt50a/lydPnrTnn3/eSpYs6YsGVa1alf8tR4Z+J19//XWrVq2a5cmTx8qWLWv9+/e3CxcupNt4kfWtW7fOHn/8cStVqpT/7/Ef/vCHFP9m7dq1Vr9+ff/vycqVK9uMGTMsvRG6s4i3337bBgwY4Kv1ffrpp1anTh179NFH7ejRo8kev3HjRuvYsaN1797dtm3b5v8PpG5//etf033syLpu9Hup/1LU93LNmjW2adMm/x/sRx55xL766qt0Hzuyrhv9XoYcPHjQBg0a5BeGgIz+Xl66dMlatmzp38uFCxfanj17/MJl6dKl033syJpu9Dv51ltv2S9+8Qs/fteuXfa///u/fo6XXnop3ceOrOvf//63fxd1QSg1vvjiC2vTpo099NBDtn37dvvpT39qzz77rK1cudLSlVYvR+Z33333JTz//PPh+1euXEkoVapUwujRo5M9vn379glt2rRJ9FijRo0SevXqFfhYcfu40e9lUt9++21C/vz5E2bOnBngKHG7uZnvpb6LTZs2TYiPj0/o2rVrwg9/+MN0Gi1uFzf6vZw0aVJCxYoVEy5dupSOo8Tt5Ea/kzr2+9//fqLHBgwYkHD//fcHPlbcnswsYdGiRdc95sUXX0y45557Ej3WoUOHhEcffTQhPVHpzgJ0tXvr1q3eihuSLVs2v69qYXL0eOTxoquX1zoeSI/vZVLnzp2zy5cvW2xsbIAjxe3kZr+Xr7zyihUvXty7g4Bo+F4uWbLEmjRp4u3ld955p9WsWdNGjRplV65cSceRI6u6me9k06ZN/W9CLegHDhzw6Q6PPfZYuo0biNbMkyNdXw2B+Oabb/x/ZPU/upF0f/fu3cn+zZEjR5I9Xo8DGfW9TOrnP/+5z9lJ+l+WQHp+L9evX+9tkmpLA6Lle6lA88EHH1inTp082Ozbt8969+7tFyrV3guk93fyqaee8r9r1qyZOmnt22+/tZ/85Ce0lyNDXSvznD592s6fP+/rD6QHKt0AotJvfvMbX7Rq0aJFvoALkBHOnDljP/7xj32ubNGiRTN6OEDY1atXvfti6tSpdu+991qHDh3sl7/8pU2ePDmjh4bblNZlUbfFxIkTfQ74u+++a8uWLbPhw4dn9NCADEelOwvQ/yOYPXt2+/rrrxM9rvslSpRI9m/0+I0cD6TH9zLk1Vdf9dC9atUqq127dsAjxe3kRr+X+/fv94WqtFJqZNiRHDly+OJVlSpVSoeRIyu7mf++1IrlOXPm9L8LqV69uld11BqcK1euwMeNrOtmvpNDhgzxi5RapEq0M44WverZs+f/a+/OY2P6ogCOH3RTNLVvqaLUTiW1pQRBiKX4gyBqiTVNxdqoIIo2hMYasSZtSCipvU2a0pREqQQlpVVbqD/a2EosCcX75dxfZjLtD9HGdPw6308yxltm3n2Tm+mcd+49z1wQ0uHpQHX7Wczj5+dXbVluRe+vAfQPq17lzszMLPejUJd1vteP6HrH/dWFCxd+uj9QHf1SbdmyxVwVT09Pl9DQ0GpqLdxFZful3lYxLy/PDC23PcLDw+1VULXCPuCK78uwsDAzpNx2EUg9ePDABOME3HBFn9Q6LBUDa9tFoX9rXgHV76+Jeaq1bBucJjk52fL29raSkpKs/Px8a/78+Za/v79VUlJitkdERFgxMTH2/bOzsy0PDw8rISHBKigosNatW2d5enpaeXl5LjwLuHu/3Lx5s+Xl5WWlpKRYxcXF9sf79+9deBZw935ZEdXL8Tf0y6KiInN3h6ioKKuwsNBKTU21mjVrZsXFxbnwLODOfVJ/S2qfPHbsmPXkyRMrIyPDCgoKMnfMAf4U/U2Ym5trHhrKbtu2zfz/2bNnZrv2Se2bNtoXfX19rejoaBPz7Nmzx6pTp46Vnp5uVSeC7hpk9+7dVps2bUzQord5yMnJsW8bPHiw+aHo6MSJE1ZwcLDZX0vpp6WluaDVqOkq0y8DAwPNF2jFh/4hB1z5femIoBt/S7+8evWqud2nBkZ6+7D4+HhzezvAFX2yrKzMio2NNYG2j4+PFRAQYEVGRlqlpaUuaj1qoqysrB/+VrT1RX3WvlnxNSEhIaYf63dlYmJitbe7lv5Tvbl1AAAAAADcA3O6AQAAAABwEoJuAAAAAACchKAbAAAAAAAnIegGAAAAAMBJCLoBAAAAAHASgm4AAAAAAJyEoBsAAAAAACch6AYAAAAAwEkIugEAcFNDhgyRJUuWuLoZAADUaLUsy7Jc3QgAAFD93rx5I56entKgQQP521y6dEmGDh0qpaWl4u/v7+rmAABQZR5VfykAAPg/a9SokfyNysrKXN0EAAD+GIaXAwDgphyHl7dt21bi4uJkxowZUr9+fQkMDJRz587Jy5cvZfz48WZdz5495caNG/bXJyUlmSz0mTNnpGPHjuLj4yMjR46U58+flzvO3r17JSgoSLy8vKRTp05y5MiRcttr1apl9gkPD5d69erJvHnzTJZbNWzY0GyfNWuWWU5PT5eBAwea4zZu3FjGjh0rjx8/tr/X06dPzf6nTp0y7+Hr6yu9evWSa9eulTtmdna2OX/drsfQdmtWXX3//l02bdok7dq1k7p165rXp6Sk/PHPHwDgHgi6AQCAsX37dgkLC5Pc3FwZM2aMREREmCB8+vTpcuvWLRM467LjzLRPnz5JfHy8HD582ASyb9++lSlTpti3nz59WhYvXizLly+Xu3fvyoIFC2T27NmSlZVV7tixsbEyceJEycvLk/Xr18vJkyfN+sLCQikuLpadO3ea5Y8fP8qyZctM8J+ZmSm1a9c2r9NA2dHq1atlxYoVcvv2bQkODpapU6fK169fzTZdN2zYMOnatasJxq9cuSLjxo2Tb9++me0acOv57Nu3T+7duydLly41n8Hly5ed+OkDAGoq5nQDAOCmNNMbEhIiO3bsMJnuQYMG2bPQJSUl0rJlS1m7dq1s2LDBrMvJyZEBAwaYILhFixYm060BtK7v16+f2ef+/fvSpUsXuX79uvTt29cE8d26dZMDBw7Yjzt58mQTPKelpZllzUxrxl2D/srO6X716pU0bdrUBOvdu3c3mW7NUB86dEjmzJlj9snPzzdtKCgokM6dO8u0adOkqKjIBNsVff782Qy7v3jxojlXm7lz55oLDEePHv0DnzwAwJ2Q6QYAAIYOH7dp3ry5ee7Ro8d/1r148cK+zsPDQ/r06WNf1qBWg2QNcJU+a+DtSJdt221CQ0N/q40PHz40Wev27duLn5+fuVigNIj+2bnoxQPHdtsy3T/y6NEjE1yPGDHCDKm3PTTz7TiMHQCA30UhNQAAYGglcxvNPv9sXcWh3H+CzuX+HToMXOebHzx4UFq1amXaohnuL1++lNvvV+3Wedo/8+HDB/OsWfjWrVuX2+bt7V2JMwIA4F9kugEAQJXpPGnH4mo6B1vndesQc6XPOtfbkS7rfOpf0aJryjbPWr1+/dq8/5o1a0ymWt/bVvysMjQLrvPBf0TbpcG1Zs47dOhQ7hEQEFDpYwEAQKYbAABUmWaUFy1aJLt27TJDzaOioqR///5mPreKjo42c7h79+4tw4cPl/Pnz5vK4jpn+lc0m60Z6tTUVBk9erTJTmuVca1YrvPDdci4BsYxMTGVbvOqVavMsPnIyEhZuHChCfC1sNukSZOkSZMmpgCbFk/TzLhWSn/37p25UKDD2WfOnFnlzwoA4J7IdAMAgCrTW26tXLnSFCfTudo6//n48eP27RMmTDCVxxMSEkwxs/3790tiYqIp4vYrOrRbq5hrUK1zyTWY10rlycnJcvPmTTOkXAPjrVu3VrrNWs08IyND7ty5Yy4OaMG0s2fPmosGauPGjaaAnFYx12z6qFGjzHBzLdAGAEBlUb0cAABUiVYv16rjOpwcAAD8GJluAAAAAACchKAbAAAAAAAnYXg5AAAAAABOQqYbAAAAAAAnIegGAAAAAMBJCLoBAAAAAHASgm4AAAAAAJyEoBsAAAAAACch6AYAAAAAwEkIugEAAAAAcBKCbgAAAAAAnISgGwAAAAAAcY5/ABwyHM5sEObmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. FINAL REALITY CHECK\n",
      "============================================================\n",
      "Negative predictions: 0\n",
      "Prediction range: $89,358 - $132,058,612\n",
      "Actual range: $797 - $135,200,000\n",
      "\n",
      "6. IMPROVEMENT SUMMARY\n",
      "============================================================\n",
      "Original Model (Overfitted):\n",
      "  MAE: $62,707, RÂ²: 0.9997, CV MAE: $668,723\n",
      "Final Model (Gradient Boosting):\n",
      "  MAE: $36,247, RÂ²: 0.9928, CV MAE: $58,011\n",
      "Improvement: More realistic and trustworthy model\n",
      "\n",
      "CLEANING COMPLETE: From 1790 to 168 features\n",
      "REALISTIC MODEL: RÂ² 0.9928 with proper validation\n"
     ]
    }
   ],
   "source": [
    "def clean_and_rebuild_dataset(X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Clean the dataset by removing constant and problematic features\"\"\"\n",
    "    \n",
    "    print(\"CLEANING DATASET AND REBUILDING MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Remove constant features\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "    print(\"1. Removing constant features...\")\n",
    "    selector = VarianceThreshold(threshold=0.01)  # Remove features with <1% variance\n",
    "    X_train_clean = selector.fit_transform(X_train_imputed)\n",
    "    X_test_clean = selector.transform(X_test_imputed)\n",
    "    \n",
    "    n_features_original = X_train_imputed.shape[1]\n",
    "    n_features_clean = X_train_clean.shape[1]\n",
    "    print(f\"   Removed {n_features_original - n_features_clean} constant features\")\n",
    "    print(f\"   Remaining features: {n_features_clean}\")\n",
    "    \n",
    "    # 2. Remove highly correlated features\n",
    "    print(\"2. Removing highly correlated features...\")\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(X_train_clean, rowvar=False)\n",
    "    \n",
    "    # Find features to remove (correlation > 0.95)\n",
    "    to_drop = set()\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i+1, len(corr_matrix)):\n",
    "            if abs(corr_matrix[i, j]) > 0.95:\n",
    "                to_drop.add(j)\n",
    "    \n",
    "    # Keep only non-redundant features\n",
    "    keep_indices = [i for i in range(X_train_clean.shape[1]) if i not in to_drop]\n",
    "    X_train_clean = X_train_clean[:, keep_indices]\n",
    "    X_test_clean = X_test_clean[:, keep_indices]\n",
    "    \n",
    "    print(f\"   Removed {len(to_drop)} highly correlated features\")\n",
    "    print(f\"   Final features: {X_train_clean.shape[1]}\")\n",
    "    \n",
    "    # 3. Create meaningful feature names for the cleaned dataset\n",
    "    feature_names_clean = [f'feature_{i}' for i in range(X_train_clean.shape[1])]\n",
    "    \n",
    "    return X_train_clean, X_test_clean, feature_names_clean\n",
    "\n",
    "def train_final_models(X_train_clean, y_train, X_test_clean, y_test):\n",
    "    \"\"\"Train models on cleaned data\"\"\"\n",
    "    \n",
    "    print(\"\\n3. TRAINING MODELS ON CLEANED DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            min_samples_split=15,\n",
    "            min_samples_leaf=10,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': XGBRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.5,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=150,\n",
    "            max_depth=15,\n",
    "            min_samples_split=15,\n",
    "            min_samples_leaf=5,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            model.fit(X_train_clean, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_clean)\n",
    "            \n",
    "            # Ensure no negative predictions\n",
    "            y_pred = np.maximum(y_pred, 0)  # Clip negative predictions to 0\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            cv_scores = cross_val_score(model, X_train_clean, y_train, \n",
    "                                      cv=kf, scoring='neg_mean_absolute_error')\n",
    "            cv_mae = -cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "            # Train score for overfitting check\n",
    "            y_pred_train = model.predict(X_train_clean)\n",
    "            train_r2 = r2_score(y_train, y_pred_train)\n",
    "            overfit_gap = train_r2 - r2\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'cv_mae': cv_mae,\n",
    "                'cv_std': cv_std,\n",
    "                'train_r2': train_r2,\n",
    "                'overfit_gap': overfit_gap,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"Test MAE: ${mae:,.2f}\")\n",
    "            print(f\"Test RÂ²: {r2:.4f}\")\n",
    "            print(f\"Train RÂ²: {train_r2:.4f}\")\n",
    "            print(f\"Overfit Gap: {overfit_gap:.4f}\")\n",
    "            print(f\"CV MAE: ${cv_mae:,.2f} Â± ${cv_std:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, X_train_clean, top_n=20):\n",
    "    \"\"\"Analyze which features are most important\"\"\"\n",
    "    \n",
    "    print(f\"\\n4. FEATURE IMPORTANCE ANALYSIS (Top {top_n})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(top_n))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=importance_df.head(top_n), x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Most Important Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model doesn't support feature importance analysis\")\n",
    "        return None\n",
    "\n",
    "def run_complete_cleaning_pipeline(X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Complete pipeline for cleaning data and rebuilding models\"\"\"\n",
    "    \n",
    "    print(\"STARTING DATA CLEANING AND MODEL REBUILDING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Clean the dataset\n",
    "    X_train_clean, X_test_clean, feature_names_clean = clean_and_rebuild_dataset(\n",
    "        X_train_imputed, y_train, X_test_imputed, y_test\n",
    "    )\n",
    "\n",
    "    # Step 2: Train models on cleaned data\n",
    "    final_results = train_final_models(X_train_clean, y_train, X_test_clean, y_test)\n",
    "\n",
    "    if final_results:\n",
    "        # Step 3: Analyze results\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL MODEL RESULTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for name, result in final_results.items():\n",
    "            print(f\"{name:20} | MAE: ${result['mae']:,.2f} | RÂ²: {result['r2']:.4f} | Overfit: {result['overfit_gap']:.4f}\")\n",
    "        \n",
    "        # Step 4: Find best model\n",
    "        best_final_name = min(final_results.keys(), key=lambda x: final_results[x]['mae'])\n",
    "        best_final_model = final_results[best_final_name]['model']\n",
    "        best_final_result = final_results[best_final_name]\n",
    "        \n",
    "        print(f\"\\nBEST FINAL MODEL: {best_final_name}\")\n",
    "        print(f\"   Test MAE: ${best_final_result['mae']:,.2f}\")\n",
    "        print(f\"   Test RÂ²: {best_final_result['r2']:.4f}\")\n",
    "        print(f\"   CV MAE: ${best_final_result['cv_mae']:,.2f} Â± ${best_final_result['cv_std']:,.2f}\")\n",
    "        print(f\"   Overfit Gap: {best_final_result['overfit_gap']:.4f}\")\n",
    "        \n",
    "        # Step 5: Feature importance analysis\n",
    "        importance_df = analyze_feature_importance(\n",
    "            best_final_model, feature_names_clean, X_train_clean\n",
    "        )\n",
    "        \n",
    "        # Step 6: Final reality check\n",
    "        print(f\"\\n5. FINAL REALITY CHECK\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        y_pred_final = best_final_result['predictions']\n",
    "        \n",
    "        # Check for negative predictions\n",
    "        negative_count = (y_pred_final < 0).sum()\n",
    "        print(f\"Negative predictions: {negative_count}\")\n",
    "        \n",
    "        # Check prediction range\n",
    "        print(f\"Prediction range: ${y_pred_final.min():,.0f} - ${y_pred_final.max():,.0f}\")\n",
    "        print(f\"Actual range: ${y_test.min():,.0f} - ${y_test.max():,.0f}\")\n",
    "        \n",
    "        # Compare with original problematic model\n",
    "        print(f\"\\n6. IMPROVEMENT SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Original Model (Overfitted):\")\n",
    "        print(f\"  MAE: $62,707, RÂ²: 0.9997, CV MAE: $668,723\")\n",
    "        print(f\"Final Model ({best_final_name}):\")\n",
    "        print(f\"  MAE: ${best_final_result['mae']:,.0f}, RÂ²: {best_final_result['r2']:.4f}, CV MAE: ${best_final_result['cv_mae']:,.0f}\")\n",
    "        print(f\"Improvement: More realistic and trustworthy model\")\n",
    "        \n",
    "        # Save the cleaned data and final model\n",
    "        final_model_data = {\n",
    "            'model': best_final_model,\n",
    "            'feature_names': feature_names_clean,\n",
    "            'X_train_clean': X_train_clean,\n",
    "            'X_test_clean': X_test_clean,\n",
    "            'results': final_results\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCLEANING COMPLETE: From {X_train_imputed.shape[1]} to {X_train_clean.shape[1]} features\")\n",
    "        print(f\"REALISTIC MODEL: RÂ² {best_final_result['r2']:.4f} with proper validation\")\n",
    "        \n",
    "        return final_model_data\n",
    "    else:\n",
    "        print(\"No models trained successfully\")\n",
    "        return None\n",
    "\n",
    "# Run the complete cleaning pipeline\n",
    "print(\"Running complete data cleaning and model rebuilding...\")\n",
    "final_model_data = run_complete_cleaning_pipeline(X_train_imputed, y_train, X_test_imputed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9141416d-b627-4ae9-8f8c-1e8eaa745542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating dominant feature and creating balanced models...\n",
      "INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\n",
      "============================================================\n",
      "1. Analyzing Feature 0...\n",
      "Top 5 features by correlation with target:\n",
      "   Original Feature 0: correlation = nan\n",
      "   Original Feature 1: correlation = nan\n",
      "   Original Feature 2: correlation = nan\n",
      "   Original Feature 3: correlation = nan\n",
      "   Original Feature 4: correlation = nan\n",
      "   Dominant feature is likely original feature 0\n",
      "\n",
      "2. Feature importance distribution in cleaned model:\n",
      "Top 10 features by importance:\n",
      "       feature  importance\n",
      "0    feature_0    0.984018\n",
      "11  feature_11    0.009132\n",
      "8    feature_8    0.005325\n",
      "2    feature_2    0.000884\n",
      "4    feature_4    0.000276\n",
      "3    feature_3    0.000075\n",
      "9    feature_9    0.000069\n",
      "7    feature_7    0.000063\n",
      "13  feature_13    0.000050\n",
      "1    feature_1    0.000046\n",
      "\n",
      "3. Creating balanced models without the dominant feature...\n",
      "   Training with 167 features (excluding dominant feature_0)\n",
      "\n",
      "--- Training Gradient Boosting (Balanced) ---\n",
      "Test MAE: $171,154.35\n",
      "Test RÂ²: 0.9511\n",
      "CV MAE: $233,335.88 Â± $58,763.51\n",
      "\n",
      "--- Training XGBoost (Balanced) ---\n",
      "Test MAE: $157,436.37\n",
      "Test RÂ²: 0.9558\n",
      "CV MAE: $198,204.79 Â± $69,269.33\n",
      "\n",
      "--- Training Random Forest (Balanced) ---\n",
      "Test MAE: $84,865.69\n",
      "Test RÂ²: 0.9772\n",
      "CV MAE: $127,780.25 Â± $53,041.61\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "============================================================\n",
      "ORIGINAL (Overfitted) Models:\n",
      "  Gradient Boosting: MAE: $62,707, RÂ²: 0.9997\n",
      "\n",
      "CLEANED Models:\n",
      "  Gradient Boosting: MAE: $36,247, RÂ²: 0.9928\n",
      "  XGBoost: MAE: $103,307, RÂ²: 0.9621\n",
      "  Random Forest: MAE: $404,084, RÂ²: 0.8409\n",
      "\n",
      "BALANCED Models (no dominant feature):\n",
      "  Gradient Boosting (Balanced): MAE: $171,154, RÂ²: 0.9511\n",
      "  XGBoost (Balanced): MAE: $157,436, RÂ²: 0.9558\n",
      "  Random Forest (Balanced): MAE: $84,866, RÂ²: 0.9772\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATION\n",
      "============================================================\n",
      "BEST OVERALL MODEL: Gradient Boosting\n",
      "   Test MAE: $36,246.89\n",
      "   Test RÂ²: 0.9928\n",
      "   CV MAE: $58,011.48\n",
      "   CV/Test Ratio: 1.60x\n",
      "   GOOD: Reasonable CV-Test consistency\n",
      "\n",
      "DEPLOYMENT RECOMMENDATION:\n",
      "Use the CLEANED model - it provides the best performance\n",
      "\n",
      "FINAL MODEL SELECTED: Gradient Boosting\n",
      "PERFORMANCE: MAE $36,247, RÂ² 0.9928\n",
      "MODEL SAVED: commercial_real_estate_model_final.pkl\n",
      "READY FOR DEPLOYMENT!\n",
      "\n",
      "FINAL FEATURE IMPORTANCE (Top 10):\n",
      "       feature  importance\n",
      "0    feature_0    0.984018\n",
      "11  feature_11    0.009132\n",
      "8    feature_8    0.005325\n",
      "2    feature_2    0.000884\n",
      "4    feature_4    0.000276\n",
      "3    feature_3    0.000075\n",
      "9    feature_9    0.000069\n",
      "7    feature_7    0.000063\n",
      "13  feature_13    0.000050\n",
      "1    feature_1    0.000046\n"
     ]
    }
   ],
   "source": [
    "def investigate_feature_0_and_improve(final_model_data, X_train_imputed, y_train, X_test_imputed, y_test):\n",
    "    \"\"\"Investigate the dominant feature and create even better models\"\"\"\n",
    "    \n",
    "    print(\"INVESTIGATING DOMINANT FEATURE AND CREATING BALANCED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check what Feature 0 might be by looking at correlations with original data\n",
    "    print(\"1. Analyzing Feature 0...\")\n",
    "    \n",
    "    # Create a temporary dataframe with proper column names\n",
    "    temp_df = pd.DataFrame(X_train_imputed, columns=[f'orig_feature_{i}' for i in range(X_train_imputed.shape[1])])\n",
    "    temp_df['target'] = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "    \n",
    "    # Find which original feature has highest correlation with target\n",
    "    correlations = []\n",
    "    for i in range(X_train_imputed.shape[1]):\n",
    "        col_name = f'orig_feature_{i}'\n",
    "        corr = np.corrcoef(temp_df[col_name], temp_df['target'])[0, 1]\n",
    "        correlations.append((i, abs(corr)))\n",
    "    \n",
    "    # Sort by correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top 5 features by correlation with target:\")\n",
    "    for i, (feature_idx, corr) in enumerate(correlations[:5]):\n",
    "        print(f\"   Original Feature {feature_idx}: correlation = {corr:.4f}\")\n",
    "    \n",
    "    dominant_feature_idx = correlations[0][0]\n",
    "    print(f\"   Dominant feature is likely original feature {dominant_feature_idx}\")\n",
    "    \n",
    "    # 2. Check what Feature 0 in our cleaned data represents\n",
    "    print(f\"\\n2. Feature importance distribution in cleaned model:\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': final_model_data['feature_names'],\n",
    "        'importance': final_model_data['results']['Gradient Boosting']['model'].feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features by importance:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # 3. Create models without the overly dominant feature\n",
    "    print(f\"\\n3. Creating balanced models without the dominant feature...\")\n",
    "    \n",
    "    # Check if we have multiple features to work with\n",
    "    if len(final_model_data['feature_names']) > 1:\n",
    "        # Remove the dominant feature (feature_0)\n",
    "        X_train_balanced = final_model_data['X_train_clean'][:, 1:]  # Remove feature_0\n",
    "        X_test_balanced = final_model_data['X_test_clean'][:, 1:]   # Remove feature_0\n",
    "        feature_names_balanced = final_model_data['feature_names'][1:]\n",
    "        \n",
    "        print(f\"   Training with {X_train_balanced.shape[1]} features (excluding dominant feature_0)\")\n",
    "        \n",
    "        # Train balanced models\n",
    "        balanced_models = {\n",
    "            'Gradient Boosting (Balanced)': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=15,\n",
    "                subsample=0.7,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'XGBoost (Balanced)': XGBRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.7,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'Random Forest (Balanced)': RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                # max_depth=12,\n",
    "                # min_samples_split=20,\n",
    "                # min_samples_leaf=10,\n",
    "                # max_features=0.6,\n",
    "                # bootstrap=True,\n",
    "                random_state=42,\n",
    "                # n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        balanced_results = {}\n",
    "        \n",
    "        for name, model in balanced_models.items():\n",
    "            print(f\"\\n--- Training {name} ---\")\n",
    "            \n",
    "            try:\n",
    "                model.fit(X_train_balanced, y_train)\n",
    "                y_pred = model.predict(X_test_balanced)\n",
    "                y_pred = np.maximum(y_pred, 0)  # No negative prices\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, X_train_balanced, y_train, \n",
    "                                          cv=5, scoring='neg_mean_absolute_error')\n",
    "                cv_mae = -cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                balanced_results[name] = {\n",
    "                    'model': model,\n",
    "                    'mae': mae,\n",
    "                    'r2': r2,\n",
    "                    'cv_mae': cv_mae,\n",
    "                    'cv_std': cv_std,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "                print(f\"Test MAE: ${mae:,.2f}\")\n",
    "                print(f\"Test RÂ²: {r2:.4f}\")\n",
    "                print(f\"CV MAE: ${cv_mae:,.2f} Â± ${cv_std:,.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "    else:\n",
    "        print(\"   Not enough features to create balanced models (only 1 feature remaining)\")\n",
    "        balanced_results = {}\n",
    "        X_train_balanced = None\n",
    "        X_test_balanced = None\n",
    "        feature_names_balanced = None\n",
    "    \n",
    "    return balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced\n",
    "\n",
    "def compare_all_models(original_results, cleaned_results, balanced_results):\n",
    "    \"\"\"Compare performance across all model versions\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"ORIGINAL (Overfitted) Models:\")\n",
    "    print(f\"  Gradient Boosting: MAE: $62,707, RÂ²: 0.9997\")\n",
    "    \n",
    "    print(\"\\nCLEANED Models:\")\n",
    "    for name, result in cleaned_results.items():\n",
    "        print(f\"  {name}: MAE: ${result['mae']:,.0f}, RÂ²: {result['r2']:.4f}\")\n",
    "    \n",
    "    if balanced_results:\n",
    "        print(\"\\nBALANCED Models (no dominant feature):\")\n",
    "        for name, result in balanced_results.items():\n",
    "            print(f\"  {name}: MAE: ${result['mae']:,.0f}, RÂ²: {result['r2']:.4f}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RECOMMENDATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_results = {}\n",
    "    if cleaned_results:\n",
    "        all_results.update(cleaned_results)\n",
    "    if balanced_results:\n",
    "        all_results.update(balanced_results)\n",
    "    \n",
    "    if all_results:\n",
    "        best_overall = min(all_results.items(), key=lambda x: x[1]['mae'])\n",
    "        best_name, best_result = best_overall\n",
    "        \n",
    "        cv_test_ratio = best_result['cv_mae'] / best_result['mae']\n",
    "        \n",
    "        print(f\"BEST OVERALL MODEL: {best_name}\")\n",
    "        print(f\"   Test MAE: ${best_result['mae']:,.2f}\")\n",
    "        print(f\"   Test RÂ²: {best_result['r2']:.4f}\")\n",
    "        print(f\"   CV MAE: ${best_result['cv_mae']:,.2f}\")\n",
    "        print(f\"   CV/Test Ratio: {cv_test_ratio:.2f}x\")\n",
    "        \n",
    "        if cv_test_ratio > 2.0:\n",
    "            print(\"   WARNING: High CV-Test gap suggests some overfitting remains\")\n",
    "        else:\n",
    "            print(\"   GOOD: Reasonable CV-Test consistency\")\n",
    "        \n",
    "        return best_name, best_result\n",
    "    else:\n",
    "        print(\"No valid models to compare\")\n",
    "        return None, None\n",
    "\n",
    "# Run the investigation and improvement\n",
    "print(\"Investigating dominant feature and creating balanced models...\")\n",
    "balanced_results, X_train_balanced, X_test_balanced, feature_names_balanced = investigate_feature_0_and_improve(\n",
    "    final_model_data, X_train_imputed, y_train, X_test_imputed, y_test\n",
    ")\n",
    "\n",
    "# Compare all models\n",
    "best_name, best_result = compare_all_models(\n",
    "    {'Gradient Boosting': {'mae': 62707, 'r2': 0.9997}},  # Original overfitted\n",
    "    final_model_data['results'],  # Cleaned models\n",
    "    balanced_results  # Balanced models\n",
    ")\n",
    "\n",
    "# Final model selection and deployment\n",
    "if best_result:\n",
    "    print(f\"\\nDEPLOYMENT RECOMMENDATION:\")\n",
    "    \n",
    "    if balanced_results and best_name in balanced_results:\n",
    "        print(\"Use the BALANCED model - it's more robust and doesn't rely on a single dominant feature\")\n",
    "        final_deployment_model = balanced_results[best_name]['model']\n",
    "        deployment_features = feature_names_balanced\n",
    "        model_type = \"balanced\"\n",
    "    else:\n",
    "        print(\"Use the CLEANED model - it provides the best performance\")\n",
    "        final_deployment_model = final_model_data['results'][best_name]['model']\n",
    "        deployment_features = final_model_data['feature_names']\n",
    "        model_type = \"cleaned\"\n",
    "    \n",
    "    print(f\"\\nFINAL MODEL SELECTED: {best_name}\")\n",
    "    print(f\"PERFORMANCE: MAE ${best_result['mae']:,.0f}, RÂ² {best_result['r2']:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    import pickle\n",
    "    import datetime\n",
    "    \n",
    "    deployment_package = {\n",
    "        'model': final_deployment_model,\n",
    "        'feature_names': deployment_features,\n",
    "        'performance': best_result,\n",
    "        'timestamp': datetime.datetime.now(),\n",
    "        'model_type': model_type,\n",
    "        'data_shape': {\n",
    "            'n_features': len(deployment_features),\n",
    "            'n_train_samples': X_train_imputed.shape[0],\n",
    "            'n_test_samples': X_test_imputed.shape[0]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('industrial_real_estate_model_final.pkl', 'wb') as f:\n",
    "        pickle.dump(deployment_package, f)\n",
    "    \n",
    "    print(f\"MODEL SAVED: commercial_real_estate_model_final.pkl\")\n",
    "    print(f\"READY FOR DEPLOYMENT!\")\n",
    "    \n",
    "    # Show final feature importance for the deployed model\n",
    "    if hasattr(final_deployment_model, 'feature_importances_'):\n",
    "        print(f\"\\nFINAL FEATURE IMPORTANCE (Top 10):\")\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': deployment_features,\n",
    "            'importance': final_deployment_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3af2ae-ba6b-416d-b216-e4e53d829d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06876c0c-1f58-4cfd-8f1f-a321faf5c64a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
